{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/model_Training_and_Evaluation_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZxG26MjG3e7"
      },
      "source": [
        "# Set up: mount drive + import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn4hsdQa0oyk"
      },
      "source": [
        "**Important Information:** We need to activate the GPU on Colab (_Runtime --> Change runtime type_). \\\n",
        "Every time you start a new session (or reopen the notebook after a few hours) check that the GPU is still active. If we are not using the GPU it can take up to tens of hours to train the models. \\\n",
        "_GPU T4 is the best choice._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafrlWt-G9_y",
        "outputId": "0a7623e1-755d-4899-80f9-6e23f5259119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZRM0tKnG-Oj",
        "outputId": "16710eaa-7a0a-48d6-8046-28a09eaefb7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.168-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.168-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.168 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics # Installation of Ultralytics for YOLO models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iIi5eOj24kp"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO # Import of Ultralytics for YOLO models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B7lwWkpiu0JY"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Import base for EfficientDet:\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from effdet.efficientdet import HeadNet\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
        "from effdet.bench import unwrap_bench\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import json\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlwU4zYYYxnX"
      },
      "source": [
        "# General Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOtSOJRWpnEx"
      },
      "source": [
        "### Backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNetjej6sKWr"
      },
      "source": [
        "In Computer Vision, a _Backbone_ is the part of a convolutional neural network responsible for extracting the main features from an image. \\\n",
        "It serves as the shared base upon which subsequent modules are built (such as heads for classification, object detection, segmentation, etc.).\n",
        "\n",
        "\\\n",
        "Each backbone has been pre-trained on specific datasets (e.g., ImageNet) using particular preprocessing steps, input dimensions, normalization, and augmentation techniques, which should ideally be replicated during fine-tuning to maintain compatibility and achieve optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWlCNAmDfJPv"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0R1TXAeOOLn"
      },
      "source": [
        "To train a deep learning model, it is essential to properly handle data loading and preparation. This is the task of the _Data Loader_, a component responsible for:\n",
        "- Loading images and their corresponding annotations (e.g., .txt or .json) from the dataset.\n",
        "- Applying preprocessing operations, such as resizing, normalization, data augmentation, etc.\n",
        "- Organizing data into batches to feed the model during training.\n",
        "\n",
        "\\\n",
        "Considering the limited resources of my development environment, at first I decided to perform the image and annotation resizing in a separate phase (prior to training), in order to:\n",
        "- Reduce the workload of the data loader during fine-tuning;\n",
        "- Increase data loading and training speed;\n",
        "- Ensure consistency between images and annotations.\n",
        "\n",
        "But due to the different type of scaling technique, I want to try to fine-tuning the model without any pre-scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RCnQiNkOdt4"
      },
      "source": [
        "The other transformations, instead, are handled by the model-specific data loader, since each model uses different preprocessing and normalization techniques. \\\n",
        "Moreover, some models require specific transformations to achieve optimal performance, and the libraries that provide the models (e.g., Ultralytics for YOLO, torchvision for EfficientDet/SSD) already implement loaders that are properly configured and optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSgaMN1SZHeW"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxCOR7b1PV1W"
      },
      "source": [
        "Image normalization consists in scaling pixel values from the range [0, 255] to a more suitable interval (e.g., [0, 1] or [-1, 1]), often based on the mean and standard deviation of the pre-training dataset, with the goal of:\n",
        "- Avoiding overly large values in the tensors;\n",
        "- Making the model more stable during training;\n",
        "- Speeding up convergence.\n",
        "\n",
        "\\\n",
        "Normalization helps maintain a consistent pixel range and distribution, which is essential for pre-trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tw6aba8f75J"
      },
      "source": [
        "### Data Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPcoqopoPxS3"
      },
      "source": [
        "Data augmentation consists of random transformations (e.g., rotations, flips, crops, brightness changes, etc.) applied during training. Their purpose is to:\n",
        "- Simulate new visual conditions;\n",
        "- Increase dataset variety;\n",
        "- Reduce overfitting by improving the model’s ability to generalize.\n",
        "\n",
        "\\\n",
        "In practice, the semantic content of the image doesn't change (e.g., a person remains a person), but its visual appearance is altered to help the model \"learn better.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2QIAGYbgIhJ"
      },
      "source": [
        "\n",
        "My goal is to evaluate the real-world performance of each model in its ideal scenario, in order to select the most suitable one for deployment on the Jetson Nano. \\\n",
        "For this reason, each model is trained using its native augmentations, meaning the ones that were designed and optimized as part of its original architecture. \\\n",
        "It wouldn’t make sense to disable them or enforce a uniform setup across models, because what we want to observe is the maximum potential of each model, working in the way it was designed to perform best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7HKNc7udFy"
      },
      "source": [
        "# Fine-Tuning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdC8RvUw1F_9"
      },
      "source": [
        "### YOLOv8n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z51qZOzlxvca"
      },
      "source": [
        "First of all we need to save the dataset locally:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfB0GzrFxu7o",
        "outputId": "e7917fd3-87e0-4832-834d-71834a894f46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copy completed: True\n"
          ]
        }
      ],
      "source": [
        "src = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n'\n",
        "dst = '/content/AERALIS_YOLOv8n_local'  # is now on the local VM, NOT on drive\n",
        "\n",
        "# If the destination folder already exists, I delete it\n",
        "if os.path.exists(dst):\n",
        "  shutil.rmtree(dst)\n",
        "\n",
        "# Recursive copy of ENTIRE folder (and subfolders)\n",
        "shutil.copytree(src, dst)\n",
        "print(\"Copy completed:\", os.path.exists(dst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rf3uEUe0j7t"
      },
      "source": [
        "Let's check the total free space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Wf22_i0Rok",
        "outputId": "a10861a9-3942-405f-aef4-b2e1b765130a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   56G   58G  49% /\n"
          ]
        }
      ],
      "source": [
        "!df -h / # It shows the total, used and free space on the root (/) of the Colab VM.\n",
        "\n",
        "# Avail column: space still available for your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTQQX_sw0fQb",
        "outputId": "1d972f7a-a19e-4f8c-f5df-308f0bbfbac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.6G\t/content/AERALIS_YOLOv8n_local\n"
          ]
        }
      ],
      "source": [
        "# Show space used by your local folder\n",
        "!du -sh /content/AERALIS_YOLOv8n_local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj3PMKWJ1Z3f",
        "outputId": "fc323f6a-38ea-4289-d622-d474efd37499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "140K\t/content/.config\n",
            "du: cannot access '/content/drive/.Encrypted/.shortcut-targets-by-id/1LQbD7p_iS5KLqGNdfrYEvsAx0i_bgB0h/projectUPV': No such file or directory\n",
            "67G\t/content/drive\n",
            "6.6G\t/content/AERALIS_YOLOv8n_local\n",
            "31M\t/content/runs_finetune\n",
            "55M\t/content/sample_data\n",
            "74G\t/content/\n"
          ]
        }
      ],
      "source": [
        "# Show space occupied by various folders in /content/.\n",
        "!du -h --max-depth=1 /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtwQcmy01Odt"
      },
      "source": [
        "We want to create the data.yaml file, which YOLO uses to know:\n",
        "- the path to the training, validation, and test images\n",
        "- the number of classes (nc)\n",
        "- the names of the classes (names)\n",
        "\n",
        "\\\n",
        "This file is used by YOLO to locate the images and their annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6VlyhGe1N10",
        "outputId": "6131ad44-926c-484b-ff7c-f80f3280a0e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YAML dataset (edit routes)\n",
        "data_yaml = \"\"\"\n",
        "train: /content/AERALIS_YOLOv8n_local/train/images\n",
        "val:   /content/AERALIS_YOLOv8n_local/val/images\n",
        "test:  /content/AERALIS_YOLOv8n_local/test/images\n",
        "\n",
        "nc: 1\n",
        "names: ['person']\n",
        "\"\"\"\n",
        "open('data.yaml', 'w').write(data_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmvOfl4IslZw"
      },
      "source": [
        "Perfect, we have correctly written the data.yaml file for the AERALIS_YOLOv8n dataset. Let's continue with the loading of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6PJD4WHubXR"
      },
      "outputs": [],
      "source": [
        "# Upload the pre-trained model we want to use as a starting point\n",
        "\n",
        "model_YOLOv8n = YOLO('yolov8n.pt') # it is the model that will be fine-tuned on the custom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67c-lmU6sdXp"
      },
      "source": [
        "We have now downloaded the pre-trained model from the official Ultralytics repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0gsz0CG6PPi"
      },
      "source": [
        "The **Batch size** is the number of images processed simultaneously in each training step. With 4-8GB of RAM, a batch size of 8 (or even less) is recommended, so we'll start with that value and reduce it if necessary.\n",
        "\n",
        "**Early Stopping** is a technique that automatically stops the training process if the model stops improving after a certain number of epochs. This helps prevent overfitting and saves time.\n",
        "\n",
        "**Workers** are the parallel processes used to load and preprocess data while the model is training. However, due to our limited resources, we’ll start with 2 workers, and if data loading errors occur, we'll reduce this number to 1 or even 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYQFf9Vf8Jwf",
        "outputId": "282c0e33-8e6c-47ef-d6ea-db5fcec90798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# To see the available GPU\n",
        "print(torch.cuda.is_available()) # True = you have GPU --> if False then use device='cpu'\n",
        "print(torch.cuda.device_count()) # Name of GPU\n",
        "\n",
        "# If True and at least 1, you can use device=0.\n",
        "# If you don't have GPU: use device='cpu' (much slower).\n",
        "# Locally (not Colab): check with nvidia-smi from terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmQfxSxns7I3"
      },
      "source": [
        "Now that we have confirmation that the GPU is active we can train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMKnsnJl3Ywr",
        "outputId": "c666aa22-e33d-4327-e9e4-67bec60500d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=person_yolov8n4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs_finetune, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs_finetune/person_yolov8n4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3514.4±245.8 MB/s, size: 2026.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/AERALIS_YOLOv8n_local/train/labels.cache... 2395 images, 388 backgrounds, 0 corrupt: 100%|██████████| 2395/2395 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 901.0±20.7 MB/s, size: 2606.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/AERALIS_YOLOv8n_local/val/labels.cache... 515 images, 75 backgrounds, 0 corrupt: 100%|██████████| 515/515 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs_finetune/person_yolov8n4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns_finetune/person_yolov8n4\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      1/100      2.03G      2.025      1.712      1.023         61        640: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.728       0.54      0.593      0.249\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      2/100      2.03G       2.04      1.462      1.036         52        640: 100%|██████████| 150/150 [01:05<00:00,  2.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.713      0.505      0.537      0.211\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      3/100      2.03G      2.064      1.333      1.051         50        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.713      0.506       0.56      0.226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      4/100       2.1G      2.013      1.301      1.046         39        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.719      0.551      0.594       0.24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      5/100       2.1G      2.005      1.235       1.04         45        640: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.807      0.518      0.612      0.268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      6/100      2.12G      1.969      1.188      1.033         37        640: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.613       0.49      0.499       0.21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      7/100      2.12G      1.943      1.184      1.027         47        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.728      0.567      0.613      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      8/100      2.14G      1.924      1.141      1.013         39        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.74      0.536      0.613      0.263\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      9/100      2.14G      1.878       1.11     0.9923         40        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.765      0.599       0.66      0.289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     10/100      2.14G      1.859      1.072     0.9979         61        640: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.851      0.582       0.67      0.306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     11/100      2.14G      1.861      1.068      0.997         40        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.797      0.609      0.693      0.323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     12/100      2.14G      1.791      1.034     0.9833         42        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.794      0.613      0.673      0.309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     13/100      2.14G      1.773     0.9961     0.9758         47        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.788      0.623      0.685      0.327\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     14/100      2.14G      1.769     0.9976     0.9777         36        640: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.817      0.603       0.69      0.322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     15/100      2.15G      1.774     0.9841     0.9741         61        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.775      0.632      0.693      0.326\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     16/100      2.15G      1.775     0.9779     0.9722         41        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.808      0.642       0.71      0.338\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     17/100      2.17G      1.736      0.957     0.9694         61        640: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.84       0.64      0.722      0.341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     18/100      2.17G      1.749     0.9632     0.9686         65        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.827      0.626      0.712      0.339\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     19/100      2.19G      1.723     0.9329      0.961         66        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.815      0.643      0.713      0.344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     20/100      2.19G      1.689     0.9235     0.9546         29        640: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.842      0.661      0.732      0.354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     21/100      2.19G      1.698     0.9341     0.9538         65        640: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.839      0.647      0.739      0.356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     22/100      2.19G      1.696     0.9209     0.9524         39        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.846      0.648      0.741      0.373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     23/100      2.28G      1.663     0.9109     0.9496         42        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873      0.643      0.752      0.376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     24/100      2.28G      1.648     0.8686     0.9518         57        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.809      0.668      0.747      0.378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     25/100      2.28G      1.629     0.8655     0.9397         44        640: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.858      0.691       0.77      0.381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     26/100      2.28G      1.636     0.8714     0.9447         65        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.869      0.682      0.779       0.38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     27/100      2.28G      1.625     0.8701     0.9404         52        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.82      0.676      0.755      0.365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     28/100      2.28G      1.623     0.8511     0.9372         42        640: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.841      0.684      0.773      0.386\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     29/100      2.28G       1.61     0.8552     0.9404         55        640: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.846      0.689      0.768      0.381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     30/100      2.28G      1.597     0.8364     0.9339         36        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.859      0.693       0.77      0.385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     31/100      2.28G      1.578     0.8265     0.9351         49        640: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.851      0.689      0.774      0.392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     32/100      2.28G      1.584     0.8297     0.9363         56        640: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.854      0.684       0.77      0.398\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     33/100      2.28G      1.589     0.8483     0.9337         44        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.856       0.71      0.793      0.406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     34/100      2.28G      1.543     0.8078     0.9247         55        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.827      0.699      0.777      0.386\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     35/100      2.28G      1.557     0.8229     0.9245         31        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.854      0.715       0.79      0.397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     36/100      2.28G       1.56     0.8033     0.9244         32        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.861      0.722      0.802      0.405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     37/100      2.28G      1.536     0.7947     0.9257         47        640: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882        0.7      0.794       0.41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     38/100      2.28G      1.515     0.7877     0.9102         47        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.845      0.721      0.797      0.407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     39/100      2.28G      1.523     0.7687     0.9156         68        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.868       0.71      0.798      0.403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     40/100      2.28G      1.519     0.7921     0.9201         42        640: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.875      0.684      0.785      0.402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     41/100      2.28G      1.497     0.7757     0.9225         40        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.851      0.719      0.801      0.415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     42/100      2.28G      1.496     0.7626     0.9123         52        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.87      0.728      0.809      0.422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     43/100      2.28G      1.511     0.7667     0.9127         44        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.852      0.726      0.804      0.406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     44/100      2.28G      1.516     0.7719     0.9157        102        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.868      0.734      0.817       0.42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     45/100      2.28G      1.495     0.7586     0.9097         66        640: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.866      0.731      0.814      0.417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     46/100      2.28G      1.482     0.7577     0.9089         31        640: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.868       0.75      0.823      0.425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     47/100      2.28G      1.476     0.7516     0.9117         41        640: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.871      0.734      0.812      0.422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     48/100      2.28G      1.466     0.7448     0.9066         40        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.854      0.732      0.803      0.411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     49/100      2.28G      1.463     0.7505     0.9063         34        640: 100%|██████████| 150/150 [01:06<00:00,  2.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.875      0.742      0.823      0.436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     50/100      2.38G      1.447     0.7293     0.8971         24        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.872      0.726      0.817      0.427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     51/100      2.38G      1.421     0.7202     0.9023         59        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.84      0.724      0.806      0.425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     52/100      2.38G      1.444      0.722     0.9002         31        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.866      0.712      0.806      0.425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     53/100      2.38G      1.442      0.726     0.8949         51        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.876      0.729      0.827      0.428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     54/100      2.38G      1.444     0.7267     0.8971         50        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873      0.733       0.82      0.424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     55/100      2.38G      1.419     0.7161     0.8971         29        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.85      0.753      0.824      0.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     56/100      2.38G      1.413     0.7102     0.8993         44        640: 100%|██████████| 150/150 [01:06<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.87      0.746       0.83      0.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     57/100      2.38G      1.413     0.7024     0.9053         36        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.879      0.753      0.828      0.433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     58/100      2.38G      1.384     0.6933     0.8941         80        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.875      0.724      0.814      0.422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     59/100      2.38G      1.398     0.6965     0.8937         29        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.876      0.749      0.834       0.44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     60/100      2.38G      1.376       0.69     0.8971         60        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.87      0.746      0.827      0.443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     61/100      2.38G      1.382     0.6847     0.8938         41        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873      0.732       0.82      0.444\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     62/100      2.38G       1.39     0.6894     0.8923         32        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.868      0.742      0.822      0.443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     63/100      2.38G      1.362     0.6772     0.8889         64        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.859      0.728      0.812      0.443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     64/100      2.38G      1.378     0.6752     0.8906         63        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873      0.731      0.825      0.448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     65/100      2.38G      1.344     0.6669     0.8851         43        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.866      0.751      0.834      0.446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     66/100      2.38G      1.349     0.6609     0.8864         72        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.856      0.756      0.828      0.451\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     67/100      2.38G      1.333     0.6547     0.8763         56        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.889      0.746      0.837      0.454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     68/100      2.48G      1.354     0.6614     0.8842         39        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.881       0.73      0.829      0.448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     69/100      2.48G       1.33     0.6526      0.883         55        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.884      0.739      0.833      0.453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     70/100      2.48G      1.318     0.6449     0.8754         28        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.861      0.762      0.836      0.454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     71/100      2.48G      1.324     0.6467     0.8785         22        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.862      0.759      0.832      0.455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     72/100      2.48G      1.321      0.648     0.8729         58        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.874      0.741      0.832      0.453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     73/100      2.48G      1.308     0.6405     0.8755         50        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.871      0.754      0.835      0.458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     74/100      2.48G      1.314     0.6406     0.8758         49        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.893       0.75      0.844      0.463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     75/100      2.48G      1.309     0.6383     0.8745         43        640: 100%|██████████| 150/150 [01:05<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.893      0.753      0.845      0.465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     76/100      2.48G      1.285     0.6355      0.877         31        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882      0.762      0.841      0.461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     77/100      2.48G      1.303     0.6355     0.8734         45        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.86      0.758      0.839      0.464\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     78/100      2.48G      1.302     0.6324     0.8725         29        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.745      0.834      0.456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     79/100      2.48G      1.273      0.617     0.8774         44        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.872      0.755      0.843      0.463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     80/100      2.48G      1.279      0.616     0.8718         42        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.884      0.754      0.843       0.46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     81/100      2.48G      1.264     0.6098     0.8733         65        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.875      0.767      0.845       0.47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     82/100      2.48G      1.257     0.6066     0.8662         49        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.86      0.782      0.849       0.47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     83/100      2.48G      1.269     0.6173     0.8651         40        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.89      0.754       0.84      0.467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     84/100      2.48G      1.272     0.6106     0.8697         66        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.763      0.846      0.466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     85/100      2.48G      1.248     0.6054     0.8662         40        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.88      0.769      0.856      0.476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     86/100      2.48G      1.233     0.5921     0.8635         50        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.872      0.764      0.844      0.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     87/100      2.48G      1.258     0.6098     0.8646         43        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.876      0.769      0.855      0.472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     88/100      2.48G      1.245     0.6047     0.8615         42        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.761      0.852      0.462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     89/100      2.48G      1.241     0.6014     0.8658         43        640: 100%|██████████| 150/150 [01:06<00:00,  2.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258        0.9      0.759      0.854      0.473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     90/100      2.48G      1.227     0.5937     0.8615         28        640: 100%|██████████| 150/150 [01:06<00:00,  2.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.902      0.756      0.856      0.473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     91/100      2.48G      1.246     0.6033     0.8731         22        640: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:06<00:00,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.874      0.771      0.847      0.465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     92/100      2.48G      1.227     0.5855     0.8642         36        640: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.762      0.851      0.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     93/100      2.48G        1.2      0.576     0.8624         23        640: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.852      0.787      0.853      0.474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     94/100      2.48G      1.188     0.5708     0.8609         35        640: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.872      0.783      0.859      0.477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     95/100      2.48G      1.199      0.571     0.8616         23        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.904      0.757      0.853      0.472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     96/100      2.48G      1.192     0.5721     0.8664         18        640: 100%|██████████| 150/150 [01:05<00:00,  2.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258        0.9      0.758      0.851      0.474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     97/100      2.48G      1.173     0.5639      0.856         21        640: 100%|██████████| 150/150 [01:04<00:00,  2.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.887       0.77      0.856      0.479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     98/100      2.48G      1.161     0.5505     0.8515         32        640: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.905       0.77      0.861      0.483\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     99/100      2.48G       1.16     0.5538     0.8518         29        640: 100%|██████████| 150/150 [01:05<00:00,  2.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.908      0.761      0.855      0.479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    100/100      2.48G      1.154     0.5461     0.8617         15        640: 100%|██████████| 150/150 [01:04<00:00,  2.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.896      0.769      0.857      0.481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "100 epochs completed in 2.009 hours.\n",
            "Optimizer stripped from runs_finetune/person_yolov8n4/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs_finetune/person_yolov8n4/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs_finetune/person_yolov8n4/weights/best.pt...\n",
            "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:05<00:00,  2.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.905      0.771      0.861      0.482\n",
            "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns_finetune/person_yolov8n4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Fine‑tuning\n",
        "results_YOLOv8n = model_YOLOv8n.train(\n",
        "  data='data.yaml', # use the newly created yaml file\n",
        "  epochs=100, # Maximum number of training epochs\n",
        "  imgsz=640, # Image input size (recommended for YOLO).\n",
        "  batch=16,  # Batch size\n",
        "  patience=20, # Early stopping if the metrics do not improve for 20 epochs\n",
        "  workers=2, # Number of workers for the dataloader\n",
        "  device=0, # Use GPU 0 (or put 'cpu' if you don't have GPU)\n",
        "  # device='cpu',\n",
        "  project='runs_finetune', # Folder where it will save the results of the experiments (the folder will be created automatically)\n",
        "  name='person_yolov8n' # Subfolder/name specific to our experiment\n",
        ")\n",
        "\n",
        "# results -->  will contain metrics, logs, and the path of the best weights found during the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fyPjFuZeghX"
      },
      "source": [
        "We now want to evaluate the trained model using the Test set defined in data.yaml. \\\n",
        "YOLO does not compute standard accuracy, because in object detection True Negatives (TN) are not counted. Therefore, traditional accuracy is not applicable or useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYLO2bxBfHoI"
      },
      "source": [
        "So, we will compute:\n",
        "- **Precision**: how correct your detected positives are\n",
        "- **Recall**: how many of the real objects you detected\n",
        "- **mAP50**: mean Average Precision with IoU ≥ 0.5 (how accurate the predictions are)\n",
        "- **mAP50-95**: average over various IoU thresholds, a more strict metric\n",
        "- **F1_score**: combination of precision and recall (you can compute it as: 2 * (P * R) / (P + R))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWuTdRMH3Yt7",
        "outputId": "dfdf08cf-d2f2-4fe1-ed51-ff69efda6303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 80.0±11.5 MB/s, size: 1605.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/AERALIS_YOLOv8n_local/test/labels... 516 images, 93 backgrounds, 0 corrupt: 100%|██████████| 516/516 [00:01<00:00, 505.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/AERALIS_YOLOv8n_local/test/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 33/33 [00:15<00:00,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        516       1271      0.905      0.829      0.904      0.506\n",
            "Speed: 0.3ms preprocess, 1.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns_finetune/person_yolov8n42\u001b[0m\n",
            "Test metrics: ultralytics.utils.metrics.DetMetrics object with attributes:\n",
            "\n",
            "ap_class_index: array([0])\n",
            "box: ultralytics.utils.metrics.Metric object\n",
            "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7d498713b390>\n",
            "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
            "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,\n",
            "            0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99857,     0.99717,     0.99717,     0.99717,     0.99717,\n",
            "            0.99717,     0.99717,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,\n",
            "            0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,     0.99601,      0.9947,      0.9947,      0.9947,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,\n",
            "            0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99363,     0.99241,     0.99241,     0.99241,     0.99125,     0.99125,     0.99125,     0.99125,\n",
            "            0.99125,     0.99125,     0.99125,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,     0.99036,\n",
            "            0.99036,     0.99036,     0.99036,     0.98923,     0.98923,     0.98923,     0.98923,     0.98697,     0.98697,     0.98697,     0.98697,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98488,     0.98488,     0.98488,     0.98488,     0.98383,\n",
            "            0.98383,     0.98383,     0.98383,     0.98272,     0.98161,     0.98073,     0.98073,     0.98073,     0.98073,     0.98073,     0.98073,     0.98073,     0.98073,     0.97984,     0.97984,     0.97984,     0.97984,     0.97984,     0.97984,     0.97984,     0.97984,     0.97907,     0.97907,\n",
            "            0.97907,     0.97907,     0.97907,     0.97907,     0.97907,     0.97907,     0.97907,     0.97907,     0.97907,     0.97697,     0.97697,     0.97495,     0.97495,     0.97495,     0.97394,     0.97394,     0.97109,     0.97109,     0.97109,     0.97109,     0.97109,     0.97109,     0.97109,\n",
            "            0.96908,     0.96908,     0.96709,     0.96709,     0.96324,     0.96324,     0.96324,     0.96324,     0.96266,     0.96266,     0.96266,     0.96266,     0.96266,     0.96266,     0.96266,     0.96266,     0.96266,     0.96186,     0.96186,     0.96186,     0.96186,     0.96115,     0.96115,\n",
            "            0.96115,     0.96115,     0.96115,     0.96049,     0.96049,     0.96049,     0.96049,     0.96049,     0.96049,     0.96049,     0.95976,     0.95976,     0.95976,     0.95976,     0.95796,     0.95796,     0.95796,     0.95713,     0.95713,     0.95648,     0.95648,     0.95648,     0.95648,\n",
            "            0.95648,     0.95648,     0.95486,     0.95486,     0.95486,     0.95486,     0.95308,     0.95308,     0.95233,     0.95233,     0.95233,     0.95053,     0.94976,     0.94976,     0.94727,     0.94727,     0.94727,     0.94727,     0.94376,     0.94376,     0.94302,     0.94302,     0.94234,\n",
            "            0.94234,     0.94234,     0.94173,     0.94173,     0.94173,     0.94173,     0.94002,     0.93838,     0.93779,     0.93779,     0.93779,     0.93779,     0.93704,     0.93704,     0.93548,     0.93548,     0.93468,     0.93394,     0.93394,     0.93163,     0.93163,     0.93163,     0.93097,\n",
            "            0.93097,     0.92857,     0.92857,     0.92473,     0.92473,     0.92473,     0.92473,     0.92321,     0.92164,     0.92089,     0.91961,     0.91961,     0.91961,     0.91961,     0.91887,     0.91419,     0.91346,     0.91297,     0.91297,     0.91297,     0.90838,     0.90838,      0.9062,\n",
            "            0.90558,     0.90558,     0.90497,     0.90051,     0.90051,     0.89983,     0.89772,     0.89722,     0.89722,     0.89722,     0.89438,     0.89381,     0.89381,     0.89316,     0.89037,     0.88751,     0.88687,      0.8856,     0.88062,      0.8802,      0.8802,     0.87682,     0.87682,\n",
            "            0.87349,     0.87349,     0.87019,     0.86821,     0.86555,     0.86324,     0.86324,     0.86324,     0.86063,      0.8548,     0.85292,     0.85248,     0.85248,     0.85139,     0.85085,     0.84716,     0.84146,     0.83776,     0.83535,     0.83046,     0.82467,     0.82467,     0.82418,\n",
            "            0.81346,     0.80959,     0.80753,     0.80753,     0.80043,     0.80043,       0.799,       0.799,     0.79196,     0.78711,     0.78671,     0.78412,     0.78116,     0.78023,     0.77717,     0.77534,     0.77444,     0.77263,     0.77263,     0.77137,     0.76581,      0.7634,        0.76,\n",
            "            0.75679,     0.75247,     0.74045,     0.73274,     0.72733,     0.72156,     0.71411,     0.70484,     0.69356,     0.68672,     0.68487,      0.6808,     0.67661,      0.6764,     0.65169,     0.64215,     0.63152,     0.63035,     0.60415,     0.60415,     0.59724,     0.53163,     0.50715,\n",
            "            0.50715,     0.50365,     0.49452,     0.45702,     0.43443,     0.42186,     0.35006,     0.31961,      0.2924,     0.28386,     0.25678,     0.24338,     0.23164,     0.22322,     0.21869,     0.21372,      0.2092,     0.16519,     0.16253,     0.15986,      0.1572,     0.15453,     0.15187,\n",
            "             0.1492,     0.14654,     0.14388,     0.14121,     0.13855,     0.13588,     0.13322,     0.13055,     0.12789,     0.12523,     0.12256,      0.1199,     0.11723,     0.11457,      0.1119,     0.10924,     0.10657,     0.10391,     0.10125,    0.098582,    0.095917,    0.093253,    0.090589,\n",
            "           0.087924,     0.08526,    0.082596,    0.079931,    0.077267,    0.074602,    0.071938,    0.069274,    0.066609,    0.063945,    0.061281,    0.058616,    0.055952,    0.053287,    0.050623,    0.047959,    0.045294,     0.04263,    0.039966,    0.037301,    0.034637,    0.031972,    0.029308,\n",
            "           0.026644,    0.023979,    0.021315,    0.018651,    0.015986,    0.013322,    0.010657,   0.0079931,   0.0053287,   0.0026644,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.28126,     0.28141,     0.36342,     0.41691,     0.45023,     0.48067,     0.50737,     0.52669,     0.54754,     0.56672,     0.57948,     0.59464,     0.60589,      0.6158,     0.62546,     0.63551,     0.64366,      0.6527,     0.65849,     0.66696,     0.67266,     0.67999,     0.68591,\n",
            "            0.69149,     0.69635,     0.70226,     0.70653,     0.71034,     0.71479,     0.72019,     0.72497,     0.72766,     0.73042,     0.73423,     0.73657,     0.73992,     0.74419,     0.74613,     0.74919,     0.75197,     0.75377,     0.75707,     0.75987,     0.76269,     0.76421,     0.76632,\n",
            "            0.76831,     0.77109,     0.77288,     0.77619,     0.77622,     0.77736,      0.7784,      0.7791,     0.78072,     0.78169,     0.78229,     0.78393,     0.78551,     0.78816,     0.78987,     0.79157,     0.79298,     0.79413,     0.79585,     0.79737,      0.7978,     0.79864,     0.79968,\n",
            "            0.80101,     0.80174,     0.80249,     0.80414,     0.80501,     0.80608,     0.80842,      0.8092,     0.81009,     0.81104,     0.81185,     0.81262,     0.81337,     0.81574,     0.81734,     0.81882,     0.81898,     0.81942,     0.82024,     0.82129,     0.82149,     0.82082,     0.82184,\n",
            "             0.8228,     0.82333,     0.82357,      0.8238,     0.82543,     0.82554,     0.82624,     0.82704,     0.82836,     0.82827,     0.82841,     0.82838,     0.82858,     0.82897,      0.8288,      0.8293,     0.82993,      0.8304,     0.83077,      0.8315,     0.83182,     0.83158,     0.83197,\n",
            "            0.83292,     0.83248,     0.83316,     0.83399,     0.83497,     0.83506,     0.83514,     0.83523,     0.83489,     0.83579,     0.83598,     0.83647,     0.83751,     0.83783,     0.83818,     0.83811,     0.83822,     0.83775,     0.83848,     0.83775,     0.83737,     0.83798,     0.83854,\n",
            "            0.83882,     0.83904,     0.83935,     0.84127,     0.84103,     0.84051,     0.84116,     0.84149,     0.84139,     0.84171,     0.84187,       0.843,     0.84322,      0.8435,      0.8444,     0.84513,     0.84565,     0.84662,     0.84718,     0.84735,     0.84766,     0.84837,     0.84817,\n",
            "            0.84758,     0.84835,     0.84995,     0.84966,     0.84999,     0.85046,     0.85143,     0.85174,     0.85192,     0.85209,     0.85206,     0.85248,     0.85272,     0.85287,     0.85309,     0.85311,     0.85293,      0.8535,     0.85454,     0.85444,     0.85516,     0.85547,      0.8556,\n",
            "            0.85587,     0.85657,      0.8568,     0.85697,      0.8572,     0.85683,     0.85662,      0.8566,     0.85717,     0.85733,     0.85748,     0.85804,     0.85805,     0.85802,     0.85753,     0.85814,     0.85738,     0.85782,       0.858,     0.85778,     0.85789,     0.85911,     0.85968,\n",
            "            0.85988,     0.86004,     0.86017,     0.86021,     0.86016,     0.86106,     0.86057,     0.85998,     0.86014,     0.86065,     0.86063,     0.86052,     0.86039,     0.86054,     0.86089,     0.86163,     0.86156,     0.86145,     0.86133,     0.86123,     0.86142,     0.86159,     0.86171,\n",
            "            0.86183,     0.86208,     0.86196,     0.86219,     0.86279,     0.86291,     0.86303,     0.86257,     0.86296,     0.86365,     0.86381,     0.86358,     0.86313,     0.86351,     0.86413,      0.8644,      0.8644,     0.86377,      0.8636,     0.86344,     0.86337,     0.86348,     0.86359,\n",
            "            0.86335,     0.86407,     0.86466,     0.86503,     0.86515,     0.86527,     0.86541,     0.86555,     0.86558,     0.86538,     0.86515,     0.86476,     0.86519,     0.86518,     0.86517,     0.86494,     0.86585,     0.86606,       0.866,     0.86636,      0.8661,     0.86629,     0.86657,\n",
            "             0.8664,     0.86657,     0.86629,     0.86585,     0.86577,     0.86657,     0.86636,     0.86636,     0.86651,     0.86633,     0.86615,     0.86595,     0.86575,     0.86557,     0.86543,     0.86528,     0.86522,     0.86543,     0.86464,     0.86493,     0.86553,     0.86539,       0.865,\n",
            "            0.86573,     0.86601,     0.86669,     0.86644,      0.8657,     0.86541,     0.86557,     0.86518,     0.86519,     0.86538,     0.86561,     0.86545,     0.86529,     0.86504,     0.86481,     0.86495,     0.86509,     0.86561,     0.86582,     0.86661,       0.866,     0.86575,     0.86553,\n",
            "            0.86544,     0.86535,     0.86526,     0.86518,     0.86509,     0.86516,     0.86528,      0.8654,     0.86497,     0.86512,     0.86527,     0.86486,     0.86502,      0.8652,     0.86539,     0.86512,     0.86559,     0.86589,     0.86612,     0.86626,     0.86611,     0.86622,     0.86633,\n",
            "            0.86639,     0.86623,     0.86607,     0.86592,     0.86583,     0.86574,     0.86565,     0.86556,     0.86547,     0.86432,     0.86407,     0.86421,     0.86438,     0.86457,     0.86434,     0.86456,     0.86459,     0.86444,     0.86429,     0.86458,     0.86476,     0.86443,     0.86398,\n",
            "            0.86426,     0.86448,     0.86413,     0.86313,     0.86276,      0.8625,     0.86234,     0.86252,     0.86276,     0.86314,     0.86333,      0.8636,     0.86391,     0.86359,     0.86331,     0.86393,      0.8642,     0.86369,     0.86345,     0.86283,     0.86295,     0.86307,     0.86312,\n",
            "            0.86203,       0.862,     0.86183,     0.86166,     0.86173,     0.86189,     0.86216,     0.86147,     0.86174,     0.86156,     0.86137,     0.86155,     0.86132,     0.86051,     0.86052,     0.86063,     0.86072,     0.86082,     0.86091,     0.86034,     0.86022,      0.8601,     0.85997,\n",
            "             0.8593,     0.85868,     0.85787,     0.85747,     0.85744,     0.85779,     0.85789,     0.85751,     0.85789,     0.85727,     0.85673,     0.85662,     0.85652,     0.85641,      0.8563,     0.85611,     0.85594,      0.8557,     0.85541,     0.85501,     0.85422,     0.85441,     0.85422,\n",
            "            0.85385,      0.8527,     0.85287,     0.85309,     0.85326,     0.85338,      0.8535,     0.85368,     0.85392,     0.85364,     0.85329,      0.8529,     0.85272,     0.85254,     0.85168,      0.8519,     0.85213,     0.85229,     0.85243,     0.85253,     0.85178,     0.85132,     0.85142,\n",
            "            0.85108,      0.8516,      0.8515,     0.85128,     0.85067,     0.85042,     0.85016,     0.84987,     0.84921,     0.84922,     0.84949,      0.8497,     0.84912,     0.84853,     0.84764,     0.84737,     0.84742,      0.8476,     0.84729,     0.84719,     0.84733,     0.84746,     0.84724,\n",
            "            0.84673,     0.84609,     0.84569,     0.84501,     0.84398,     0.84414,     0.84431,     0.84385,     0.84358,     0.84332,     0.84284,     0.84285,     0.84291,     0.84297,     0.84302,     0.84308,     0.84314,     0.84206,     0.84166,     0.84181,       0.842,     0.84202,     0.84169,\n",
            "            0.84143,     0.84102,     0.84061,     0.84014,      0.8394,     0.83963,     0.83914,     0.83871,     0.83767,     0.83616,     0.83557,     0.83568,      0.8358,     0.83591,     0.83489,     0.83473,     0.83456,      0.8344,     0.83402,      0.8329,      0.8323,      0.8319,     0.83166,\n",
            "             0.8309,     0.83009,     0.83016,     0.83023,      0.8303,     0.83037,      0.8302,     0.82939,     0.82869,     0.82851,      0.8281,     0.82705,     0.82679,     0.82573,     0.82492,     0.82465,     0.82498,     0.82463,     0.82437,     0.82416,     0.82396,     0.82339,     0.82304,\n",
            "            0.82246,     0.82265,     0.82322,     0.82315,     0.82326,     0.82288,     0.82246,       0.822,     0.82164,     0.81991,     0.81758,      0.8177,       0.818,     0.81752,     0.81716,     0.81578,     0.81622,     0.81588,     0.81552,     0.81541,     0.81574,     0.81461,     0.81341,\n",
            "            0.81289,     0.81234,     0.81159,      0.8109,     0.80974,     0.80946,     0.80889,     0.80753,      0.8072,     0.80602,     0.80482,     0.80438,     0.80349,     0.80324,     0.80287,     0.80121,     0.80068,     0.80012,     0.79961,     0.79934,     0.79906,     0.79855,     0.79746,\n",
            "            0.79723,     0.79702,     0.79709,     0.79584,     0.79531,     0.79492,     0.79457,     0.79475,     0.79492,     0.79384,     0.79252,     0.79215,     0.79225,     0.79244,     0.79176,     0.79072,     0.78946,     0.78897,     0.78855,     0.78763,     0.78741,     0.78754,     0.78767,\n",
            "            0.78736,     0.78583,     0.78502,     0.78443,     0.78458,     0.78488,     0.78452,     0.78333,     0.78239,     0.78106,      0.7803,     0.77893,     0.77823,     0.77785,     0.77684,     0.77596,     0.77551,      0.7736,     0.77251,     0.77163,      0.7708,     0.76992,     0.76884,\n",
            "            0.76719,     0.76679,     0.76652,     0.76635,     0.76617,     0.76588,     0.76518,     0.76484,     0.76438,     0.76339,     0.76292,      0.7614,      0.7606,      0.7606,      0.7597,     0.75899,     0.75839,     0.75692,     0.75492,     0.75328,      0.7522,     0.74931,      0.7476,\n",
            "            0.74663,     0.74592,     0.74334,     0.74269,      0.7419,     0.74127,     0.73963,       0.739,     0.73834,     0.73799,     0.73711,     0.73666,     0.73366,     0.73278,     0.73247,     0.73154,      0.7298,     0.72941,     0.72779,     0.72577,     0.72535,     0.72408,     0.72372,\n",
            "             0.7233,      0.7225,      0.7217,     0.71987,     0.71754,     0.71562,     0.71438,     0.71348,     0.71185,     0.71206,     0.71176,     0.70795,     0.70698,     0.70727,     0.70635,     0.70526,     0.70137,     0.70009,     0.69638,      0.6951,     0.69386,     0.69248,     0.69143,\n",
            "            0.68968,     0.68899,       0.688,     0.68451,      0.6829,      0.6814,     0.68098,      0.6803,     0.67856,     0.67781,     0.67689,      0.6744,     0.67207,     0.66919,     0.66713,      0.6648,      0.6613,     0.65905,     0.65853,     0.65794,     0.65721,     0.65441,      0.6524,\n",
            "            0.64929,     0.64784,     0.64637,     0.64368,     0.64151,     0.64024,     0.63668,     0.63461,     0.63244,     0.62893,     0.62562,      0.6188,     0.61532,     0.61388,     0.61082,     0.60674,     0.60326,     0.60055,      0.5955,      0.5919,     0.58819,     0.58639,     0.58317,\n",
            "            0.57581,     0.57241,     0.57003,       0.569,     0.56567,     0.56363,     0.56011,       0.554,     0.55206,     0.55091,     0.54642,     0.54354,     0.54259,     0.53813,     0.53382,     0.52903,     0.51807,     0.51473,     0.51347,     0.51179,     0.50954,     0.50558,     0.50181,\n",
            "            0.49798,     0.49209,     0.49008,       0.484,     0.47924,     0.47292,     0.47132,     0.46749,     0.46447,     0.46255,      0.4579,     0.45415,     0.44926,     0.44395,     0.43783,     0.43322,     0.42962,     0.42576,     0.42355,     0.41981,     0.41537,     0.41012,     0.40418,\n",
            "            0.39743,     0.39273,     0.38968,     0.38206,      0.3765,     0.37429,     0.37133,     0.36397,     0.35952,      0.3555,     0.35334,       0.347,     0.34558,     0.34139,     0.33808,     0.33075,      0.3283,     0.32479,     0.31778,     0.31163,     0.30649,     0.29862,     0.29126,\n",
            "            0.28813,      0.2834,     0.28041,     0.27524,     0.26773,     0.25843,     0.25292,     0.24565,     0.24079,     0.23454,     0.22513,     0.21648,     0.21264,     0.20653,     0.19682,     0.18744,     0.18154,     0.17695,     0.17319,     0.16519,     0.16316,     0.15898,     0.15089,\n",
            "            0.14526,     0.13491,     0.13381,     0.13079,     0.12728,      0.1218,     0.11671,     0.11316,     0.10449,     0.10147,    0.099914,    0.097742,    0.096726,    0.095148,    0.093102,    0.089595,    0.085745,    0.083841,    0.082437,    0.081617,    0.078385,    0.073662,    0.072052,\n",
            "           0.069334,     0.06777,    0.065985,    0.063563,    0.061968,    0.060435,    0.059364,    0.058442,    0.055909,    0.054738,      0.0542,    0.053662,    0.049748,    0.046804,    0.042708,    0.041179,    0.040529,    0.036455,    0.034889,    0.034093,    0.032295,    0.031473,    0.028519,\n",
            "           0.027043,    0.024194,    0.022338,    0.021546,    0.021156,    0.020766,    0.020376,    0.018355,    0.017374,    0.014884,    0.012479,    0.012115,    0.011751,    0.011386,    0.011022,   0.0097095,   0.0085204,   0.0077611,   0.0075615,   0.0073618,   0.0071621,   0.0069624,   0.0067626,\n",
            "          0.0065628,   0.0063629,   0.0030066,   0.0026636,   0.0023206,   0.0019775,   0.0016342,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.16544,     0.16554,     0.22566,     0.26861,     0.29717,     0.32437,     0.34918,     0.36799,     0.38868,     0.40829,     0.42176,     0.43828,     0.45061,     0.46186,     0.47281,     0.48439,     0.49393,     0.50536,     0.51282,     0.52317,     0.53047,     0.53992,     0.54743,\n",
            "            0.55458,     0.56085,     0.56855,     0.57418,     0.57923,     0.58516,     0.59244,     0.59928,     0.60295,      0.6078,     0.61309,     0.61636,     0.62106,     0.62711,     0.63018,       0.635,     0.63901,     0.64196,      0.6468,      0.6509,     0.65546,     0.65812,     0.66124,\n",
            "            0.66421,     0.66838,     0.67108,     0.67609,     0.67699,     0.67873,     0.68066,     0.68183,     0.68476,     0.68672,     0.68809,     0.69062,     0.69342,     0.69769,     0.70038,     0.70305,     0.70576,     0.70758,     0.71032,     0.71324,     0.71392,     0.71575,     0.71743,\n",
            "            0.71958,     0.72075,     0.72247,     0.72514,     0.72656,     0.72882,     0.73272,     0.73446,     0.73593,     0.73751,     0.73938,     0.74119,     0.74243,     0.74639,     0.74908,     0.75156,     0.75239,     0.75314,     0.75452,     0.75629,     0.75676,      0.7565,     0.75836,\n",
            "            0.76055,     0.76145,     0.76186,     0.76227,     0.76563,      0.7664,      0.7676,     0.76898,     0.77128,     0.77229,     0.77261,     0.77366,     0.77435,     0.77529,     0.77619,     0.77715,     0.77877,      0.7796,      0.7811,     0.78276,     0.78409,       0.784,     0.78483,\n",
            "            0.78668,     0.78697,      0.7882,     0.78968,     0.79145,      0.7916,     0.79176,     0.79191,     0.79184,     0.79356,      0.7939,     0.79478,     0.79667,     0.79725,     0.79798,     0.79839,     0.79893,     0.79877,     0.80035,     0.80018,      0.8003,     0.80141,     0.80243,\n",
            "            0.80294,     0.80335,     0.80392,     0.80751,     0.80743,     0.80727,     0.80857,     0.80918,     0.80967,     0.81027,     0.81056,     0.81344,     0.81444,     0.81495,     0.81663,       0.818,     0.81898,     0.82081,     0.82185,     0.82217,     0.82277,     0.82417,     0.82443,\n",
            "            0.82471,     0.82616,     0.82932,     0.82924,     0.83039,     0.83162,     0.83347,     0.83407,      0.8344,     0.83474,     0.83525,     0.83622,     0.83667,     0.83696,     0.83739,     0.83771,      0.8378,     0.83891,     0.84092,     0.84146,     0.84286,     0.84346,     0.84372,\n",
            "            0.84425,     0.84561,     0.84606,      0.8464,     0.84711,     0.84702,     0.84696,     0.84717,     0.84829,     0.84859,     0.84889,     0.84998,     0.85074,      0.8513,     0.85127,     0.85247,     0.85286,     0.85415,     0.85451,     0.85471,     0.85583,     0.85826,      0.8594,\n",
            "            0.85981,     0.86012,     0.86038,     0.86061,     0.86117,     0.86297,     0.86309,     0.86295,     0.86349,     0.86454,     0.86491,      0.8655,     0.86562,     0.86591,     0.86663,     0.86812,     0.86819,     0.86816,     0.86813,     0.86813,     0.86851,     0.86886,      0.8691,\n",
            "            0.86935,     0.86986,     0.87042,     0.87171,     0.87295,     0.87319,     0.87342,     0.87415,     0.87495,     0.87636,     0.87681,     0.87676,     0.87666,     0.87776,     0.87904,      0.8796,     0.88013,        0.88,     0.87996,     0.87993,     0.87999,     0.88022,     0.88046,\n",
            "            0.88055,      0.8823,     0.88353,     0.88432,     0.88456,     0.88481,      0.8851,      0.8854,     0.88558,     0.88554,     0.88549,     0.88541,     0.88636,     0.88682,     0.88748,     0.88743,     0.88948,     0.89032,     0.89068,     0.89145,     0.89178,     0.89219,     0.89312,\n",
            "            0.89331,     0.89366,     0.89374,     0.89391,     0.89463,     0.89635,     0.89649,      0.8968,     0.89721,     0.89718,     0.89714,     0.89711,     0.89707,     0.89703,     0.89701,     0.89698,     0.89707,     0.89752,     0.89763,     0.89825,     0.89956,     0.89978,     0.90041,\n",
            "            0.90275,     0.90336,     0.90485,     0.90491,      0.9055,     0.90545,     0.90613,     0.90684,     0.90719,      0.9076,     0.90836,     0.90834,     0.90831,     0.90827,     0.90825,     0.90856,     0.90887,     0.91001,     0.91047,     0.91223,     0.91281,     0.91278,     0.91274,\n",
            "            0.91272,     0.91271,      0.9127,     0.91268,     0.91267,     0.91286,     0.91313,      0.9134,     0.91342,     0.91375,     0.91409,     0.91411,      0.9145,     0.91491,     0.91569,     0.91565,     0.91677,     0.91743,     0.91796,     0.91883,     0.91891,     0.91916,      0.9194,\n",
            "            0.91961,     0.91958,     0.91956,     0.91954,     0.91952,     0.91951,      0.9195,     0.91948,     0.91947,     0.91929,     0.91926,     0.91962,        0.92,     0.92086,     0.92092,      0.9214,     0.92163,     0.92161,     0.92158,     0.92247,     0.92287,     0.92314,     0.92313,\n",
            "            0.92377,      0.9247,     0.92465,     0.92451,     0.92446,     0.92442,     0.92449,     0.92489,     0.92545,     0.92633,     0.92677,     0.92737,      0.9281,      0.9285,     0.92846,     0.93024,     0.93085,      0.9309,     0.93087,     0.93084,     0.93111,     0.93139,     0.93162,\n",
            "            0.93148,     0.93228,     0.93226,     0.93224,     0.93251,     0.93287,      0.9335,     0.93403,     0.93467,     0.93466,     0.93464,     0.93529,     0.93545,     0.93535,     0.93611,     0.93638,      0.9366,     0.93681,     0.93703,     0.93697,     0.93695,     0.93694,     0.93692,\n",
            "            0.93767,      0.9376,     0.93833,     0.93828,     0.93868,     0.93952,     0.94001,     0.93997,     0.94169,     0.94162,     0.94156,     0.94155,     0.94154,     0.94152,     0.94151,     0.94221,     0.94232,     0.94229,     0.94226,     0.94222,      0.9422,       0.943,     0.94298,\n",
            "            0.94294,     0.94367,     0.94466,     0.94519,      0.9456,     0.94591,     0.94621,     0.94664,     0.94723,     0.94724,      0.9472,     0.94716,     0.94714,     0.94713,     0.94704,     0.94813,     0.94869,     0.94909,     0.94943,     0.94976,     0.94968,     0.94964,     0.95053,\n",
            "            0.95084,     0.95214,     0.95232,      0.9523,     0.95224,     0.95222,     0.95219,     0.95265,       0.953,     0.95343,     0.95411,     0.95466,      0.9548,     0.95475,     0.95467,     0.95464,     0.95503,     0.95557,     0.95555,      0.9557,     0.95604,     0.95638,     0.95646,\n",
            "            0.95641,     0.95636,     0.95632,     0.95627,     0.95624,     0.95666,     0.95708,     0.95709,     0.95707,     0.95704,       0.957,     0.95714,     0.95728,     0.95743,     0.95757,     0.95772,     0.95787,     0.95787,     0.95783,     0.95828,     0.95876,     0.95973,      0.9597,\n",
            "            0.95968,     0.95965,     0.95962,     0.95958,     0.95975,     0.96036,     0.96044,     0.96041,     0.96033,     0.96021,      0.9602,      0.9605,      0.9608,     0.96111,     0.96107,     0.96105,     0.96104,     0.96103,       0.961,     0.96091,     0.96183,      0.9618,     0.96178,\n",
            "            0.96172,     0.96176,     0.96195,     0.96213,     0.96232,      0.9625,     0.96264,     0.96258,     0.96253,     0.96251,     0.96248,      0.9624,     0.96239,     0.96231,     0.96225,     0.96226,     0.96323,     0.96321,     0.96319,     0.96317,     0.96316,     0.96312,     0.96309,\n",
            "            0.96332,     0.96384,      0.9654,     0.96608,     0.96694,     0.96877,     0.96905,     0.97105,     0.97103,     0.97093,      0.9708,     0.97187,     0.97392,     0.97494,     0.97492,     0.97547,     0.97697,     0.97695,     0.97694,     0.97747,     0.97907,     0.97902,     0.97897,\n",
            "            0.97894,     0.97892,     0.97889,     0.97886,     0.97881,      0.9788,     0.97877,      0.9798,     0.97978,     0.97973,     0.97968,     0.97967,     0.97963,     0.97986,      0.9807,     0.98063,     0.98061,     0.98059,     0.98057,     0.98056,     0.98055,     0.98053,     0.98071,\n",
            "            0.98159,      0.9827,     0.98382,     0.98378,     0.98376,     0.98375,     0.98378,     0.98434,     0.98488,     0.98485,     0.98481,      0.9848,     0.98528,     0.98584,     0.98593,      0.9859,     0.98586,     0.98585,     0.98583,     0.98581,     0.98604,     0.98644,     0.98684,\n",
            "            0.98696,     0.98692,     0.98689,     0.98688,     0.98788,     0.98923,     0.98922,      0.9892,     0.99005,     0.99033,     0.99032,     0.99029,     0.99028,     0.99027,     0.99025,     0.99023,     0.99022,     0.99018,     0.99016,     0.99014,     0.99012,     0.99011,     0.99008,\n",
            "            0.99005,     0.99004,     0.99004,     0.99003,     0.99003,     0.99002,     0.99124,     0.99123,     0.99122,     0.99121,      0.9912,     0.99117,     0.99173,      0.9924,     0.99239,     0.99238,     0.99363,     0.99361,     0.99358,     0.99356,     0.99354,      0.9935,     0.99348,\n",
            "            0.99346,     0.99346,     0.99342,     0.99341,      0.9934,     0.99435,     0.99516,       0.996,     0.99599,     0.99599,     0.99598,     0.99598,     0.99595,     0.99594,     0.99594,     0.99593,     0.99592,     0.99591,      0.9959,     0.99588,     0.99588,     0.99587,     0.99586,\n",
            "            0.99586,     0.99585,     0.99585,     0.99583,     0.99581,     0.99579,     0.99578,     0.99577,     0.99584,     0.99667,     0.99716,     0.99714,     0.99724,     0.99839,     0.99856,     0.99856,     0.99855,     0.99854,     0.99853,     0.99853,     0.99852,     0.99852,     0.99851,\n",
            "            0.99851,     0.99851,      0.9985,     0.99849,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.93784,     0.93784,     0.93312,     0.93076,      0.9284,     0.92762,     0.92762,     0.92604,     0.92604,     0.92604,     0.92565,     0.92447,     0.92447,     0.92368,     0.92368,     0.92368,     0.92368,     0.92132,     0.91975,     0.91975,     0.91896,     0.91817,     0.91817,\n",
            "            0.91817,     0.91817,     0.91817,     0.91817,     0.91817,     0.91817,     0.91817,     0.91739,     0.91739,     0.91503,     0.91503,     0.91503,     0.91503,     0.91503,     0.91437,     0.91345,     0.91345,     0.91273,     0.91267,     0.91267,     0.91188,     0.91109,     0.91109,\n",
            "            0.91109,     0.91109,     0.91109,     0.91109,     0.90952,     0.90952,     0.90891,     0.90873,     0.90795,     0.90715,     0.90637,     0.90637,      0.9058,     0.90559,     0.90559,     0.90559,      0.9048,      0.9048,      0.9048,     0.90401,     0.90401,     0.90323,     0.90323,\n",
            "            0.90323,     0.90323,     0.90244,     0.90244,     0.90244,     0.90165,     0.90156,     0.90087,     0.90087,     0.90087,     0.90008,     0.89929,     0.89929,     0.89929,     0.89929,     0.89929,     0.89851,     0.89851,     0.89851,     0.89851,     0.89833,     0.89709,     0.89693,\n",
            "            0.89614,     0.89614,     0.89614,     0.89614,     0.89536,     0.89457,     0.89457,     0.89457,     0.89457,       0.893,     0.89289,     0.89142,     0.89097,     0.89064,     0.88906,     0.88896,     0.88828,     0.88828,     0.88718,      0.8867,     0.88573,      0.8853,     0.88513,\n",
            "            0.88493,     0.88358,     0.88356,     0.88356,     0.88356,     0.88356,     0.88356,     0.88356,     0.88289,     0.88277,     0.88277,     0.88277,     0.88277,     0.88277,     0.88264,     0.88198,     0.88158,     0.88073,     0.88041,     0.87903,     0.87805,     0.87805,     0.87805,\n",
            "            0.87805,     0.87805,     0.87805,     0.87798,     0.87753,      0.8766,     0.87648,     0.87648,     0.87569,     0.87569,     0.87569,     0.87479,     0.87411,     0.87411,     0.87411,     0.87411,     0.87411,     0.87411,     0.87411,     0.87411,     0.87411,     0.87404,     0.87333,\n",
            "            0.87175,     0.87175,     0.87163,     0.87111,     0.87055,     0.87018,     0.87018,     0.87018,     0.87018,     0.87018,     0.86956,     0.86939,     0.86939,     0.86939,     0.86939,      0.8691,     0.86861,     0.86861,     0.86861,     0.86782,     0.86782,     0.86782,     0.86782,\n",
            "            0.86782,     0.86782,     0.86782,     0.86782,     0.86752,     0.86688,      0.8665,     0.86625,     0.86625,     0.86625,     0.86625,     0.86625,     0.86548,     0.86484,     0.86389,     0.86389,     0.86194,     0.86153,     0.86153,     0.86087,     0.85995,     0.85995,     0.85995,\n",
            "            0.85995,     0.85995,     0.85995,     0.85981,     0.85917,     0.85917,     0.85807,     0.85702,     0.85681,     0.85681,     0.85638,     0.85561,     0.85523,     0.85523,     0.85523,     0.85523,     0.85504,     0.85484,     0.85464,     0.85445,     0.85445,     0.85445,     0.85445,\n",
            "            0.85445,     0.85445,     0.85366,     0.85287,     0.85287,     0.85287,     0.85287,      0.8513,      0.8513,      0.8513,     0.85119,     0.85079,     0.85002,     0.84972,     0.84972,     0.84972,     0.84922,     0.84814,     0.84784,     0.84755,     0.84736,     0.84736,     0.84736,\n",
            "             0.8468,     0.84658,     0.84658,     0.84658,     0.84658,     0.84658,     0.84658,     0.84658,     0.84646,     0.84611,     0.84572,     0.84504,       0.845,     0.84457,     0.84396,     0.84357,     0.84343,     0.84308,     0.84264,     0.84264,     0.84186,     0.84186,     0.84156,\n",
            "            0.84107,     0.84107,     0.84047,      0.8395,     0.83871,     0.83871,     0.83818,     0.83792,     0.83784,     0.83753,     0.83722,     0.83689,     0.83655,     0.83625,       0.836,     0.83575,     0.83556,     0.83556,     0.83399,     0.83399,     0.83399,     0.83352,     0.83227,\n",
            "            0.83163,     0.83163,     0.83163,     0.83111,     0.82925,     0.82876,     0.82848,     0.82718,     0.82691,     0.82691,     0.82671,     0.82643,     0.82616,     0.82574,     0.82533,     0.82533,     0.82533,     0.82533,     0.82533,     0.82533,     0.82375,     0.82334,     0.82296,\n",
            "            0.82281,     0.82266,     0.82252,     0.82237,     0.82222,     0.82219,     0.82219,     0.82219,      0.8214,      0.8214,      0.8214,     0.82064,     0.82061,     0.82061,     0.82033,     0.81987,     0.81983,     0.81983,     0.81983,     0.81938,     0.81904,     0.81904,     0.81904,\n",
            "              0.819,     0.81873,     0.81845,     0.81821,     0.81806,     0.81792,     0.81777,     0.81762,     0.81747,     0.81555,     0.81513,     0.81511,     0.81511,     0.81477,     0.81432,     0.81432,     0.81419,     0.81395,      0.8137,     0.81353,     0.81353,     0.81273,     0.81196,\n",
            "            0.81196,     0.81162,     0.81104,      0.8094,     0.80878,     0.80835,     0.80803,     0.80803,     0.80803,     0.80803,     0.80803,     0.80803,     0.80803,     0.80717,      0.8067,     0.80645,     0.80645,     0.80554,     0.80513,     0.80409,     0.80409,     0.80409,       0.804,\n",
            "            0.80222,     0.80157,     0.80129,     0.80102,     0.80094,     0.80094,     0.80094,     0.79937,     0.79937,     0.79906,     0.79875,     0.79858,     0.79809,     0.79676,     0.79622,     0.79622,     0.79622,     0.79622,     0.79622,     0.79531,      0.7951,      0.7949,      0.7947,\n",
            "            0.79302,     0.79201,     0.79012,     0.78948,     0.78914,     0.78914,     0.78897,     0.78836,     0.78779,     0.78679,     0.78592,     0.78575,     0.78558,     0.78541,     0.78524,     0.78442,     0.78407,     0.78368,     0.78321,     0.78259,     0.78127,     0.78103,     0.78072,\n",
            "            0.78014,     0.77772,     0.77734,     0.77734,     0.77734,     0.77734,     0.77734,     0.77734,     0.77734,     0.77688,     0.77631,      0.7757,     0.77542,     0.77514,     0.77378,     0.77341,     0.77341,     0.77341,     0.77341,     0.77336,     0.77217,     0.77145,     0.77102,\n",
            "            0.77026,     0.77026,     0.76998,     0.76963,     0.76868,     0.76828,     0.76787,     0.76711,      0.7658,     0.76554,     0.76554,     0.76554,     0.76451,     0.76358,      0.7622,     0.76176,     0.76161,     0.76155,     0.76107,     0.76082,     0.76082,     0.76082,     0.76042,\n",
            "            0.75962,     0.75862,       0.758,     0.75694,     0.75531,     0.75531,     0.75531,     0.75457,     0.75415,     0.75375,     0.75301,     0.75295,     0.75295,     0.75295,     0.75295,     0.75295,     0.75295,     0.75123,     0.75063,     0.75059,     0.75059,     0.75003,     0.74952,\n",
            "            0.74912,      0.7485,     0.74786,     0.74714,     0.74587,     0.74587,     0.74503,     0.74438,      0.7428,      0.7405,     0.73958,     0.73958,     0.73958,     0.73958,     0.73801,     0.73775,     0.73751,     0.73726,     0.73668,     0.73499,     0.73351,     0.73292,     0.73255,\n",
            "             0.7314,     0.73013,     0.73013,     0.73013,     0.73013,     0.73013,     0.72979,     0.72858,     0.72752,     0.72725,     0.72664,     0.72507,     0.72468,      0.7231,     0.72189,     0.72148,     0.72143,     0.72091,     0.72052,     0.72022,     0.71992,     0.71907,     0.71855,\n",
            "            0.71755,     0.71755,     0.71755,     0.71706,     0.71676,     0.71518,      0.7144,     0.71262,     0.71208,     0.70954,     0.70614,     0.70574,     0.70511,     0.70387,     0.70335,     0.70102,      0.7009,      0.7004,     0.69988,     0.69945,     0.69912,     0.69748,     0.69575,\n",
            "            0.69499,     0.69421,     0.69313,     0.69214,     0.69047,     0.69008,     0.68926,     0.68678,     0.68631,     0.68463,     0.68292,      0.6823,     0.68104,     0.68057,     0.67963,     0.67728,     0.67655,     0.67575,     0.67503,     0.67465,     0.67426,     0.67354,     0.67191,\n",
            "            0.67117,     0.67036,     0.66993,     0.66819,     0.66745,      0.6669,      0.6664,      0.6664,     0.66639,     0.66488,     0.66306,     0.66255,     0.66247,     0.66247,     0.66149,     0.66004,     0.65831,     0.65763,     0.65706,     0.65579,     0.65539,     0.65539,     0.65539,\n",
            "            0.65491,     0.65281,     0.65171,      0.6509,     0.65067,      0.6505,     0.65001,     0.64839,     0.64673,     0.64481,     0.64378,     0.64193,     0.64097,     0.64047,      0.6391,     0.63792,     0.63732,     0.63475,      0.6333,     0.63212,     0.63102,     0.62985,     0.62841,\n",
            "            0.62623,      0.6257,     0.62535,     0.62511,     0.62487,     0.62449,     0.62308,     0.62263,     0.62203,     0.62073,     0.62011,     0.61812,     0.61684,     0.61659,     0.61541,     0.61447,     0.61321,      0.6113,     0.60871,     0.60658,     0.60519,     0.60148,     0.59928,\n",
            "            0.59804,     0.59714,     0.59385,     0.59302,     0.59202,     0.59087,     0.58851,     0.58743,     0.58659,     0.58615,     0.58505,     0.58448,     0.58072,     0.57962,     0.57924,     0.57807,     0.57591,     0.57542,     0.57341,     0.57093,     0.57041,     0.56883,     0.56839,\n",
            "            0.56788,      0.5669,     0.56592,     0.56367,     0.56083,     0.55849,     0.55698,     0.55589,     0.55389,     0.55389,     0.55338,     0.54879,      0.5476,      0.5476,     0.54644,     0.54514,      0.5405,     0.53899,      0.5346,     0.53311,     0.53164,     0.53003,      0.5288,\n",
            "            0.52676,     0.52595,      0.5248,     0.52076,     0.51849,     0.51676,     0.51627,      0.5155,      0.5135,     0.51264,      0.5116,     0.50875,     0.50611,     0.50284,     0.50052,      0.4979,     0.49399,     0.49148,     0.49091,     0.49024,     0.48943,     0.48634,     0.48412,\n",
            "            0.48071,     0.47911,     0.47751,     0.47458,     0.47222,     0.47085,       0.467,     0.46478,     0.46246,     0.45872,      0.4552,     0.44801,     0.44438,     0.44288,      0.4397,     0.43548,     0.43191,     0.42913,     0.42399,     0.42035,     0.41662,     0.41481,      0.4116,\n",
            "             0.4043,     0.40096,     0.39863,     0.39762,     0.39438,      0.3924,     0.38899,     0.38313,     0.38127,     0.38017,     0.37591,     0.37319,      0.3723,     0.36811,     0.36409,     0.35965,     0.34959,     0.34655,     0.34542,     0.34389,     0.34187,     0.33831,     0.33494,\n",
            "            0.33154,     0.32634,     0.32457,     0.31926,     0.31513,     0.30968,     0.30832,     0.30505,     0.30248,     0.30085,     0.29693,     0.29379,     0.28971,      0.2853,     0.28027,      0.2765,     0.27358,     0.27046,     0.26868,     0.26567,     0.26213,     0.25796,     0.25328,\n",
            "            0.24799,     0.24435,     0.24199,     0.23614,     0.23191,     0.23023,       0.228,     0.22247,     0.21916,     0.21617,     0.21458,     0.20992,     0.20889,     0.20583,     0.20343,     0.19814,     0.19638,     0.19388,     0.18891,     0.18458,     0.18098,     0.17552,     0.17045,\n",
            "            0.16831,     0.16509,     0.16307,     0.15958,     0.15456,     0.14839,     0.14477,     0.14003,     0.13687,     0.13285,     0.12684,     0.12138,     0.11897,     0.11516,     0.10915,     0.10341,    0.099834,    0.097062,    0.094805,    0.090029,    0.088825,    0.086354,    0.081601,\n",
            "           0.078321,    0.072336,    0.071704,     0.06997,    0.067966,     0.06485,    0.061969,    0.059974,    0.055124,    0.053446,    0.052584,    0.051382,    0.050821,     0.04995,    0.048824,    0.046898,    0.044793,    0.043755,    0.042991,    0.042545,    0.040791,    0.038239,    0.037373,\n",
            "           0.035912,    0.035073,    0.034118,    0.032825,    0.031975,    0.031159,     0.03059,      0.0301,    0.028758,    0.028139,    0.027855,    0.027571,    0.025509,    0.023963,     0.02182,    0.021022,    0.020684,    0.018566,    0.017754,    0.017342,    0.016412,    0.015988,    0.014466,\n",
            "           0.013707,    0.012245,    0.011295,     0.01089,    0.010691,    0.010492,    0.010293,   0.0092626,    0.008763,   0.0074978,   0.0062785,   0.0060943,     0.00591,   0.0057257,   0.0055415,   0.0048785,   0.0042784,   0.0038957,   0.0037951,   0.0036945,   0.0035939,   0.0034933,   0.0033928,\n",
            "          0.0032922,   0.0031916,   0.0015055,   0.0013336,   0.0011617,  0.00098971,  0.00081777,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
            "fitness: np.float64(0.5453828711918451)\n",
            "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
            "maps: array([    0.50553])\n",
            "names: {0: 'person'}\n",
            "nt_per_class: array([1271])\n",
            "nt_per_image: array([423])\n",
            "results_dict: {'metrics/precision(B)': np.float64(0.905445428294539), 'metrics/recall(B)': np.float64(0.8287565305487659), 'metrics/mAP50(B)': np.float64(0.9040697722534584), 'metrics/mAP50-95(B)': np.float64(0.5055287710738882), 'fitness': np.float64(0.5453828711918451)}\n",
            "save_dir: PosixPath('runs_finetune/person_yolov8n42')\n",
            "speed: {'preprocess': 0.27402932362239757, 'inference': 0.9507557538799042, 'loss': 0.0006324205038930982, 'postprocess': 1.0993306860380176}\n",
            "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
            "task: 'detect'\n"
          ]
        }
      ],
      "source": [
        "# Evaluates the trained model using the TEST SET defined in data.yaml\n",
        "\n",
        "metrics_YOLOv8n = model_YOLOv8n.val(data='data.yaml', split='test') # returns accuracy metrics (e.g., mAP, precision, recall, etc.) on the test set\n",
        "print(\"Test metrics:\", metrics_YOLOv8n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmdyUMPKd_V8"
      },
      "source": [
        "Confidence (confidence) is the probability estimated by the model that a detected object is actually real (i.e., not a false positive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2s3bR6w3Yjv",
        "outputId": "408473d3-b100-4ac9-ac3b-dc6d0b3f640c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#  Inference (practical use of the fine-tuned model)\n",
        "\n",
        "# Load the best weights found\n",
        "best_YOLOv8n = '/content/runs_finetune/person_yolov8n2/weights/best.pt'\n",
        "model_inf_YOLOv8n = YOLO(best_YOLOv8n) # Creates a new model instance by loading the best weights\n",
        "\n",
        "# Performs inference on one or more images, or on a video, by specifying the path in the source parameter.\n",
        "# conf=0.25 → Confidence threshold for considering a detection valid.\n",
        "preds_YOLOv8n = model_inf_YOLOv8n.predict(\n",
        "  # source='/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/images',\n",
        "  source='/content/AERALIS_YOLOv8n_local/test/images',\n",
        "  conf=0.25,\n",
        "  verbose=False, # disable on-screen printing\n",
        "  save=True\n",
        ")\n",
        "\n",
        "# Too many images to show!\n",
        "# for result in preds_YOLOv8n:\n",
        "#    result.show() # displays the predictions (eventually you can also save them)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJS_ZLwznJS9"
      },
      "source": [
        "In order to use it on the Jetson Nano we now want to download the model, so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vk8Pg-kKMGW"
      },
      "outputs": [],
      "source": [
        "# Create folders on Google Drive (first time only):\n",
        "!mkdir -p /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/results/annotated\n",
        "!mkdir -p /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbYQ5wv5Kac9"
      },
      "outputs": [],
      "source": [
        "# Copy annotated images:\n",
        "!cp -r /content/runs/detect/predict/* /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/results/annotated/\n",
        "\n",
        "# Copy the best weights (best.pt) after training:\n",
        "!cp /content/runs_finetune/person_yolov8n2/weights/best.pt /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-qaF-hFnTnC"
      },
      "outputs": [],
      "source": [
        "# To download it directly to the computer:\n",
        "from google.colab import files\n",
        "files.download('/content/runs_finetune/person_yolov8n/weights/best.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veHRWTjpnk34"
      },
      "source": [
        "When we have the .pt file on the PC, we could:\n",
        "- Upload it to the Jetson Nano,\n",
        "- Convert it to ONNX/TensorRT if you need it for optimization,\n",
        "- Use it with the PyTorch/Ultralytics version on any computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQv-8zHUOLde"
      },
      "source": [
        "Weight access code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbx4l9dVOKhY"
      },
      "outputs": [],
      "source": [
        "# Path of saved weights:\n",
        "weights_path_YOLOv8n = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/weights/best.pt'\n",
        "model_YOLOv8n = YOLO(weights_path_YOLOv8n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp2RfzXhOO1U"
      },
      "source": [
        "Access code for annotated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU2quOVVSu0A"
      },
      "outputs": [],
      "source": [
        "# Directory of annotated images\n",
        "annotated_dir = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/results/annotated'\n",
        "\n",
        "# Get all the .jpg files in the directory\n",
        "annotated_imgs_YOLOv8n = [\n",
        "  os.path.join(annotated_dir, f)\n",
        "  for f in os.listdir(annotated_dir)\n",
        "  if f.lower().endswith('.jpg')\n",
        "]\n",
        "\n",
        "# View the first 5 annotated images\n",
        "for img_path in annotated_imgs_YOLOv8n[:5]:\n",
        "  img = Image.open(img_path)\n",
        "  img.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n4EVx_z57Io"
      },
      "source": [
        "### YOLOv11n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfcXa428v5ux"
      },
      "source": [
        "Ora eseguiamo lo stesso procedimento per YOLOv11n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBAol2_a3F4d",
        "outputId": "1d635f83-43f1-4f8d-ca77-dda746bdee86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copy completed: True\n"
          ]
        }
      ],
      "source": [
        "src = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n'\n",
        "dst = '/content/AERALIS_YOLOv11n_local'  # is now on the local VM, NOT on drive\n",
        "\n",
        "# If the destination folder already exists, I delete it\n",
        "if os.path.exists(dst):\n",
        "  shutil.rmtree(dst)\n",
        "\n",
        "# Recursive copy of ENTIRE folder (and subfolders)\n",
        "shutil.copytree(src, dst)\n",
        "print(\"Copy completed:\", os.path.exists(dst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO8YB8G43Kkr",
        "outputId": "811954f0-bb62-485c-8bf2-8f8e04e4875d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   55G   58G  49% /\n"
          ]
        }
      ],
      "source": [
        "!df -h / # It shows the total, used and free space on the root (/) of the Colab VM.\n",
        "\n",
        "# Avail column: space still available for your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbpl3LPY3NJV",
        "outputId": "9d5cd061-5ad0-49dc-ab1a-1c4ac35ce250"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.6G\t/content/AERALIS_YOLOv11n_local\n"
          ]
        }
      ],
      "source": [
        "# Show space used by your local folder\n",
        "!du -sh /content/AERALIS_YOLOv11n_local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmqFX4xo3P5E",
        "outputId": "29a21efc-8dd0-42b2-a051-35a607d1ed52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "140K\t/content/.config\n",
            "6.6G\t/content/AERALIS_YOLOv11n_local\n",
            "du: cannot access '/content/drive/.Encrypted/.shortcut-targets-by-id/1LQbD7p_iS5KLqGNdfrYEvsAx0i_bgB0h/projectUPV': No such file or directory\n",
            "69G\t/content/drive\n",
            "55M\t/content/sample_data\n",
            "75G\t/content/\n"
          ]
        }
      ],
      "source": [
        "# Show space occupied by various folders in /content/.\n",
        "!du -h --max-depth=1 /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7CtXM7O3Uu9",
        "outputId": "cd8d3ea2-f93c-4b8e-8271-5e4392a0dddf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YAML dataset (edit routes)\n",
        "data_yaml = \"\"\"\n",
        "train: /content/AERALIS_YOLOv11n_local/train/images\n",
        "val:   /content/AERALIS_YOLOv11n_local/val/images\n",
        "test:  /content/AERALIS_YOLOv11n_local/test/images\n",
        "\n",
        "nc: 1\n",
        "names: ['person']\n",
        "\"\"\"\n",
        "open('data.yaml', 'w').write(data_yaml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGDA_FB5zV1F",
        "outputId": "171ccaad-9e99-4005-96bb-a79611b055ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 78.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Upload the pre-trained model we want to use as a starting point\n",
        "\n",
        "model_YOLOv11n = YOLO('yolo11n.pt') # it is the model that will be fine-tuned on the custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo3UQF95vbjy",
        "outputId": "c2848ea3-3ee4-40fe-b805-d6fe51e36a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# To see the available GPU\n",
        "import torch\n",
        "print(torch.cuda.is_available()) # True = you have GPU --> if False then use device='cpu'\n",
        "print(torch.cuda.device_count()) # Name of GPU\n",
        "\n",
        "# If True and at least 1, you can use device=0.\n",
        "# If you don't have GPU: use device='cpu' (much slower).\n",
        "# Locally (not Colab): check with nvidia-smi from terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBBPkeikYhDS",
        "outputId": "8054841d-ec1f-4a04-d3c4-5d0d2193e957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=person_yolov11n, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs_finetune, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs_finetune/person_yolov11n, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 19.5MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2916.3±1744.5 MB/s, size: 2026.4 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/AERALIS_YOLOv11n_local/train/labels... 2395 images, 388 backgrounds, 0 corrupt: 100%|██████████| 2395/2395 [00:01<00:00, 1677.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/AERALIS_YOLOv11n_local/train/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 4029.4±2087.3 MB/s, size: 2606.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/AERALIS_YOLOv11n_local/val/labels... 515 images, 75 backgrounds, 0 corrupt: 100%|██████████| 515/515 [00:00<00:00, 1544.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/AERALIS_YOLOv11n_local/val/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs_finetune/person_yolov11n/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns_finetune/person_yolov11n\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      1/100      2.35G      2.228       3.31      1.089         61        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:08<00:00,  1.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.567      0.447      0.451      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      2/100      2.44G      2.186      1.962      1.079         52        640: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.575      0.378      0.422      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      3/100      2.46G      2.179      1.566      1.088         50        640: 100%|██████████| 150/150 [01:13<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.669        0.5      0.537      0.208\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      4/100      2.47G      2.109      1.435      1.065         39        640: 100%|██████████| 150/150 [01:13<00:00,  2.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258        0.7      0.508       0.55      0.222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      5/100      2.49G      2.076      1.335      1.056         45        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.713      0.541       0.58      0.236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      6/100       2.5G      2.024      1.253      1.047         37        640: 100%|██████████| 150/150 [01:15<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.704       0.56      0.594      0.253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      7/100      2.52G      1.993      1.258      1.039         47        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.694       0.49      0.542      0.223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      8/100      2.53G      1.977      1.192      1.023         39        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.721      0.561      0.602      0.248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      9/100      2.55G      1.922      1.139      1.003         40        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.823      0.584      0.659      0.296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     10/100      2.56G      1.894        1.1      1.005         61        640: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.787       0.58      0.651      0.287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     11/100      2.58G      1.891      1.107     0.9989         40        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.786      0.591      0.647      0.292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     12/100      2.59G      1.845      1.073     0.9877         42        640: 100%|██████████| 150/150 [01:16<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.78        0.6      0.668      0.303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     13/100      2.61G      1.833      1.058     0.9868         47        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.745      0.593      0.641      0.294\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     14/100      2.62G      1.818      1.042     0.9823         36        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.772      0.626      0.685      0.303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     15/100      2.64G      1.818      1.024     0.9825         61        640: 100%|██████████| 150/150 [01:08<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.769        0.6      0.665      0.309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     16/100      2.65G      1.793      1.017     0.9739         41        640: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.814      0.605      0.698      0.337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     17/100      2.67G      1.776      1.001     0.9732         61        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.85      0.617      0.704      0.333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     18/100      2.68G      1.784     0.9895     0.9729         65        640: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.794      0.617      0.691      0.325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     19/100       2.7G      1.765     0.9795     0.9651         66        640: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.789      0.616      0.677      0.322\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     20/100      2.71G      1.746     0.9641     0.9622         29        640: 100%|██████████| 150/150 [01:13<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.801      0.628      0.693      0.324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     21/100      2.73G      1.743     0.9615     0.9586         65        640: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.816      0.626      0.721       0.34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     22/100      2.74G      1.736     0.9486     0.9558         39        640: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.82      0.621      0.703      0.347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     23/100      2.75G      1.699     0.9304      0.953         42        640: 100%|██████████| 150/150 [01:10<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.805      0.656      0.724      0.356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     24/100      2.77G      1.678     0.9007      0.954         57        640: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.808      0.656       0.74      0.363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     25/100      2.79G      1.675     0.9041     0.9448         44        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.815      0.648      0.725      0.352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     26/100       2.8G      1.682     0.9133     0.9542         65        640: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.827      0.653      0.726      0.365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     27/100      2.81G      1.683     0.9205      0.949         52        640: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.829      0.651      0.733      0.349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     28/100      2.83G      1.666     0.8888     0.9442         42        640: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.835      0.657       0.74      0.368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     29/100      2.84G      1.667     0.8914     0.9459         55        640: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.856      0.615      0.729      0.356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     30/100      2.86G      1.632     0.8742     0.9386         36        640: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.838      0.665      0.737      0.351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     31/100      2.87G      1.633     0.8677     0.9398         49        640: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.827      0.679      0.749      0.366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     32/100      2.89G      1.627     0.8686     0.9434         56        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.818      0.645      0.712      0.356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     33/100       2.9G      1.643     0.8805     0.9426         44        640: 100%|██████████| 150/150 [01:14<00:00,  2.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.825      0.686      0.757      0.373\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     34/100      2.92G      1.585     0.8505     0.9295         55        640: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.827      0.693      0.764      0.377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     35/100      2.93G      1.616     0.8578     0.9349         31        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.868      0.672      0.768      0.386\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     36/100      2.95G      1.611     0.8434     0.9296         32        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.881      0.695      0.784      0.397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     37/100      2.96G      1.585     0.8318      0.927         47        640: 100%|██████████| 150/150 [01:12<00:00,  2.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873      0.696      0.788      0.399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     38/100      2.98G      1.543     0.8215     0.9148         47        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.86      0.703      0.788      0.399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     39/100      2.99G      1.559     0.8074     0.9206         68        640: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.836      0.702      0.782      0.384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     40/100      3.01G      1.568     0.8212     0.9258         42        640: 100%|██████████| 150/150 [01:16<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.838      0.702      0.776      0.403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     41/100      3.02G      1.541     0.8131     0.9252         40        640: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.85      0.693      0.781      0.403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     42/100      3.04G      1.544     0.8065     0.9149         52        640: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.863      0.718      0.793      0.403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     43/100      3.05G      1.568     0.8091     0.9157         44        640: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.847      0.724        0.8      0.408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     44/100      3.07G      1.559     0.8103     0.9204        102        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.881      0.713      0.797      0.409\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     45/100      3.08G      1.539     0.7987     0.9149         66        640: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.858      0.734      0.809      0.403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     46/100       3.1G       1.53      0.787      0.914         31        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882      0.712      0.794      0.402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     47/100      3.11G      1.514      0.794     0.9142         41        640: 100%|██████████| 150/150 [01:09<00:00,  2.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.896      0.721      0.812      0.413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     48/100      3.12G      1.515     0.7785     0.9114         40        640: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.867      0.707      0.806      0.412\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     49/100      3.14G      1.501     0.7769     0.9083         34        640: 100%|██████████| 150/150 [01:10<00:00,  2.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.895      0.719      0.813      0.419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     50/100      3.15G      1.493     0.7717     0.9041         24        640: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.86      0.742      0.815      0.421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     51/100      3.17G       1.46     0.7535     0.9024         59        640: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873       0.72        0.8       0.42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     52/100      3.19G      1.497     0.7668     0.9075         31        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882      0.713      0.806      0.428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     53/100       3.2G      1.479      0.756     0.8978         51        640: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.857      0.729      0.805      0.417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     54/100      3.21G      1.501     0.7605     0.9015         50        640: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.874      0.734      0.813      0.419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     55/100      3.23G       1.47      0.751      0.905         29        640: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.87      0.738      0.809      0.422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     56/100      3.25G       1.46     0.7459     0.9024         44        640: 100%|██████████| 150/150 [01:09<00:00,  2.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.884      0.721      0.822      0.433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     57/100      3.26G      1.462     0.7434     0.9085         36        640: 100%|██████████| 150/150 [01:05<00:00,  2.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.914      0.736       0.83      0.438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     58/100      3.27G      1.426     0.7235     0.8973         80        640: 100%|██████████| 150/150 [01:08<00:00,  2.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.872      0.756      0.833      0.432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     59/100      3.29G      1.457     0.7424      0.898         29        640: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.738      0.821      0.435\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     60/100       3.3G      1.431     0.7284     0.9055         60        640: 100%|██████████| 150/150 [01:07<00:00,  2.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.869       0.75      0.824      0.429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     61/100      3.32G      1.445     0.7245     0.9006         41        640: 100%|██████████| 150/150 [01:15<00:00,  1.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.877      0.757      0.828      0.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     62/100      3.33G      1.438     0.7284     0.8974         32        640: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.903      0.748       0.83      0.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     63/100      3.35G      1.401     0.7099     0.8905         64        640: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.871      0.752       0.83      0.433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     64/100      3.36G      1.428     0.7143     0.8919         63        640: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.749      0.828      0.436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     65/100      3.38G      1.396     0.7025     0.8887         43        640: 100%|██████████| 150/150 [01:12<00:00,  2.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.87       0.76      0.827      0.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     66/100      3.39G      1.397     0.6991     0.8898         72        640: 100%|██████████| 150/150 [01:10<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.875      0.767       0.84      0.436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     67/100      3.41G      1.396     0.6988     0.8843         56        640: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.896       0.73      0.829      0.437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     68/100      3.52G      1.411      0.706     0.8928         39        640: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.862      0.757      0.832      0.442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     69/100      3.54G      1.381     0.6903     0.8856         55        640: 100%|██████████| 150/150 [01:07<00:00,  2.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.855      0.767      0.837      0.448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     70/100      3.55G      1.365     0.6799     0.8802         28        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.86       0.77      0.838      0.448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     71/100      3.57G      1.378     0.6846     0.8813         22        640: 100%|██████████| 150/150 [01:08<00:00,  2.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.878      0.754      0.837      0.443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     72/100      3.58G      1.371     0.6867     0.8768         58        640: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873      0.743      0.831      0.437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     73/100       3.6G      1.365     0.6794     0.8799         50        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.888      0.749      0.835      0.442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     74/100      3.61G      1.372     0.6757     0.8817         49        640: 100%|██████████| 150/150 [01:11<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.748       0.84      0.452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     75/100      3.63G      1.364     0.6763     0.8801         43        640: 100%|██████████| 150/150 [01:11<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258        0.9      0.756      0.844      0.455\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     76/100      3.64G      1.348     0.6725     0.8807         31        640: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886       0.75      0.839      0.453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     77/100      3.66G      1.368     0.6758     0.8816         45        640: 100%|██████████| 150/150 [01:07<00:00,  2.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.895      0.761      0.842      0.461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     78/100      3.67G      1.359     0.6749     0.8766         29        640: 100%|██████████| 150/150 [01:07<00:00,  2.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.885      0.762      0.853      0.466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     79/100      3.69G       1.33     0.6528     0.8795         44        640: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.861      0.779      0.845      0.457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     80/100       3.7G      1.333     0.6557      0.877         42        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.891       0.75      0.846      0.462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     81/100      3.72G      1.323     0.6499     0.8796         65        640: 100%|██████████| 150/150 [01:15<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.89      0.782      0.853      0.462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     82/100      3.73G      1.308     0.6427      0.871         49        640: 100%|██████████| 150/150 [01:11<00:00,  2.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.881      0.777      0.856      0.466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     83/100      3.75G      1.336     0.6552     0.8717         40        640: 100%|██████████| 150/150 [01:09<00:00,  2.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.892      0.769       0.85      0.467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     84/100      3.76G      1.327     0.6514     0.8741         66        640: 100%|██████████| 150/150 [01:08<00:00,  2.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882       0.77      0.849      0.465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     85/100      3.78G      1.308     0.6499     0.8738         40        640: 100%|██████████| 150/150 [01:12<00:00,  2.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.896      0.762      0.849      0.465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     86/100      3.79G      1.298     0.6292     0.8691         50        640: 100%|██████████| 150/150 [01:13<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.892      0.762      0.847      0.464\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     87/100       3.8G      1.311     0.6404     0.8697         43        640: 100%|██████████| 150/150 [01:13<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.888      0.762      0.847      0.468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     88/100      3.82G        1.3      0.639     0.8654         42        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.888      0.775      0.855      0.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     89/100      3.83G      1.313     0.6388     0.8733         43        640: 100%|██████████| 150/150 [01:10<00:00,  2.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.907      0.761      0.861      0.464\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     90/100      3.85G      1.278     0.6316     0.8672         28        640: 100%|██████████| 150/150 [01:09<00:00,  2.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.778      0.859      0.479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     91/100      3.86G      1.308      0.642     0.8773         22        640: 100%|██████████| 150/150 [01:18<00:00,  1.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258       0.88      0.781      0.856      0.467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     92/100      3.88G      1.287     0.6271     0.8715         36        640: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.878      0.775      0.848      0.464\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     93/100      3.89G       1.26     0.6184      0.867         23        640: 100%|██████████| 150/150 [01:15<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882      0.777      0.856      0.473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     94/100      3.91G      1.247     0.6136     0.8653         35        640: 100%|██████████| 150/150 [01:13<00:00,  2.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.886      0.772      0.854      0.467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     95/100      3.92G      1.252     0.6071     0.8662         23        640: 100%|██████████| 150/150 [01:16<00:00,  1.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.884      0.771      0.855      0.471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     96/100      3.94G      1.262     0.6115     0.8725         18        640: 100%|██████████| 150/150 [01:15<00:00,  1.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.878      0.782      0.852      0.472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     97/100      3.95G      1.236     0.5945     0.8627         21        640: 100%|██████████| 150/150 [01:14<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.877      0.789      0.857      0.475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     98/100      3.97G      1.212     0.5846     0.8572         32        640: 100%|██████████| 150/150 [01:16<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.873       0.79      0.858      0.477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "     99/100      3.98G      1.212     0.5884     0.8579         29        640: 100%|██████████| 150/150 [01:17<00:00,  1.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.882      0.783      0.858      0.477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    100/100         4G       1.22     0.5835     0.8653         15        640: 100%|██████████| 150/150 [01:15<00:00,  2.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.877      0.786      0.858      0.479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "100 epochs completed in 2.197 hours.\n",
            "Optimizer stripped from runs_finetune/person_yolov11n/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from runs_finetune/person_yolov11n/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating runs_finetune/person_yolov11n/weights/best.pt...\n",
            "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:07<00:00,  2.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        515       1258      0.887      0.778      0.859      0.479\n",
            "Speed: 0.1ms preprocess, 1.3ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns_finetune/person_yolov11n\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Fine‑tuning\n",
        "results_YOLOv11n = model_YOLOv11n.train(\n",
        "  data='data.yaml',\n",
        "  epochs=100,\n",
        "  imgsz=640,\n",
        "  batch=16,\n",
        "  patience=20,\n",
        "  workers=2,\n",
        "  device=0,\n",
        "  project='runs_finetune',\n",
        "  name='person_yolov11n'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg0cTtIOYg_7",
        "outputId": "48da4381-6a6e-4973-e4dd-87059d5dde9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.168 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3435.2±1553.1 MB/s, size: 1605.8 KB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/AERALIS_YOLOv11n_local/test/labels... 516 images, 93 backgrounds, 0 corrupt: 100%|██████████| 516/516 [00:00<00:00, 1565.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/AERALIS_YOLOv11n_local/test/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 33/33 [00:15<00:00,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all        516       1271      0.885      0.808      0.882      0.486\n",
            "Speed: 0.4ms preprocess, 1.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns_finetune/person_yolov11n2\u001b[0m\n",
            "Test metrics: ultralytics.utils.metrics.DetMetrics object with attributes:\n",
            "\n",
            "ap_class_index: array([0])\n",
            "box: ultralytics.utils.metrics.Metric object\n",
            "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7ba49076b190>\n",
            "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
            "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,\n",
            "            0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,\n",
            "            0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99792,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,\n",
            "            0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,\n",
            "            0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99628,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,     0.99463,\n",
            "            0.99463,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,\n",
            "            0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,\n",
            "            0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99381,     0.99241,\n",
            "            0.99241,     0.99241,     0.99241,     0.99241,     0.99241,     0.99241,     0.99241,     0.99241,     0.99241,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,\n",
            "            0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99143,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,\n",
            "            0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.99033,     0.98907,     0.98907,     0.98907,     0.98907,     0.98907,     0.98907,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,\n",
            "             0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,      0.9881,     0.98693,     0.98693,     0.98693,     0.98693,     0.98693,     0.98693,     0.98595,     0.98595,     0.98595,     0.98595,\n",
            "            0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98595,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98496,     0.98383,     0.98383,     0.98383,\n",
            "            0.98383,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97939,     0.97849,     0.97849,     0.97849,     0.97849,     0.97849,     0.97849,     0.97849,     0.97849,\n",
            "            0.97738,     0.97738,     0.97653,     0.97653,     0.97653,     0.97653,     0.97653,     0.97653,     0.97653,     0.97653,     0.97541,      0.9743,     0.97326,     0.97326,     0.97216,     0.97106,     0.96907,     0.96907,     0.96907,     0.96907,     0.96907,     0.96829,     0.96829,\n",
            "            0.96829,     0.96829,     0.96829,     0.96829,     0.96829,     0.96829,     0.96727,     0.96641,     0.96641,     0.96641,     0.96641,     0.96641,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,      0.9659,\n",
            "             0.9659,     0.96507,     0.96507,     0.96507,     0.96507,     0.95991,     0.95991,     0.95896,     0.95833,     0.95833,     0.95833,     0.95833,     0.95833,     0.95833,     0.95833,     0.95833,      0.9574,     0.95648,     0.95648,      0.9557,      0.9557,      0.9557,      0.9557,\n",
            "            0.95478,      0.9544,      0.9544,      0.9544,      0.9544,      0.9544,      0.9544,      0.9544,      0.9544,      0.9544,      0.9544,     0.95346,      0.9516,      0.9516,     0.95072,      0.9499,      0.9499,      0.9499,     0.94903,     0.94534,     0.94534,     0.94534,     0.94271,\n",
            "            0.94271,     0.94271,     0.94206,     0.94206,     0.94206,     0.94206,     0.94164,     0.94164,     0.94164,     0.94164,     0.94164,     0.94164,     0.94164,     0.93799,     0.93738,     0.93738,     0.93738,     0.93652,     0.93574,     0.93574,     0.93514,     0.93514,     0.93514,\n",
            "            0.93514,      0.9343,     0.93186,     0.93186,     0.93186,     0.93028,     0.93028,     0.92952,     0.92952,     0.92796,     0.92796,     0.92642,     0.92642,     0.92474,     0.92402,     0.92402,     0.92344,     0.92344,     0.92344,     0.92179,     0.92022,     0.91952,     0.91952,\n",
            "            0.91713,     0.91476,     0.91476,       0.914,      0.9134,      0.9134,     0.91182,     0.91041,     0.91041,     0.90967,     0.90729,     0.90656,     0.90268,     0.90205,     0.90205,     0.89771,     0.89771,     0.89771,     0.89771,     0.89414,     0.89414,     0.89414,     0.89354,\n",
            "            0.88831,     0.88831,     0.88611,     0.88544,     0.88412,     0.88195,      0.8813,     0.87925,     0.87563,     0.87553,     0.87553,     0.87553,     0.87553,     0.87553,       0.875,     0.87302,     0.87302,     0.87167,     0.87043,     0.86766,     0.86595,     0.86595,     0.86595,\n",
            "            0.86323,     0.85854,     0.85854,     0.85599,     0.85496,     0.85496,     0.85462,     0.85462,     0.85462,     0.83991,     0.83621,     0.83503,     0.83385,     0.83075,     0.82972,     0.82743,     0.82743,     0.82137,     0.81901,     0.81681,     0.81681,     0.79602,     0.79267,\n",
            "            0.78878,     0.78836,     0.78696,      0.7854,     0.78233,     0.78136,     0.77984,     0.77667,     0.77463,     0.77113,     0.76966,     0.76875,     0.76837,     0.76778,     0.76778,     0.76528,     0.75461,     0.74915,      0.7473,     0.74445,     0.73541,     0.73171,     0.72614,\n",
            "            0.71788,     0.71612,     0.71612,     0.71264,     0.71229,     0.71229,      0.7122,      0.7122,     0.70896,     0.69127,     0.67894,     0.67264,     0.67183,     0.67003,     0.66627,      0.6502,     0.64084,     0.63423,     0.63337,     0.61261,      0.6018,     0.57294,      0.5534,\n",
            "            0.54098,     0.53486,     0.53483,     0.49206,     0.49017,     0.48215,     0.48159,     0.48159,     0.47801,     0.46965,     0.46308,     0.44038,     0.39022,     0.36507,      0.3567,     0.33565,     0.32395,     0.31793,     0.27068,      0.2622,     0.25819,     0.24941,     0.22265,\n",
            "            0.21649,     0.21015,     0.19841,     0.17503,     0.15247,     0.14773,     0.14574,     0.14374,     0.14174,     0.13975,     0.13775,     0.13575,     0.13376,     0.13176,     0.12976,     0.12777,     0.12577,     0.12378,     0.12178,     0.11978,     0.11779,     0.11579,     0.11379,\n",
            "             0.1118,      0.1098,      0.1078,     0.10581,     0.10381,     0.10182,    0.099819,    0.097822,    0.095826,     0.09383,    0.091833,    0.089837,     0.08784,    0.085844,    0.083848,    0.081851,    0.079855,    0.077859,    0.075862,    0.073866,    0.071869,    0.069873,    0.067877,\n",
            "            0.06588,    0.063884,    0.061888,    0.059891,    0.057895,    0.055898,    0.053902,    0.051906,    0.049909,    0.047913,    0.045917,     0.04392,    0.041924,    0.039927,    0.037931,    0.035935,    0.033938,    0.031942,    0.029946,    0.027949,    0.025953,    0.023956,     0.02196,\n",
            "           0.019964,    0.017967,    0.015971,    0.013975,    0.011978,   0.0099819,   0.0079855,   0.0059891,   0.0039927,   0.0019964,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.25677,     0.25685,     0.33489,     0.38864,     0.42393,     0.45779,      0.4837,      0.5043,     0.52131,     0.53778,     0.55217,     0.56633,     0.57789,     0.59071,     0.60043,     0.61243,     0.61957,     0.62818,       0.634,     0.64206,     0.64746,     0.65306,     0.65728,\n",
            "            0.66364,     0.66872,      0.6751,     0.67857,     0.68192,     0.68623,     0.68834,     0.69155,     0.69471,      0.6997,      0.7051,     0.70882,     0.71156,     0.71618,      0.7198,     0.72169,     0.72609,     0.72751,     0.73015,     0.73346,      0.7352,     0.73891,      0.7412,\n",
            "            0.74367,     0.74567,     0.74833,      0.7509,     0.75177,     0.75338,     0.75656,     0.75807,     0.76019,     0.76037,     0.76307,     0.76473,     0.76438,     0.76603,     0.76804,     0.77065,      0.7719,     0.77445,     0.77581,     0.77691,     0.77861,     0.77946,     0.78234,\n",
            "            0.78382,     0.78519,      0.7858,     0.78715,     0.78685,     0.78659,     0.78716,     0.78738,     0.78813,      0.7885,     0.78943,     0.79188,     0.79302,     0.79469,      0.7953,     0.79569,     0.79603,     0.79785,     0.79782,     0.79781,     0.79891,     0.80009,     0.80056,\n",
            "            0.80179,     0.80245,     0.80247,     0.80285,     0.80387,     0.80411,     0.80454,     0.80519,     0.80616,     0.80731,     0.80744,     0.80832,     0.80956,     0.81016,     0.81041,     0.81095,     0.81076,     0.81211,     0.81238,     0.81283,     0.81404,     0.81351,     0.81333,\n",
            "             0.8129,     0.81279,     0.81315,     0.81332,     0.81381,     0.81457,     0.81415,     0.81469,     0.81505,      0.8154,     0.81582,     0.81593,     0.81711,     0.81758,     0.81759,     0.81714,     0.81822,     0.81844,     0.81882,     0.81866,     0.81922,     0.81917,     0.81895,\n",
            "            0.81889,     0.81892,     0.81983,     0.82013,     0.82087,     0.82119,     0.82211,     0.82195,     0.82274,     0.82344,     0.82403,     0.82535,     0.82616,     0.82664,     0.82762,     0.82907,      0.8298,      0.8298,     0.83045,     0.83092,     0.83124,     0.83208,     0.83188,\n",
            "            0.83176,     0.83189,     0.83256,     0.83287,     0.83284,     0.83297,      0.8331,     0.83332,     0.83339,     0.83385,     0.83433,     0.83467,      0.8352,     0.83544,     0.83592,     0.83524,     0.83562,     0.83608,     0.83575,     0.83606,     0.83606,     0.83659,     0.83713,\n",
            "            0.83737,     0.83703,     0.83729,     0.83753,     0.83763,     0.83782,      0.8382,     0.83821,     0.83843,     0.83863,     0.83879,     0.83884,     0.83856,     0.83912,     0.83986,     0.84017,     0.84045,     0.84061,     0.84076,     0.84101,     0.84137,     0.84171,     0.84196,\n",
            "             0.8425,     0.84364,     0.84426,     0.84544,     0.84485,     0.84465,     0.84445,     0.84472,     0.84345,     0.84379,     0.84375,     0.84454,     0.84441,     0.84424,     0.84407,     0.84359,     0.84391,     0.84412,     0.84449,       0.845,      0.8456,     0.84575,      0.8459,\n",
            "            0.84553,     0.84559,     0.84578,     0.84621,     0.84639,     0.84669,     0.84573,     0.84495,     0.84549,     0.84556,      0.8462,     0.84686,     0.84628,     0.84597,     0.84622,     0.84651,      0.8467,     0.84651,     0.84687,      0.8466,     0.84617,     0.84609,     0.84623,\n",
            "            0.84587,     0.84482,     0.84439,     0.84419,     0.84398,     0.84357,     0.84381,     0.84501,     0.84463,     0.84445,     0.84427,     0.84434,      0.8449,     0.84501,     0.84487,     0.84473,     0.84469,     0.84479,     0.84488,     0.84506,      0.8455,     0.84546,     0.84478,\n",
            "            0.84518,     0.84538,     0.84514,     0.84495,     0.84512,     0.84517,     0.84501,     0.84554,     0.84573,     0.84577,     0.84559,     0.84542,     0.84515,     0.84517,     0.84541,     0.84596,      0.8464,      0.8471,     0.84713,     0.84671,     0.84662,      0.8465,     0.84639,\n",
            "            0.84628,     0.84542,     0.84573,     0.84596,     0.84625,     0.84637,     0.84649,     0.84565,     0.84448,     0.84491,     0.84511,     0.84549,     0.84613,     0.84535,      0.8455,     0.84545,     0.84606,     0.84628,     0.84647,     0.84653,     0.84627,      0.8464,     0.84616,\n",
            "            0.84633,     0.84597,      0.8464,     0.84691,     0.84655,     0.84621,     0.84598,     0.84549,     0.84557,     0.84594,     0.84562,     0.84584,     0.84607,     0.84626,     0.84598,     0.84576,     0.84566,     0.84556,     0.84546,     0.84536,     0.84471,     0.84496,     0.84468,\n",
            "            0.84451,     0.84444,     0.84437,     0.84431,     0.84424,     0.84418,     0.84411,     0.84444,     0.84457,      0.8447,     0.84504,     0.84467,     0.84457,     0.84491,     0.84523,     0.84428,     0.84448,     0.84429,     0.84402,     0.84405,      0.8443,     0.84387,     0.84441,\n",
            "            0.84418,     0.84358,     0.84301,     0.84265,     0.84273,     0.84232,      0.8423,     0.84216,     0.84202,     0.84188,     0.84228,     0.84122,     0.84177,     0.84164,     0.84149,     0.84134,     0.84107,     0.84077,     0.84058,     0.84038,     0.84037,     0.84052,     0.84095,\n",
            "            0.84009,     0.83945,     0.83955,     0.83972,     0.83933,     0.83842,     0.83818,      0.8383,     0.83842,      0.8385,     0.83866,     0.83846,     0.83825,     0.83835,     0.83845,     0.83812,     0.83723,     0.83655,     0.83606,     0.83583,     0.83558,     0.83542,     0.83557,\n",
            "            0.83533,     0.83557,     0.83526,     0.83451,     0.83381,     0.83316,     0.83339,     0.83325,     0.83367,     0.83387,     0.83418,     0.83417,     0.83387,     0.83333,     0.83293,     0.83269,      0.8325,     0.83232,     0.83113,     0.83089,     0.83056,      0.8299,     0.82952,\n",
            "            0.82932,     0.82913,     0.82845,     0.82809,     0.82754,      0.8277,     0.82785,     0.82772,     0.82751,     0.82715,     0.82594,     0.82639,     0.82584,     0.82542,     0.82645,     0.82651,     0.82616,     0.82582,     0.82603,     0.82488,     0.82494,     0.82481,     0.82468,\n",
            "            0.82456,     0.82443,     0.82423,     0.82407,     0.82391,     0.82375,     0.82363,     0.82368,     0.82329,     0.82204,     0.82139,     0.82107,     0.82089,     0.82071,        0.82,     0.81972,      0.8195,     0.81935,      0.8192,     0.81904,     0.81884,     0.81863,      0.8184,\n",
            "            0.81811,     0.81773,     0.81733,     0.81706,     0.81714,     0.81668,     0.81649,      0.8163,     0.81543,     0.81453,     0.81397,     0.81339,      0.8128,      0.8124,     0.81062,     0.81041,     0.81021,     0.80982,     0.80929,     0.80878,     0.80821,     0.80765,     0.80733,\n",
            "            0.80751,     0.80744,     0.80687,       0.807,     0.80707,     0.80715,     0.80722,     0.80729,      0.8079,     0.80822,     0.80785,     0.80756,     0.80725,      0.8068,     0.80653,     0.80628,     0.80596,     0.80566,     0.80552,     0.80538,     0.80524,     0.80521,     0.80545,\n",
            "            0.80395,     0.80317,     0.80267,     0.80236,     0.80189,     0.80038,     0.79973,     0.79906,     0.79855,     0.79725,     0.79759,     0.79702,     0.79681,     0.79659,     0.79634,     0.79607,     0.79546,     0.79516,     0.79494,     0.79446,     0.79398,     0.79359,      0.7933,\n",
            "            0.79318,     0.79306,     0.79293,     0.79281,     0.79266,     0.79248,      0.7923,     0.79161,     0.79129,     0.79058,     0.79011,     0.78923,     0.78915,     0.78883,     0.78739,     0.78669,     0.78635,     0.78526,     0.78594,     0.78551,     0.78557,     0.78562,     0.78527,\n",
            "            0.78542,      0.7843,     0.78386,       0.784,     0.78417,     0.78377,     0.78382,     0.78285,     0.78205,     0.78167,       0.781,     0.77995,     0.77868,     0.77787,     0.77759,     0.77763,     0.77687,     0.77686,     0.77624,     0.77405,     0.77285,     0.77255,     0.77217,\n",
            "            0.77167,     0.77064,     0.77064,     0.76937,     0.76882,     0.76799,     0.76759,     0.76709,      0.7643,     0.76362,     0.76254,     0.76182,     0.76126,      0.7617,     0.76182,     0.76193,     0.76213,     0.76096,     0.76039,     0.75967,     0.75971,     0.75936,     0.75827,\n",
            "            0.75562,     0.75352,     0.75246,     0.75203,     0.75152,     0.74914,     0.74806,     0.74673,      0.7456,     0.74355,     0.74257,     0.74141,     0.74159,     0.74126,     0.74048,     0.73962,     0.73913,     0.73811,     0.73664,     0.73428,     0.73316,     0.73276,     0.73227,\n",
            "            0.73154,     0.72928,      0.7286,     0.72806,     0.72724,     0.72603,     0.72386,      0.7235,     0.72317,     0.72273,     0.72276,     0.72244,     0.72083,     0.71899,     0.71852,     0.71651,     0.71591,     0.71528,     0.71466,     0.71427,     0.71383,     0.71336,     0.71218,\n",
            "            0.71139,     0.70884,     0.70706,     0.70656,     0.70516,      0.7049,     0.70463,     0.70361,      0.7019,     0.69932,     0.69855,      0.6978,     0.69543,     0.69491,     0.69446,      0.6935,      0.6919,     0.69129,     0.68939,     0.68667,     0.68374,     0.68165,     0.68017,\n",
            "             0.6785,     0.67739,     0.67684,      0.6764,     0.67474,     0.67353,     0.67147,     0.67086,      0.6702,     0.66966,     0.66889,     0.66691,     0.66375,     0.66171,     0.66012,     0.65893,     0.65828,     0.65647,     0.65577,     0.65488,     0.65451,      0.6522,     0.65006,\n",
            "            0.64929,      0.6478,      0.6463,     0.64251,       0.641,     0.63919,     0.63863,     0.63736,     0.63489,     0.63409,      0.6327,     0.63203,     0.63046,     0.62823,     0.62663,     0.62003,     0.61941,     0.61722,     0.61322,     0.61081,     0.60797,     0.60611,     0.60186,\n",
            "            0.59758,     0.59398,      0.5911,     0.58756,     0.58671,     0.58601,     0.58424,     0.58261,     0.58078,     0.57778,     0.57527,     0.57282,     0.57039,     0.56744,     0.56529,     0.56111,     0.55569,     0.55292,     0.54943,     0.54552,     0.54302,     0.53794,     0.53656,\n",
            "            0.53326,      0.5292,     0.52817,     0.52403,     0.51879,     0.51586,      0.5111,     0.50637,     0.50071,     0.49782,     0.49481,     0.49225,     0.48592,     0.48194,     0.47367,     0.46763,     0.46058,     0.45806,     0.45529,      0.4517,     0.45079,     0.44732,      0.4453,\n",
            "            0.43526,     0.43403,     0.43289,     0.42823,     0.42227,     0.41859,     0.41282,     0.40531,     0.39786,     0.39333,     0.38775,     0.38015,     0.37732,     0.37217,     0.36858,     0.36315,     0.36047,     0.35564,     0.35061,     0.34862,     0.34493,     0.34316,     0.33505,\n",
            "            0.32956,     0.32693,     0.32486,      0.3206,     0.31769,     0.31099,     0.30245,     0.29792,     0.29393,     0.29274,     0.28789,     0.28448,     0.28253,     0.27745,     0.27254,     0.26805,     0.26513,     0.26276,     0.25957,     0.25599,     0.25252,     0.24811,      0.2419,\n",
            "            0.23721,     0.23316,     0.22927,     0.22521,     0.21988,     0.21654,     0.21069,     0.20688,     0.20456,     0.19687,     0.19455,     0.19306,     0.18879,     0.18188,     0.17517,     0.16899,     0.16754,     0.16348,     0.15781,     0.15586,     0.15077,     0.14814,     0.14494,\n",
            "            0.14192,     0.13587,     0.13164,     0.12968,     0.12787,     0.12645,     0.12061,     0.11879,     0.11021,     0.10796,     0.10723,     0.10097,    0.098899,    0.095013,    0.091717,    0.087186,    0.085895,    0.084429,    0.081759,    0.080135,    0.075668,    0.073983,     0.06669,\n",
            "           0.065496,    0.064665,    0.063753,    0.062399,    0.061595,    0.059982,    0.058535,    0.056172,     0.05267,     0.04736,    0.041951,    0.036763,     0.03487,    0.034093,    0.031911,    0.030068,    0.028948,    0.028257,    0.025445,    0.022725,    0.021274,    0.018866,    0.018286,\n",
            "           0.017848,    0.017409,    0.016936,    0.016419,      0.0159,    0.015386,    0.014877,    0.014367,    0.013802,    0.013155,    0.012508,     0.01174,    0.010972,   0.0092806,   0.0091513,    0.009022,   0.0088927,   0.0087634,    0.008634,   0.0085046,   0.0083753,   0.0082459,   0.0081164,\n",
            "           0.007987,   0.0078576,   0.0045175,   0.0035704,   0.0031021,   0.0030289,   0.0029557,   0.0028825,   0.0028093,   0.0027361,   0.0026629,   0.0025897,   0.0025165,   0.0024432,     0.00237,   0.0022968,   0.0022235,   0.0021503,    0.002077,   0.0020038,   0.0019305,   0.0018572,    0.001784,\n",
            "          0.0017107,   0.0016374,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.14907,     0.14913,      0.2046,     0.24642,      0.2758,     0.30516,     0.32894,     0.34841,     0.36524,     0.38161,     0.39642,     0.41118,     0.42348,     0.43739,     0.44851,     0.46204,     0.47065,     0.48088,     0.48865,     0.49877,     0.50532,     0.51217,     0.51739,\n",
            "            0.52531,     0.53197,     0.54067,     0.54572,     0.55005,     0.55599,     0.55877,     0.56301,     0.56722,     0.57422,     0.58153,     0.58661,     0.59036,     0.59676,     0.60215,     0.60481,     0.61101,     0.61377,     0.61754,     0.62228,      0.6248,     0.63017,     0.63412,\n",
            "            0.63794,     0.64128,     0.64523,     0.64906,     0.65078,      0.6532,       0.658,     0.66028,     0.66351,     0.66423,      0.6688,     0.67182,     0.67217,     0.67518,     0.67875,     0.68287,     0.68483,     0.68886,     0.69119,     0.69323,     0.69594,     0.69731,     0.70194,\n",
            "            0.70431,     0.70653,     0.70751,     0.71073,     0.71202,     0.71258,     0.71384,     0.71421,     0.71702,     0.71815,      0.7197,     0.72379,     0.72603,     0.72904,     0.73006,     0.73073,     0.73186,     0.73494,     0.73545,     0.73599,     0.73786,     0.73988,     0.74068,\n",
            "            0.74279,     0.74392,     0.74453,     0.74519,     0.74722,     0.74795,     0.74905,      0.7504,     0.75209,      0.7541,     0.75491,     0.75645,     0.75863,     0.75969,     0.76013,     0.76108,     0.76134,     0.76373,     0.76421,     0.76561,     0.76778,     0.76758,     0.76774,\n",
            "            0.76822,      0.7686,     0.76928,     0.76959,     0.77172,     0.77314,     0.77299,     0.77395,     0.77524,     0.77588,     0.77667,     0.77749,     0.78028,     0.78114,     0.78181,     0.78207,     0.78428,     0.78468,      0.7854,     0.78576,     0.78679,     0.78691,     0.78683,\n",
            "             0.7875,     0.78891,      0.7906,     0.79117,     0.79265,     0.79382,     0.79554,     0.79593,     0.79742,     0.79873,     0.79985,     0.80234,     0.80386,     0.80478,     0.80664,      0.8094,     0.81078,     0.81132,     0.81275,     0.81365,     0.81427,     0.81587,     0.81659,\n",
            "            0.81672,     0.81697,     0.81827,     0.81899,     0.81954,     0.81979,     0.82004,     0.82047,     0.82135,     0.82298,     0.82392,     0.82459,     0.82562,      0.8261,     0.82704,     0.82717,     0.82796,     0.82886,      0.8295,     0.83069,     0.83111,     0.83216,     0.83322,\n",
            "            0.83383,      0.8338,     0.83432,     0.83479,     0.83615,     0.83694,       0.838,     0.83849,     0.83895,     0.83933,     0.83966,     0.83989,     0.83981,     0.84111,     0.84261,     0.84322,      0.8438,     0.84411,     0.84442,     0.84491,     0.84565,     0.84633,     0.84684,\n",
            "            0.84793,     0.85024,      0.8515,     0.85453,     0.85438,     0.85433,     0.85428,      0.8549,     0.85478,     0.85548,     0.85623,     0.85794,     0.85852,     0.85848,     0.85844,     0.85842,     0.85908,     0.85952,     0.86029,     0.86134,      0.8626,     0.86291,     0.86322,\n",
            "            0.86314,     0.86342,     0.86382,     0.86471,     0.86509,     0.86572,      0.8657,     0.86551,     0.86665,     0.86767,     0.86903,     0.87043,      0.8703,     0.87029,     0.87083,     0.87142,     0.87271,     0.87294,     0.87499,     0.87492,     0.87483,       0.875,     0.87529,\n",
            "            0.87542,     0.87518,     0.87509,     0.87504,       0.875,     0.87557,     0.87644,     0.87923,     0.87914,     0.87911,     0.87907,      0.8794,     0.88062,     0.88126,      0.8815,     0.88188,     0.88204,     0.88224,     0.88243,     0.88283,     0.88379,     0.88408,     0.88394,\n",
            "            0.88496,     0.88542,     0.88539,      0.8854,     0.88579,     0.88609,     0.88649,     0.88767,     0.88808,      0.8883,     0.88826,     0.88823,     0.88817,     0.88875,     0.88929,      0.8905,     0.89147,     0.89303,      0.8935,      0.8941,     0.89412,     0.89409,     0.89407,\n",
            "            0.89405,     0.89389,     0.89485,     0.89536,     0.89698,     0.89725,     0.89752,     0.89753,     0.89731,     0.89894,     0.89939,     0.90024,     0.90202,     0.90194,     0.90265,     0.90318,     0.90458,     0.90509,     0.90552,     0.90582,     0.90577,     0.90655,      0.9065,\n",
            "            0.90729,     0.90746,     0.90844,     0.90961,     0.91036,     0.91031,     0.91027,     0.91019,     0.91069,      0.9118,     0.91186,     0.91237,      0.9129,      0.9134,     0.91335,     0.91332,      0.9133,     0.91328,     0.91327,     0.91325,     0.91398,     0.91455,      0.9147,\n",
            "            0.91467,     0.91466,     0.91465,     0.91464,     0.91463,     0.91462,     0.91461,     0.91546,     0.91578,      0.9161,     0.91711,     0.91705,     0.91794,     0.91874,     0.91951,     0.91937,      0.9202,     0.92017,     0.92013,     0.92108,     0.92178,     0.92172,     0.92341,\n",
            "            0.92338,     0.92329,     0.92321,     0.92316,     0.92398,     0.92392,     0.92474,     0.92472,      0.9247,     0.92468,     0.92638,     0.92623,     0.92792,     0.92794,     0.92792,      0.9279,     0.92786,     0.92782,      0.9278,     0.92777,     0.92795,     0.92832,     0.92951,\n",
            "            0.92958,     0.93015,     0.93056,     0.93099,     0.93183,     0.93171,     0.93187,     0.93217,     0.93246,     0.93342,      0.9343,     0.93427,     0.93425,     0.93466,     0.93513,     0.93509,     0.93498,     0.93489,     0.93483,     0.93567,     0.93564,     0.93576,      0.9365,\n",
            "            0.93672,     0.93738,     0.93734,     0.93725,     0.93716,     0.93708,     0.93794,     0.93883,     0.93989,     0.94041,     0.94121,     0.94162,     0.94159,     0.94153,     0.94148,     0.94145,     0.94143,     0.94141,     0.94128,     0.94125,     0.94121,     0.94153,     0.94199,\n",
            "            0.94197,     0.94195,     0.94187,     0.94183,      0.9418,     0.94221,     0.94262,      0.9427,     0.94267,     0.94263,      0.9425,     0.94396,     0.94523,     0.94534,     0.94805,       0.949,     0.94897,     0.94893,     0.94989,     0.94977,     0.95072,     0.95071,     0.95069,\n",
            "            0.95068,     0.95067,     0.95159,     0.95158,     0.95156,     0.95155,      0.9526,     0.95439,     0.95436,     0.95425,     0.95419,     0.95416,     0.95414,     0.95413,     0.95406,     0.95404,     0.95402,       0.954,     0.95399,     0.95398,     0.95396,     0.95394,     0.95392,\n",
            "            0.95389,     0.95386,     0.95382,     0.95379,     0.95477,     0.95473,     0.95471,      0.9547,      0.9556,     0.95552,     0.95561,     0.95689,     0.95833,     0.95829,     0.95815,     0.95813,     0.95811,     0.95808,     0.95804,     0.95799,      0.9585,      0.9589,       0.959,\n",
            "            0.95951,      0.9599,     0.96067,     0.96105,     0.96126,     0.96147,     0.96167,     0.96188,      0.9636,     0.96451,     0.96503,     0.96501,     0.96498,     0.96495,     0.96493,     0.96492,     0.96489,     0.96487,     0.96486,     0.96485,     0.96484,     0.96505,     0.96589,\n",
            "            0.96579,     0.96574,      0.9657,     0.96568,     0.96565,     0.96554,      0.9655,     0.96545,     0.96542,     0.96538,     0.96638,     0.96637,     0.96635,     0.96634,     0.96632,      0.9663,     0.96626,     0.96624,     0.96622,     0.96678,     0.96723,     0.96745,     0.96825,\n",
            "            0.96825,     0.96824,     0.96823,     0.96822,     0.96821,      0.9682,     0.96819,     0.96814,     0.96812,     0.96808,     0.96805,     0.96799,     0.96907,     0.96905,     0.96896,     0.96892,     0.96889,     0.96883,     0.97106,     0.97104,     0.97162,     0.97215,     0.97213,\n",
            "            0.97325,     0.97319,     0.97316,     0.97367,     0.97421,     0.97468,      0.9754,     0.97648,     0.97644,     0.97642,     0.97639,     0.97634,     0.97628,     0.97624,     0.97622,     0.97737,     0.97795,     0.97849,     0.97846,     0.97836,     0.97831,     0.97829,     0.97828,\n",
            "            0.97826,     0.97821,     0.97938,     0.97932,      0.9793,     0.97927,     0.97925,     0.97923,     0.97911,     0.97908,     0.97903,       0.979,     0.97897,     0.98146,     0.98185,     0.98225,     0.98289,     0.98378,     0.98376,     0.98374,     0.98496,     0.98495,     0.98491,\n",
            "            0.98483,     0.98476,     0.98473,     0.98472,      0.9847,     0.98588,     0.98584,      0.9858,     0.98577,     0.98571,     0.98568,     0.98607,     0.98673,     0.98692,      0.9869,     0.98687,     0.98686,     0.98683,     0.98808,     0.98802,       0.988,     0.98799,     0.98797,\n",
            "            0.98795,      0.9879,     0.98788,     0.98787,     0.98784,     0.98781,     0.98776,     0.98775,     0.98774,     0.98773,     0.98907,     0.98906,     0.98902,     0.98898,     0.98897,     0.99028,     0.99027,     0.99026,     0.99024,     0.99024,     0.99023,     0.99022,     0.99019,\n",
            "            0.99018,     0.99012,     0.99008,     0.99007,     0.99004,     0.99004,     0.99003,     0.99142,     0.99139,     0.99134,     0.99132,     0.99131,     0.99126,     0.99125,     0.99124,     0.99123,      0.9912,     0.99118,     0.99115,     0.99109,     0.99104,     0.99099,     0.99097,\n",
            "            0.99093,       0.991,      0.9924,     0.99239,     0.99236,     0.99234,     0.99231,      0.9923,     0.99229,     0.99228,      0.9938,     0.99377,     0.99372,     0.99369,     0.99367,     0.99366,     0.99365,     0.99362,     0.99361,      0.9936,     0.99359,     0.99356,     0.99353,\n",
            "            0.99352,     0.99349,     0.99347,     0.99342,     0.99339,     0.99337,     0.99336,     0.99334,      0.9933,     0.99329,     0.99327,     0.99326,     0.99323,      0.9932,     0.99317,     0.99307,     0.99306,     0.99302,     0.99296,     0.99292,     0.99287,     0.99461,     0.99456,\n",
            "             0.9945,     0.99446,     0.99627,     0.99624,     0.99623,     0.99622,     0.99621,     0.99619,     0.99618,     0.99615,     0.99612,      0.9961,     0.99608,     0.99605,     0.99603,     0.99599,     0.99593,     0.99591,     0.99587,     0.99791,      0.9979,     0.99787,     0.99786,\n",
            "            0.99784,     0.99782,     0.99781,     0.99779,     0.99776,     0.99774,     0.99772,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n",
            "                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
            "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
            "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
            "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
            "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
            "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
            "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
            "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
            "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
            "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
            "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
            "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
            "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
            "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
            "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
            "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
            "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
            "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
            "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
            "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
            "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
            "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
            "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
            "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
            "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
            "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
            "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
            "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
            "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
            "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
            "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
            "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
            "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
            "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
            "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
            "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
            "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
            "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
            "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
            "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
            "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
            "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.92526,     0.92526,     0.92211,     0.91896,     0.91581,     0.91581,     0.91345,     0.91267,     0.91031,     0.91031,     0.90952,     0.90952,     0.90952,     0.90952,     0.90795,     0.90795,     0.90637,     0.90559,     0.90244,     0.90087,     0.90087,     0.90087,     0.90087,\n",
            "            0.90087,     0.90008,     0.89851,     0.89693,     0.89693,     0.89614,     0.89614,     0.89614,     0.89614,     0.89536,     0.89536,     0.89536,     0.89536,     0.89536,     0.89457,     0.89457,     0.89457,       0.893,       0.893,       0.893,       0.893,       0.893,      0.8918,\n",
            "            0.89142,     0.89064,     0.89064,     0.89064,     0.88985,     0.88985,     0.88985,     0.88985,     0.88985,     0.88906,     0.88828,     0.88746,     0.88592,     0.88513,     0.88438,     0.88434,     0.88434,     0.88434,     0.88403,     0.88356,     0.88356,     0.88356,     0.88356,\n",
            "            0.88356,     0.88356,     0.88356,     0.88198,     0.87925,     0.87776,     0.87726,     0.87726,      0.8749,     0.87411,     0.87411,     0.87411,     0.87362,     0.87333,     0.87333,     0.87333,     0.87254,     0.87254,     0.87175,     0.87097,     0.87097,     0.87097,     0.87097,\n",
            "            0.87097,     0.87097,     0.87018,     0.87018,     0.86982,     0.86939,     0.86892,     0.86861,     0.86861,     0.86861,     0.86782,     0.86782,     0.86782,     0.86782,     0.86782,     0.86782,     0.86703,     0.86703,     0.86703,     0.86625,     0.86624,     0.86528,     0.86467,\n",
            "             0.8631,     0.86238,     0.86231,     0.86231,     0.86074,     0.86069,     0.85995,     0.85995,     0.85917,     0.85917,     0.85914,     0.85838,     0.85759,     0.85759,     0.85681,      0.8555,     0.85523,     0.85523,     0.85521,     0.85445,     0.85445,     0.85419,      0.8538,\n",
            "            0.85287,      0.8513,      0.8513,      0.8513,     0.85118,     0.85051,     0.85051,     0.84972,     0.84972,     0.84972,     0.84972,     0.84972,     0.84972,     0.84972,     0.84972,     0.84972,     0.84972,     0.84914,     0.84894,     0.84894,     0.84894,     0.84894,     0.84774,\n",
            "            0.84736,     0.84736,     0.84736,     0.84724,     0.84658,     0.84658,     0.84658,     0.84658,     0.84579,       0.845,       0.845,       0.845,       0.845,       0.845,       0.845,     0.84348,     0.84343,     0.84343,      0.8421,      0.8415,     0.84107,     0.84107,     0.84107,\n",
            "            0.84093,     0.84028,     0.84028,     0.84028,     0.83912,     0.83871,     0.83841,     0.83792,     0.83792,     0.83792,     0.83792,     0.83781,     0.83731,     0.83714,     0.83714,     0.83714,     0.83714,     0.83714,     0.83714,     0.83714,     0.83714,     0.83714,     0.83714,\n",
            "            0.83714,     0.83714,     0.83714,     0.83654,     0.83553,     0.83519,     0.83485,     0.83478,     0.83242,     0.83242,     0.83163,     0.83156,     0.83076,     0.83046,     0.83017,     0.82927,     0.82927,     0.82927,     0.82927,     0.82927,     0.82927,     0.82927,     0.82927,\n",
            "            0.82864,     0.82848,     0.82848,     0.82848,     0.82848,     0.82848,     0.82666,     0.82533,     0.82533,     0.82455,     0.82455,     0.82454,     0.82356,     0.82297,     0.82297,     0.82297,     0.82219,     0.82162,     0.82051,     0.82005,     0.81933,     0.81904,     0.81904,\n",
            "            0.81824,     0.81649,     0.81576,     0.81543,     0.81508,     0.81382,     0.81353,     0.81336,     0.81271,     0.81242,     0.81212,     0.81196,     0.81196,     0.81163,     0.81117,     0.81059,     0.81039,     0.81039,     0.81039,     0.81039,     0.81039,     0.81008,     0.80894,\n",
            "            0.80881,     0.80881,      0.8084,     0.80803,     0.80803,     0.80787,     0.80724,     0.80724,     0.80724,     0.80712,     0.80684,     0.80656,      0.8061,     0.80566,     0.80566,     0.80566,     0.80566,     0.80566,     0.80533,     0.80409,     0.80391,     0.80372,     0.80353,\n",
            "            0.80335,     0.80195,     0.80173,     0.80173,     0.80094,     0.80094,     0.80094,     0.79944,     0.79753,     0.79701,     0.79701,     0.79701,     0.79677,     0.79544,     0.79516,     0.79465,     0.79465,     0.79465,     0.79465,     0.79454,     0.79411,     0.79374,     0.79334,\n",
            "            0.79304,     0.79229,     0.79229,     0.79229,     0.79109,     0.79055,     0.79017,     0.78939,     0.78914,     0.78896,     0.78836,     0.78836,     0.78836,     0.78832,     0.78786,     0.78751,     0.78735,     0.78719,     0.78703,     0.78687,     0.78521,     0.78521,     0.78462,\n",
            "            0.78434,     0.78423,     0.78413,     0.78402,     0.78392,     0.78381,      0.7837,     0.78363,     0.78363,     0.78363,     0.78347,     0.78288,     0.78206,     0.78206,     0.78206,     0.78053,     0.78028,     0.77997,     0.77953,     0.77891,     0.77884,     0.77815,     0.77787,\n",
            "            0.77749,     0.77654,     0.77563,     0.77506,     0.77461,     0.77396,     0.77335,     0.77313,     0.77291,     0.77269,     0.77218,     0.77051,     0.77026,     0.77003,     0.76979,     0.76955,     0.76913,     0.76865,     0.76834,     0.76804,      0.7679,      0.7679,      0.7678,\n",
            "            0.76633,     0.76487,     0.76475,     0.76475,     0.76354,     0.76211,     0.76161,     0.76161,     0.76161,     0.76111,     0.76079,     0.76047,     0.76015,     0.76003,     0.75989,     0.75937,     0.75798,     0.75692,     0.75617,     0.75525,     0.75486,     0.75452,     0.75428,\n",
            "            0.75374,     0.75371,     0.75323,     0.75207,     0.75099,     0.74999,      0.7498,     0.74902,     0.74902,     0.74902,     0.74902,     0.74872,     0.74826,     0.74744,     0.74682,     0.74645,     0.74617,     0.74588,     0.74407,     0.74369,     0.74318,     0.74194,     0.74105,\n",
            "            0.74074,     0.74044,     0.73942,     0.73886,       0.738,       0.738,       0.738,     0.73775,     0.73742,     0.73687,     0.73503,     0.73485,     0.73323,     0.73249,     0.73249,     0.73202,      0.7315,     0.73098,     0.73075,     0.72901,     0.72855,     0.72836,     0.72817,\n",
            "            0.72798,     0.72779,     0.72693,     0.72669,     0.72645,     0.72621,     0.72541,     0.72446,     0.72388,     0.72201,     0.72103,     0.72056,     0.72029,     0.72003,     0.71897,     0.71856,     0.71823,       0.718,     0.71778,     0.71755,     0.71725,     0.71694,      0.7166,\n",
            "            0.71617,      0.7156,     0.71502,     0.71461,     0.71419,     0.71351,     0.71322,     0.71294,     0.71113,      0.7098,     0.70889,     0.70732,     0.70565,     0.70506,     0.70246,     0.70216,     0.70186,     0.70129,     0.70053,     0.69978,     0.69866,     0.69761,     0.69709,\n",
            "            0.69709,     0.69678,     0.69552,     0.69552,     0.69552,     0.69552,     0.69552,     0.69552,     0.69552,     0.69552,      0.6947,     0.69429,     0.69384,     0.69319,     0.69281,     0.69244,     0.69198,     0.69155,     0.69135,     0.69115,     0.69095,     0.69079,     0.69072,\n",
            "            0.68856,     0.68744,     0.68674,     0.68629,     0.68562,     0.68347,     0.68254,     0.68159,     0.68087,     0.67899,     0.67899,     0.67818,     0.67787,     0.67756,     0.67722,     0.67683,     0.67597,     0.67555,     0.67524,     0.67427,     0.67336,      0.6727,      0.6719,\n",
            "            0.67173,     0.67155,     0.67138,     0.67121,     0.67099,     0.67075,      0.6705,     0.66952,     0.66908,     0.66808,     0.66743,      0.6662,     0.66558,     0.66513,     0.66313,     0.66215,     0.66169,     0.66018,     0.66009,      0.6595,     0.65932,     0.65915,     0.65866,\n",
            "            0.65836,     0.65682,     0.65621,     0.65618,     0.65618,     0.65539,     0.65515,      0.6533,     0.65221,     0.65169,     0.65077,     0.64934,      0.6476,      0.6465,     0.64612,     0.64567,     0.64437,     0.64412,     0.64329,     0.64033,     0.63871,     0.63831,      0.6378,\n",
            "            0.63712,     0.63573,     0.63524,     0.63355,     0.63281,      0.6317,     0.63116,      0.6305,     0.62679,     0.62588,     0.62445,      0.6235,     0.62276,     0.62234,     0.62234,     0.62234,     0.62234,     0.62044,     0.61968,     0.61874,     0.61831,     0.61785,     0.61642,\n",
            "            0.61296,     0.61023,     0.60885,      0.6083,     0.60763,     0.60408,      0.6027,     0.60098,     0.59953,     0.59691,     0.59566,     0.59402,     0.59402,     0.59352,     0.59253,     0.59144,     0.59082,     0.58953,     0.58721,     0.58423,     0.58283,     0.58233,     0.58172,\n",
            "             0.5808,     0.57798,     0.57713,     0.57646,     0.57544,     0.57393,     0.57124,      0.5708,     0.57039,     0.56985,     0.56944,     0.56904,     0.56706,     0.56481,     0.56422,     0.56133,      0.5606,     0.55983,     0.55907,      0.5586,     0.55806,     0.55749,     0.55606,\n",
            "             0.5551,     0.55201,     0.54988,     0.54928,     0.54759,     0.54728,     0.54696,     0.54531,     0.54327,     0.54019,     0.53928,     0.53839,     0.53559,     0.53497,     0.53444,     0.53332,     0.53143,     0.53071,      0.5285,     0.52532,     0.52191,     0.51949,     0.51778,\n",
            "            0.51586,     0.51456,     0.51354,     0.51304,     0.51114,     0.50976,     0.50741,     0.50672,     0.50597,     0.50536,     0.50409,     0.50185,     0.49828,       0.496,     0.49422,     0.49289,     0.49216,     0.49015,     0.48937,     0.48838,     0.48798,     0.48543,     0.48306,\n",
            "            0.48221,     0.48058,     0.47893,     0.47479,     0.47316,     0.47119,     0.47059,     0.46921,     0.46654,     0.46569,      0.4642,     0.46347,     0.46179,     0.45941,     0.45771,     0.45072,     0.45007,     0.44776,     0.44358,     0.44108,     0.43812,     0.43586,     0.43149,\n",
            "            0.42711,     0.42345,     0.42021,     0.41664,     0.41579,     0.41509,     0.41332,     0.41169,     0.40986,      0.4069,     0.40441,       0.402,     0.39961,     0.39673,     0.39463,     0.39058,     0.38535,      0.3827,     0.37936,     0.37536,       0.373,     0.36823,     0.36693,\n",
            "            0.36385,     0.36009,     0.35914,     0.35532,     0.35053,     0.34785,     0.34355,     0.33902,     0.33396,      0.3314,     0.32874,     0.32648,     0.32093,     0.31748,     0.31033,     0.30517,     0.29919,     0.29707,     0.29474,     0.29174,     0.29098,     0.28809,     0.28642,\n",
            "            0.27816,     0.27716,     0.27623,     0.27245,     0.26764,      0.2647,     0.26009,     0.25416,     0.24833,     0.24481,      0.2405,     0.23469,     0.23253,     0.22863,     0.22593,     0.22186,     0.21986,     0.21628,     0.21257,     0.21111,     0.20841,     0.20712,     0.20123,\n",
            "            0.19729,     0.19541,     0.19393,      0.1909,     0.18884,     0.18413,     0.17817,     0.17503,     0.17228,     0.17147,     0.16815,     0.16583,      0.1645,     0.16107,     0.15777,     0.15477,     0.15283,     0.15125,     0.14914,     0.14678,      0.1445,     0.14162,     0.13759,\n",
            "            0.13457,     0.13197,     0.12948,     0.12689,     0.12352,     0.12141,     0.11775,     0.11538,     0.11393,     0.10918,     0.10775,     0.10684,     0.10424,     0.10004,    0.095991,    0.092296,    0.091431,    0.089018,    0.085664,    0.084513,    0.081532,    0.079995,    0.078131,\n",
            "           0.076381,    0.072889,     0.07046,    0.069334,    0.068303,    0.067494,    0.064173,    0.063148,    0.058319,    0.057058,    0.056653,    0.053169,    0.052022,    0.049876,    0.048062,     0.04558,    0.044874,    0.044075,    0.042622,     0.04174,    0.039322,    0.038412,    0.034496,\n",
            "           0.033857,    0.033413,    0.032926,    0.032204,    0.031776,    0.030918,     0.03015,    0.028898,    0.027047,    0.024254,    0.021425,    0.018726,    0.017744,    0.017342,    0.016214,    0.015264,    0.014687,    0.014331,    0.012886,    0.011493,    0.010751,   0.0095226,   0.0092273,\n",
            "          0.0090041,    0.008781,   0.0085404,   0.0082772,    0.008014,   0.0077528,   0.0074941,   0.0072354,   0.0069489,   0.0066212,   0.0062933,   0.0059048,   0.0055164,   0.0046619,   0.0045967,   0.0045314,   0.0044662,    0.004401,   0.0043357,   0.0042705,   0.0042052,     0.00414,   0.0040748,\n",
            "          0.0040095,   0.0039443,   0.0022639,   0.0017884,   0.0015534,   0.0015167,     0.00148,   0.0014433,   0.0014066,   0.0013699,   0.0013332,   0.0012965,   0.0012598,   0.0012231,   0.0011864,   0.0011497,    0.001113,   0.0010763,   0.0010396,   0.0010029,  0.00096619,  0.00092949,  0.00089278,\n",
            "         0.00085608,  0.00081938,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n",
            "                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
            "fitness: np.float64(0.5258043956976847)\n",
            "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
            "maps: array([     0.4862])\n",
            "names: {0: 'person'}\n",
            "nt_per_class: array([1271])\n",
            "nt_per_image: array([423])\n",
            "results_dict: {'metrics/precision(B)': np.float64(0.8853915915561905), 'metrics/recall(B)': np.float64(0.8083977440308977), 'metrics/mAP50(B)': np.float64(0.8822210068153963), 'metrics/mAP50-95(B)': np.float64(0.48620255001793894), 'fitness': np.float64(0.5258043956976847)}\n",
            "save_dir: PosixPath('runs_finetune/person_yolov11n2')\n",
            "speed: {'preprocess': 0.3678977751941736, 'inference': 1.5841546821793036, 'loss': 0.0024317170557829498, 'postprocess': 0.9641439166563743}\n",
            "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
            "task: 'detect'\n"
          ]
        }
      ],
      "source": [
        "# Evaluates the trained model using the TEST SET defined in data.yaml\n",
        "\n",
        "metrics_YOLOv11n = model_YOLOv11n.val(data='data.yaml', split='test') # returns accuracy metrics (e.g., mAP, precision, recall, etc.) on the test set\n",
        "print(\"Test metrics:\", metrics_YOLOv11n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meN-A5gNxGLR",
        "outputId": "f927b62e-7b48-4230-f8eb-95503be7234d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#  Inference (practical use of the fine-tuned model)\n",
        "\n",
        "# Load the best weights found\n",
        "best_YOLOv11n = '/content/runs_finetune/person_yolov11n/weights/best.pt'\n",
        "model_inf_YOLOv11n = YOLO(best_YOLOv11n) # Creates a new model instance by loading the best weights\n",
        "\n",
        "# Performs inference on one or more images, or on a video, by specifying the path in the source parameter.\n",
        "# conf=0.25 → Confidence threshold for considering a detection valid.\n",
        "preds_YOLOv11n = model_inf_YOLOv11n.predict(\n",
        "  # source='/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/test/images',\n",
        "  source='/content/AERALIS_YOLOv11n_local/test/images',\n",
        "  conf=0.25,\n",
        "  verbose=False, # disable on-screen printing\n",
        "  save=True\n",
        ")\n",
        "\n",
        "# Too many images to show!\n",
        "# for result in preds_YOLOv8n:\n",
        "#    result.show() # displays the predictions (eventually you can also save them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvBabxqEwNoZ"
      },
      "outputs": [],
      "source": [
        "# Create folders on Google Drive (first time only):\n",
        "!mkdir -p /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/results/annotated\n",
        "!mkdir -p /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IcdPypVwNlS"
      },
      "outputs": [],
      "source": [
        "# Copy annotated images:\n",
        "!cp -r /content/runs/detect/predict/* /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/results/annotated/\n",
        "\n",
        "# Copy the best weights (best.pt) after training:\n",
        "!cp /content/runs_finetune/person_yolov11n/weights/best.pt /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ_G1zHIwNh5"
      },
      "outputs": [],
      "source": [
        "# To download it directly to the computer:\n",
        "from google.colab import files\n",
        "files.download('/content/runs_finetune/person_yolov11n/weights/best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuIAUf12wNe3"
      },
      "outputs": [],
      "source": [
        "# Path of saved weights:\n",
        "weights_path_YOLOv11n = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/weights/best.pt'\n",
        "model_YOLOv11n = YOLO(weights_path_YOLOv11n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvfscJcCwNYK"
      },
      "outputs": [],
      "source": [
        "# Directory of annotated images\n",
        "annotated_dir = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/results/annotated'\n",
        "\n",
        "# Get all the .jpg files in the directory\n",
        "annotated_imgs_YOLOv11n = [\n",
        "    os.path.join(annotated_dir, f)\n",
        "    for f in os.listdir(annotated_dir)\n",
        "    if f.lower().endswith('.jpg')\n",
        "]\n",
        "\n",
        "# View the first 5 annotated images\n",
        "for img_path in annotated_imgs_YOLOv11n[:5]:\n",
        "  img = Image.open(img_path)\n",
        "  img.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G38RcMsn3wbs"
      },
      "source": [
        "### EfficientDet D0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGks42T3qV5d"
      },
      "source": [
        "Iniziamo ora ad allenare il modello EfficientDet. Utilizzeremo la libreria _effdet_ di _Ross Wightman_, che semplifica tutto il processo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dcke24mkOq5"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y effdet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6UqTALhKtQk",
        "outputId": "388d9fdb-4fb5-44b4-97a3-2dadf9e21057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/rwightman/efficientdet-pytorch\n",
            "  Cloning https://github.com/rwightman/efficientdet-pytorch to /tmp/pip-req-build-ydronsja\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rwightman/efficientdet-pytorch /tmp/pip-req-build-ydronsja\n",
            "  Resolved https://github.com/rwightman/efficientdet-pytorch to commit c6dff775a36cea0bf9b76c58e59f936411c5ce01\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from effdet==0.4.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from effdet==0.4.1) (0.21.0+cu124)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from effdet==0.4.1) (1.0.17)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet==0.4.1) (2.0.10)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet==0.4.1) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet==0.4.1) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet==0.4.1) (6.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->effdet==0.4.1) (2.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.2->effdet==0.4.1) (0.33.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.2->effdet==0.4.1) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (2025.7.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.12.1->effdet==0.4.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.1->effdet==0.4.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.1->effdet==0.4.1) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->effdet==0.4.1) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.9.2->effdet==0.4.1) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.9.2->effdet==0.4.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.9.2->effdet==0.4.1) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm>=0.9.2->effdet==0.4.1) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.1->effdet==0.4.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet==0.4.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet==0.4.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet==0.4.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm>=0.9.2->effdet==0.4.1) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: effdet\n",
            "  Building wheel for effdet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for effdet: filename=effdet-0.4.1-py3-none-any.whl size=112604 sha256=f7ac1c77df39df3acc9c1a8b69ef853f5a22928f2e7d1276edbe9956b27e4ba1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-liyhviiv/wheels/d8/57/ee/c8eee2d157cddf0d3b3bd98eb4eeac3144431ec2c941b7229e\n",
            "Successfully built effdet\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, effdet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed effdet-0.4.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/rwightman/efficientdet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a2fqr4ZDaJy"
      },
      "outputs": [],
      "source": [
        "# pip install albumentations[imgaug]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La7bHpqYgMc5"
      },
      "source": [
        "Copy the entire dataset folder from Google Drive to Colab's local storage to increase speed during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4wZGQePfWwB",
        "outputId": "6fc142ca-050f-49ac-cf55-9e79f8ee430a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copy completed\n"
          ]
        }
      ],
      "source": [
        "src = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0'\n",
        "dst = '/content/AERALIS_EfficientDet_D0_local'\n",
        "\n",
        "# Delete the destination if it already exists (shutil.rmtree), then recopy from scratch\n",
        "if os.path.exists(dst):\n",
        "  shutil.rmtree(dst)\n",
        "shutil.copytree(src, dst)\n",
        "print(\"Copy completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpzRCvcFh8uL",
        "outputId": "781b6748-3cd2-4ca7-b3bc-8fec2af4f534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         113G   56G   58G  50% /\n"
          ]
        }
      ],
      "source": [
        "!df -h / # It shows the total, used and free space on the root (/) of the Colab VM.\n",
        "\n",
        "# Avail column: space still available for your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsOp6xnPh8n5",
        "outputId": "c526accc-35b0-4f0f-e49e-1910acdcd662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.6G\t/content/AERALIS_EfficientDet_D0_local\n"
          ]
        }
      ],
      "source": [
        "# Show space used by your local folder\n",
        "!du -sh /content/AERALIS_EfficientDet_D0_local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co3zXroHh8db",
        "outputId": "d27884be-8817-43a7-caa9-622c40a2af2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140K\t/content/.config\n",
            "6.6G\t/content/AERALIS_EfficientDet_D0_local\n",
            "du: cannot access '/content/drive/.Encrypted/.shortcut-targets-by-id/1LQbD7p_iS5KLqGNdfrYEvsAx0i_bgB0h/projectUPV': No such file or directory\n",
            "71G\t/content/drive\n",
            "55M\t/content/sample_data\n",
            "77G\t/content/\n"
          ]
        }
      ],
      "source": [
        "# Show space occupied by various folders in /content/.\n",
        "!du -h --max-depth=1 /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x3SHYgUQhyrd"
      },
      "outputs": [],
      "source": [
        "# Main parameters\n",
        "BASE_DIR = dst\n",
        "VARIANT = 'tf_efficientdet_d0'  # Change to 'd1' or 'd2' for other versions\n",
        "IMAGE_SIZE = 512                # D0=512, D1=640, D2=768\n",
        "NUM_CLASSES = 1                 # only one class (ex: 'person')\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "PATIENCE = 20\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z297A5UnQKgn",
        "outputId": "0b311a9f-86bf-4e9f-98fb-261d8cf0b013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "# To see the available GPU\n",
        "print(torch.cuda.is_available()) # True = you have GPU --> if False then use device='cpu'\n",
        "print(torch.cuda.device_count()) # Name of GPU\n",
        "\n",
        "# If True and at least 1, you can use device=0.\n",
        "# If you don't have GPU: use device='cpu' (much slower).\n",
        "# Locally (not Colab): check with nvidia-smi from terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1IxVBx742gR"
      },
      "source": [
        "We keep only those images that contain at least one bounding box:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5ALOjtxdLC7"
      },
      "outputs": [],
      "source": [
        "class CocoDetectionTransformed(CocoDetection):\n",
        "  def __init__(self, img_folder, ann_file, transform=None):\n",
        "    super().__init__(img_folder, ann_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    # Build a set of image ids that have annotations with bbox area > 0\n",
        "    valid_img_ids = set()\n",
        "    for ann in self.coco.dataset['annotations']:\n",
        "      if ann['bbox'][2] > 1 and ann['bbox'][3] > 1:  # bbox width & height > 1 pixel\n",
        "        valid_img_ids.add(ann['image_id'])\n",
        "\n",
        "    # Find dataset indices corresponding to valid image ids\n",
        "    self.valid_indices = []\n",
        "    for i, img_info in enumerate(self.coco.dataset['images']):\n",
        "      if img_info['id'] in valid_img_ids:\n",
        "        self.valid_indices.append(i)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.valid_indices)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    real_idx = self.valid_indices[idx]\n",
        "    img, target = super().__getitem__(real_idx)\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HoWFVUeYEUuj"
      },
      "outputs": [],
      "source": [
        "class CocoDetectionTransformed(CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transform=None):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        # 1) Trovo tutti gli image_id con almeno una bbox >1px\n",
        "        valid_img_ids = {\n",
        "            ann['image_id']\n",
        "            for ann in self.coco.dataset['annotations']\n",
        "            if ann['bbox'][2] > 1 and ann['bbox'][3] > 1\n",
        "        }\n",
        "\n",
        "        # 2) Conservo solo gli indici delle immagini valide\n",
        "        self.valid_indices = [\n",
        "            idx for idx, img_info in enumerate(self.coco.dataset['images'])\n",
        "            if img_info['id'] in valid_img_ids\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      real_idx = self.valid_indices[idx]\n",
        "      img, target = super().__getitem__(real_idx)\n",
        "\n",
        "      if self.transform:\n",
        "          bboxes = [obj['bbox'] for obj in target]\n",
        "          labels = [0] * len(bboxes)\n",
        "          augmented = self.transform(\n",
        "              image=np.array(img),\n",
        "              bboxes=bboxes,\n",
        "              category_id=labels\n",
        "          )\n",
        "          img = augmented['image']\n",
        "          for obj, new_bbox in zip(target, augmented['bboxes']):\n",
        "              obj['bbox'] = list(new_bbox)\n",
        "\n",
        "      # qui estrai l'image_id CORRETTO\n",
        "      img_id = self.coco.dataset['images'][real_idx]['id']\n",
        "      return img, target, img_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vABZph4ziDWA"
      },
      "source": [
        "Creates a DataLoader to load images and annotations in COCO format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahv72mM34_Tj"
      },
      "outputs": [],
      "source": [
        "# COCO DataLoader Function\n",
        "def get_loader(img_dir, ann_path, shuffle):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), # Ensures that all images are the same size\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    ds = CocoDetectionTransformed(img_dir, ann_path, transform=transform)\n",
        "    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
        "                      collate_fn=lambda x: tuple(zip(*x)), num_workers=2)\n",
        "\n",
        "# collate_fn allows for variable object batches (e.g., several boxes per image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h-TDi6fJDu20"
      },
      "outputs": [],
      "source": [
        "def get_loader(img_dir, ann_path, shuffle):\n",
        "    # Trasformazioni congiunte immagine + bbox in formato COCO\n",
        "    transform = A.Compose([\n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        # A.Normalize(),\n",
        "        A.Normalize(\n",
        "          mean=(0.485, 0.456, 0.406),  # ImageNet mean\n",
        "          std=(0.229, 0.224, 0.225),   # ImageNet std\n",
        "          max_pixel_value=255.0\n",
        "        ),\n",
        "        ToTensorV2(), # converte in torch.Tensor\n",
        "    ], bbox_params=A.BboxParams(\n",
        "        format='coco',\n",
        "        label_fields=['category_id'],\n",
        "        min_area=1.0,          # scarta bbox con area < 1px\n",
        "        min_visibility=0.0     # scarta bbox con visibilità < 0%\n",
        "      ))\n",
        "\n",
        "    # Passiamo l’albumentations transform al dataset\n",
        "    ds = CocoDetectionTransformed(img_dir, ann_path, transform=transform)\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNwmMttN5oVe"
      },
      "outputs": [],
      "source": [
        "# COCO DataLoader Function for inference (con Resize per matchare training)\n",
        "def get_loader_test(img_dir, ann_path):\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "  ds = CocoDetectionTransformed(img_dir, ann_path, transform=transform)\n",
        "  return DataLoader(ds, batch_size=1, shuffle=False,\n",
        "                    collate_fn=lambda x: tuple(zip(*x)), num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i3SYA2i9Ez-T"
      },
      "outputs": [],
      "source": [
        "def get_loader_test(img_dir, ann_path):\n",
        "    transform = A.Compose([\n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        # A.Normalize(),\n",
        "        A.Normalize(\n",
        "          mean=(0.485, 0.456, 0.406),  # ImageNet mean\n",
        "          std=(0.229, 0.224, 0.225),   # ImageNet std\n",
        "          max_pixel_value=255.0\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(\n",
        "        format='coco',\n",
        "        label_fields=['category_id'],\n",
        "        min_area=1.0,          # scarta bbox con area < 1px\n",
        "        min_visibility=0.0     # scarta bbox con visibilità < 0%\n",
        "      ))\n",
        "\n",
        "    ds = CocoDetectionTransformed(img_dir, ann_path, transform=transform)\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xno9syOliJrp",
        "outputId": "8a8376d2-c3e9-4c84-a5ce-a38a128c9fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "# Loading the 3 sets (train, val, test):\n",
        "train_loader = get_loader(f'{BASE_DIR}/train/images', f'{BASE_DIR}/annotations_train.json', True)\n",
        "val_loader   = get_loader(f'{BASE_DIR}/val/images', f'{BASE_DIR}/annotations_val.json', False)\n",
        "test_loader = get_loader_test(f'{BASE_DIR}/test/images', f'{BASE_DIR}/annotations_test.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwm1kP1XlLxJ"
      },
      "source": [
        "Adam (short for Adaptive Moment Estimation) is one of the most widely used optimizers in deep learning.\n",
        "\n",
        "The optimizer is an algorithm that updates model weights during training to reduce loss.\n",
        "- _AdamW_ is a variant of Adam, which also includes a weight penalty (weight decay) in a more correct way than classical Adam.\n",
        "- The _W_ stands for “Weight Decay fix”: it improves regularization and reduces overfitting.\n",
        "\n",
        "In this way:\n",
        "- Converges quickly (like Adam)\n",
        "- Reduces overfitting better than Adam\n",
        "- Is the standard in many modern models (such as BERT, EfficientDet, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "22a2b2db93404820b27c34969f8e740b",
            "270f3733dd40486085def7bdea1f96ab",
            "897dfc3ada0945ab98366c5f0e444bfb",
            "1e2bc6b00cca4bb6812c08915254cfc0",
            "53a7443a0e034e279b7cb2c8dd228df8",
            "48ebfe749d424a4eb60a25c206c784c6",
            "0fe99e253405490d815a18395913243e",
            "2335ad0847bc4cb588218de1c12c7ab6",
            "446727a9c8964dc7a028a1803e6be8eb",
            "d8e1b69121b44ff6a392b1cb0eaf3b8e",
            "2eb1180bbe8147c386238be6ec872106"
          ]
        },
        "id": "sppduScUh4m_",
        "outputId": "5d588b70-95e8-42a0-bdb0-b53ac7a89056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22a2b2db93404820b27c34969f8e740b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        }
      ],
      "source": [
        "# Model initialization\n",
        "config = get_efficientdet_config(VARIANT) # load configuration for tf_efficientdet_d0\n",
        "config.num_classes = NUM_CLASSES # sets the number of classes\n",
        "config.image_size = (IMAGE_SIZE, IMAGE_SIZE) # sets the image size\n",
        "# config.image_size = None # This tells the model to accept input of any size and apply the correct resizing internally\n",
        "net = EfficientDet(config, pretrained_backbone=True) # Initialize the model (use pre-trained backbone)\n",
        "# net = EfficientDet(config)\n",
        "net.class_net = HeadNet(config, num_outputs=NUM_CLASSES) # Create the classifier (HeadNet)\n",
        "model = DetBenchTrain(net, config).to(DEVICE) # He wraps it in a training module\n",
        "\n",
        "# CONGELAMENTO del backbone\n",
        "# Blocca i gradenti del backbone: nelle prime epoche si addestrano solo class_net e box_net\n",
        "for param in net.backbone.parameters():\n",
        "    param.requires_grad = False  # freeze backbone layers :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "# Ottimizzatore con solo parametri “trainabili”\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4\n",
        ")  # learning rate iniziale per testate :contentReference[oaicite:4]{index=4}\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # Use AdamW optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnMbbVsBl2TE"
      },
      "source": [
        "HeadNet is the final block of the EfficientDet model that does box classification and regression.\n",
        "\n",
        "EfficientDet has two “heads”:\n",
        "- Classification head: says “what” is in the image (object class)\n",
        "- Regression head: says “where” it is (bounding box)\n",
        "\n",
        "When we use: _net.class_net = HeadNet(config, num_outputs=NUM_CLASSES)_ we are customizing the classification head of the model to use our number of classes, for example 1 for ‘person’.\n",
        "\n",
        "Without this change, the model would remain pre-configured for COCO (80 classes), so it would miss everything in the custom dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TYloEaImDGr"
      },
      "source": [
        "Now initialize values to track the best validation and apply early stopping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD3YDkIbks7C",
        "outputId": "514c4cdc-b7e5-4400-bad6-5f6c987f379a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[Epoch 60] Batch loss: 0.5305060744285583\n",
            "[Epoch 60] Batch loss: 0.7975022792816162\n",
            "[Epoch 60] Batch loss: 0.7384965419769287\n",
            "[Epoch 60] Batch loss: 0.7670215368270874\n",
            "[Epoch 60] Batch loss: 0.8271852731704712\n",
            "[Epoch 61/100] Val loss: 1.1332\n",
            "Best saved model.\n",
            "[Epoch 61] Batch loss: 0.9066474437713623\n",
            "[Epoch 61] Batch loss: 0.6576187610626221\n",
            "[Epoch 61] Batch loss: 1.01454496383667\n",
            "[Epoch 61] Batch loss: 0.7341536283493042\n",
            "[Epoch 61] Batch loss: 0.5500481724739075\n",
            "[Epoch 61] Batch loss: 0.7822741270065308\n",
            "[Epoch 61] Batch loss: 0.7279778122901917\n",
            "[Epoch 61] Batch loss: 0.6833098530769348\n",
            "[Epoch 61] Batch loss: 0.5590689778327942\n",
            "[Epoch 61] Batch loss: 0.5873044729232788\n",
            "[Epoch 61] Batch loss: 0.6472947597503662\n",
            "[Epoch 61] Batch loss: 0.5579423308372498\n",
            "[Epoch 61] Batch loss: 0.5199979543685913\n",
            "[Epoch 61] Batch loss: 0.5153982043266296\n",
            "[Epoch 61] Batch loss: 0.6891880035400391\n",
            "[Epoch 61] Batch loss: 0.6157970428466797\n",
            "[Epoch 61] Batch loss: 0.826973557472229\n",
            "[Epoch 61] Batch loss: 0.7755957841873169\n",
            "[Epoch 61] Batch loss: 0.7549154162406921\n",
            "[Epoch 61] Batch loss: 0.6852846145629883\n",
            "[Epoch 61] Batch loss: 0.6803459525108337\n",
            "[Epoch 61] Batch loss: 0.6028012037277222\n",
            "[Epoch 61] Batch loss: 0.4968043565750122\n",
            "[Epoch 61] Batch loss: 0.6893233060836792\n",
            "[Epoch 61] Batch loss: 0.7404739856719971\n",
            "[Epoch 61] Batch loss: 0.5575047731399536\n",
            "[Epoch 61] Batch loss: 0.6415274143218994\n",
            "[Epoch 61] Batch loss: 0.5635110139846802\n",
            "[Epoch 61] Batch loss: 0.6533417105674744\n",
            "[Epoch 61] Batch loss: 0.6120111346244812\n",
            "[Epoch 61] Batch loss: 0.7256958484649658\n",
            "[Epoch 61] Batch loss: 0.8977199196815491\n",
            "[Epoch 61] Batch loss: 0.643040120601654\n",
            "[Epoch 61] Batch loss: 0.6463029980659485\n",
            "[Epoch 61] Batch loss: 0.6192285418510437\n",
            "[Epoch 61] Batch loss: 0.5054357051849365\n",
            "[Epoch 61] Batch loss: 0.6964835524559021\n",
            "[Epoch 61] Batch loss: 0.7778267860412598\n",
            "[Epoch 61] Batch loss: 0.6149513721466064\n",
            "[Epoch 61] Batch loss: 0.7166133522987366\n",
            "[Epoch 61] Batch loss: 0.6390795707702637\n",
            "[Epoch 61] Batch loss: 0.628914475440979\n",
            "[Epoch 61] Batch loss: 0.6259021759033203\n",
            "[Epoch 61] Batch loss: 0.5896183848381042\n",
            "[Epoch 61] Batch loss: 0.6082771420478821\n",
            "[Epoch 61] Batch loss: 0.5341793894767761\n",
            "[Epoch 61] Batch loss: 0.6375604271888733\n",
            "[Epoch 61] Batch loss: 0.7597497701644897\n",
            "[Epoch 61] Batch loss: 0.6653218269348145\n",
            "[Epoch 61] Batch loss: 0.65947026014328\n",
            "[Epoch 61] Batch loss: 0.7967695593833923\n",
            "[Epoch 61] Batch loss: 0.6003108024597168\n",
            "[Epoch 61] Batch loss: 0.5488579273223877\n",
            "[Epoch 61] Batch loss: 0.5900924205780029\n",
            "[Epoch 61] Batch loss: 0.5267305970191956\n",
            "[Epoch 61] Batch loss: 0.5811664462089539\n",
            "[Epoch 61] Batch loss: 0.6101045608520508\n",
            "[Epoch 61] Batch loss: 0.6161142587661743\n",
            "[Epoch 61] Batch loss: 0.615593671798706\n",
            "[Epoch 61] Batch loss: 0.5423957705497742\n",
            "[Epoch 61] Batch loss: 0.5891315937042236\n",
            "[Epoch 61] Batch loss: 0.6569238901138306\n",
            "[Epoch 61] Batch loss: 0.7679816484451294\n",
            "[Epoch 61] Batch loss: 0.7356640100479126\n",
            "[Epoch 61] Batch loss: 0.5225946307182312\n",
            "[Epoch 61] Batch loss: 0.790259599685669\n",
            "[Epoch 61] Batch loss: 0.5048676133155823\n",
            "[Epoch 61] Batch loss: 0.5608895421028137\n",
            "[Epoch 61] Batch loss: 0.5508264303207397\n",
            "[Epoch 61] Batch loss: 0.5653555393218994\n",
            "[Epoch 61] Batch loss: 0.7321421504020691\n",
            "[Epoch 61] Batch loss: 0.6497007608413696\n",
            "[Epoch 61] Batch loss: 0.5776305794715881\n",
            "[Epoch 61] Batch loss: 0.7582809329032898\n",
            "[Epoch 61] Batch loss: 0.5351809859275818\n",
            "[Epoch 61] Batch loss: 0.6318103671073914\n",
            "[Epoch 61] Batch loss: 0.7105163335800171\n",
            "[Epoch 61] Batch loss: 0.6096604466438293\n",
            "[Epoch 61] Batch loss: 0.5933572053909302\n",
            "[Epoch 61] Batch loss: 0.5380681157112122\n",
            "[Epoch 61] Batch loss: 0.5551494359970093\n",
            "[Epoch 61] Batch loss: 0.8094371557235718\n",
            "[Epoch 61] Batch loss: 0.9131829142570496\n",
            "[Epoch 61] Batch loss: 0.6787582635879517\n",
            "[Epoch 61] Batch loss: 0.8371226787567139\n",
            "[Epoch 61] Batch loss: 0.5545091032981873\n",
            "[Epoch 61] Batch loss: 0.6934348344802856\n",
            "[Epoch 61] Batch loss: 0.5071225166320801\n",
            "[Epoch 61] Batch loss: 0.809749960899353\n",
            "[Epoch 61] Batch loss: 0.5476064682006836\n",
            "[Epoch 61] Batch loss: 0.6301096081733704\n",
            "[Epoch 61] Batch loss: 0.5498462319374084\n",
            "[Epoch 61] Batch loss: 0.6779390573501587\n",
            "[Epoch 61] Batch loss: 0.652001678943634\n",
            "[Epoch 61] Batch loss: 0.759456992149353\n",
            "[Epoch 61] Batch loss: 0.7037620544433594\n",
            "[Epoch 61] Batch loss: 0.6204023361206055\n",
            "[Epoch 61] Batch loss: 0.6610577702522278\n",
            "[Epoch 61] Batch loss: 0.5460346937179565\n",
            "[Epoch 61] Batch loss: 0.5290611386299133\n",
            "[Epoch 61] Batch loss: 0.7515875101089478\n",
            "[Epoch 61] Batch loss: 0.7415333986282349\n",
            "[Epoch 61] Batch loss: 0.6327515244483948\n",
            "[Epoch 61] Batch loss: 0.5948494076728821\n",
            "[Epoch 61] Batch loss: 0.7998108863830566\n",
            "[Epoch 61] Batch loss: 0.6119349598884583\n",
            "[Epoch 61] Batch loss: 0.607067883014679\n",
            "[Epoch 61] Batch loss: 0.6265418529510498\n",
            "[Epoch 61] Batch loss: 0.7462255954742432\n",
            "[Epoch 61] Batch loss: 0.5280209183692932\n",
            "[Epoch 61] Batch loss: 0.7375147938728333\n",
            "[Epoch 61] Batch loss: 0.5919649004936218\n",
            "[Epoch 61] Batch loss: 0.5710765719413757\n",
            "[Epoch 61] Batch loss: 0.6758990287780762\n",
            "[Epoch 61] Batch loss: 0.596638560295105\n",
            "[Epoch 61] Batch loss: 0.6825278997421265\n",
            "[Epoch 61] Batch loss: 0.5605394244194031\n",
            "[Epoch 61] Batch loss: 0.5661791563034058\n",
            "[Epoch 61] Batch loss: 0.7521846890449524\n",
            "[Epoch 61] Batch loss: 0.7486684918403625\n",
            "[Epoch 61] Batch loss: 0.5260636210441589\n",
            "[Epoch 61] Batch loss: 0.6276687979698181\n",
            "[Epoch 61] Batch loss: 0.5317999124526978\n",
            "[Epoch 61] Batch loss: 0.5817122459411621\n",
            "[Epoch 61] Batch loss: 0.6783415079116821\n",
            "[Epoch 61] Batch loss: 0.5747809410095215\n",
            "[Epoch 62/100] Val loss: 1.0953\n",
            "Best saved model.\n",
            "[Epoch 62] Batch loss: 0.6469631195068359\n",
            "[Epoch 62] Batch loss: 0.7263171672821045\n",
            "[Epoch 62] Batch loss: 0.6340684294700623\n",
            "[Epoch 62] Batch loss: 0.5490351319313049\n",
            "[Epoch 62] Batch loss: 0.6411523818969727\n",
            "[Epoch 62] Batch loss: 0.6805596351623535\n",
            "[Epoch 62] Batch loss: 0.5921417474746704\n",
            "[Epoch 62] Batch loss: 0.6905213594436646\n",
            "[Epoch 62] Batch loss: 0.48712044954299927\n",
            "[Epoch 62] Batch loss: 0.5837048888206482\n",
            "[Epoch 62] Batch loss: 0.6539084911346436\n",
            "[Epoch 62] Batch loss: 0.5649290084838867\n",
            "[Epoch 62] Batch loss: 0.6391251087188721\n",
            "[Epoch 62] Batch loss: 0.6086292266845703\n",
            "[Epoch 62] Batch loss: 0.5931969285011292\n",
            "[Epoch 62] Batch loss: 0.504217803478241\n",
            "[Epoch 62] Batch loss: 0.7086750268936157\n",
            "[Epoch 62] Batch loss: 0.6674959063529968\n",
            "[Epoch 62] Batch loss: 0.5833001732826233\n",
            "[Epoch 62] Batch loss: 0.6105407476425171\n",
            "[Epoch 62] Batch loss: 0.7779160141944885\n",
            "[Epoch 62] Batch loss: 0.5420234203338623\n",
            "[Epoch 62] Batch loss: 0.7418875098228455\n",
            "[Epoch 62] Batch loss: 0.5438458323478699\n",
            "[Epoch 62] Batch loss: 0.7162103652954102\n",
            "[Epoch 62] Batch loss: 0.5168902277946472\n",
            "[Epoch 62] Batch loss: 0.5444435477256775\n",
            "[Epoch 62] Batch loss: 0.7238144874572754\n",
            "[Epoch 62] Batch loss: 0.6940654516220093\n",
            "[Epoch 62] Batch loss: 0.6510826945304871\n",
            "[Epoch 62] Batch loss: 0.5672100782394409\n",
            "[Epoch 62] Batch loss: 0.6074274778366089\n",
            "[Epoch 62] Batch loss: 0.4839777946472168\n",
            "[Epoch 62] Batch loss: 0.6291438341140747\n",
            "[Epoch 62] Batch loss: 0.7429142594337463\n",
            "[Epoch 62] Batch loss: 0.653784453868866\n",
            "[Epoch 62] Batch loss: 0.6751211881637573\n",
            "[Epoch 62] Batch loss: 0.6175865530967712\n",
            "[Epoch 62] Batch loss: 0.5746610760688782\n",
            "[Epoch 62] Batch loss: 0.6039851307868958\n",
            "[Epoch 62] Batch loss: 0.6664509177207947\n",
            "[Epoch 62] Batch loss: 0.5984545350074768\n",
            "[Epoch 62] Batch loss: 0.6547932624816895\n",
            "[Epoch 62] Batch loss: 0.7086824178695679\n",
            "[Epoch 62] Batch loss: 0.6296176910400391\n",
            "[Epoch 62] Batch loss: 0.5449933409690857\n",
            "[Epoch 62] Batch loss: 0.6456425189971924\n",
            "[Epoch 62] Batch loss: 0.7499784231185913\n",
            "[Epoch 62] Batch loss: 0.5774495601654053\n",
            "[Epoch 62] Batch loss: 0.61223304271698\n",
            "[Epoch 62] Batch loss: 0.6915261149406433\n",
            "[Epoch 62] Batch loss: 0.5385614037513733\n",
            "[Epoch 62] Batch loss: 0.5920720100402832\n",
            "[Epoch 62] Batch loss: 0.5507755279541016\n",
            "[Epoch 62] Batch loss: 0.8398767709732056\n",
            "[Epoch 62] Batch loss: 0.6438511610031128\n",
            "[Epoch 62] Batch loss: 0.6800627708435059\n",
            "[Epoch 62] Batch loss: 0.847629189491272\n",
            "[Epoch 62] Batch loss: 0.8331071734428406\n",
            "[Epoch 62] Batch loss: 0.6988617777824402\n",
            "[Epoch 62] Batch loss: 0.5842435956001282\n",
            "[Epoch 62] Batch loss: 0.6075002551078796\n",
            "[Epoch 62] Batch loss: 0.6312970519065857\n",
            "[Epoch 62] Batch loss: 0.5360417366027832\n",
            "[Epoch 62] Batch loss: 0.788808286190033\n",
            "[Epoch 62] Batch loss: 0.6468105316162109\n",
            "[Epoch 62] Batch loss: 0.6776472330093384\n",
            "[Epoch 62] Batch loss: 0.6720462441444397\n",
            "[Epoch 62] Batch loss: 0.7426198720932007\n",
            "[Epoch 62] Batch loss: 0.5775316953659058\n",
            "[Epoch 62] Batch loss: 0.6126682162284851\n",
            "[Epoch 62] Batch loss: 0.6709480285644531\n",
            "[Epoch 62] Batch loss: 0.8273615837097168\n",
            "[Epoch 62] Batch loss: 0.6100552082061768\n",
            "[Epoch 62] Batch loss: 0.6847336888313293\n",
            "[Epoch 62] Batch loss: 0.5365666151046753\n",
            "[Epoch 62] Batch loss: 0.6880413889884949\n",
            "[Epoch 62] Batch loss: 0.5833547115325928\n",
            "[Epoch 62] Batch loss: 0.6033137440681458\n",
            "[Epoch 62] Batch loss: 0.517998993396759\n",
            "[Epoch 62] Batch loss: 0.7904256582260132\n",
            "[Epoch 62] Batch loss: 0.6974459886550903\n",
            "[Epoch 62] Batch loss: 0.5186313986778259\n",
            "[Epoch 62] Batch loss: 0.5358282923698425\n",
            "[Epoch 62] Batch loss: 0.5570297837257385\n",
            "[Epoch 62] Batch loss: 0.6892293095588684\n",
            "[Epoch 62] Batch loss: 0.7167154550552368\n",
            "[Epoch 62] Batch loss: 0.555191159248352\n",
            "[Epoch 62] Batch loss: 0.7038539052009583\n",
            "[Epoch 62] Batch loss: 0.7073507905006409\n",
            "[Epoch 62] Batch loss: 0.6650022268295288\n",
            "[Epoch 62] Batch loss: 0.5922390818595886\n",
            "[Epoch 62] Batch loss: 0.7223320007324219\n",
            "[Epoch 62] Batch loss: 0.6211632490158081\n",
            "[Epoch 62] Batch loss: 0.671676516532898\n",
            "[Epoch 62] Batch loss: 0.5522815585136414\n",
            "[Epoch 62] Batch loss: 0.567914605140686\n",
            "[Epoch 62] Batch loss: 0.49900856614112854\n",
            "[Epoch 62] Batch loss: 0.6148673295974731\n",
            "[Epoch 62] Batch loss: 0.6946514248847961\n",
            "[Epoch 62] Batch loss: 0.5660344958305359\n",
            "[Epoch 62] Batch loss: 0.6166268587112427\n",
            "[Epoch 62] Batch loss: 0.5911344885826111\n",
            "[Epoch 62] Batch loss: 0.5268016457557678\n",
            "[Epoch 62] Batch loss: 0.6555863618850708\n",
            "[Epoch 62] Batch loss: 0.6151515245437622\n",
            "[Epoch 62] Batch loss: 0.6203183531761169\n",
            "[Epoch 62] Batch loss: 0.5654556155204773\n",
            "[Epoch 62] Batch loss: 0.5704582929611206\n",
            "[Epoch 62] Batch loss: 0.5235088467597961\n",
            "[Epoch 62] Batch loss: 0.6513681411743164\n",
            "[Epoch 62] Batch loss: 0.6169524788856506\n",
            "[Epoch 62] Batch loss: 0.6001006364822388\n",
            "[Epoch 62] Batch loss: 0.5905567407608032\n",
            "[Epoch 62] Batch loss: 0.56456458568573\n",
            "[Epoch 62] Batch loss: 0.6651980876922607\n",
            "[Epoch 62] Batch loss: 0.570000946521759\n",
            "[Epoch 62] Batch loss: 0.5453786253929138\n",
            "[Epoch 62] Batch loss: 0.5696429014205933\n",
            "[Epoch 62] Batch loss: 0.5661167502403259\n",
            "[Epoch 62] Batch loss: 0.6859570741653442\n",
            "[Epoch 62] Batch loss: 0.647822380065918\n",
            "[Epoch 62] Batch loss: 0.9148750305175781\n",
            "[Epoch 62] Batch loss: 0.5268661975860596\n",
            "[Epoch 62] Batch loss: 0.5082476139068604\n",
            "[Epoch 62] Batch loss: 0.6865624189376831\n",
            "[Epoch 63/100] Val loss: 1.0762\n",
            "Best saved model.\n",
            "[Epoch 63] Batch loss: 0.6333130598068237\n",
            "[Epoch 63] Batch loss: 0.7607091069221497\n",
            "[Epoch 63] Batch loss: 0.7191065549850464\n",
            "[Epoch 63] Batch loss: 0.7316034436225891\n",
            "[Epoch 63] Batch loss: 0.6743382215499878\n",
            "[Epoch 63] Batch loss: 0.624312698841095\n",
            "[Epoch 63] Batch loss: 0.6110921502113342\n",
            "[Epoch 63] Batch loss: 0.7184144258499146\n",
            "[Epoch 63] Batch loss: 0.7608113288879395\n",
            "[Epoch 63] Batch loss: 0.6409629583358765\n",
            "[Epoch 63] Batch loss: 0.5664352774620056\n",
            "[Epoch 63] Batch loss: 0.6356910467147827\n",
            "[Epoch 63] Batch loss: 0.816537618637085\n",
            "[Epoch 63] Batch loss: 0.5261309742927551\n",
            "[Epoch 63] Batch loss: 0.7379976511001587\n",
            "[Epoch 63] Batch loss: 0.5795674920082092\n",
            "[Epoch 63] Batch loss: 0.7299212217330933\n",
            "[Epoch 63] Batch loss: 0.5899707078933716\n",
            "[Epoch 63] Batch loss: 0.7706631422042847\n",
            "[Epoch 63] Batch loss: 0.5962299108505249\n",
            "[Epoch 63] Batch loss: 0.5294865369796753\n",
            "[Epoch 63] Batch loss: 0.5874772071838379\n",
            "[Epoch 63] Batch loss: 0.6635110378265381\n",
            "[Epoch 63] Batch loss: 0.7650574445724487\n",
            "[Epoch 63] Batch loss: 0.5681259036064148\n",
            "[Epoch 63] Batch loss: 0.5737102627754211\n",
            "[Epoch 63] Batch loss: 0.6832979321479797\n",
            "[Epoch 63] Batch loss: 0.5857104063034058\n",
            "[Epoch 63] Batch loss: 0.6880614161491394\n",
            "[Epoch 63] Batch loss: 0.5726996064186096\n",
            "[Epoch 63] Batch loss: 0.7610398530960083\n",
            "[Epoch 63] Batch loss: 0.7450034618377686\n",
            "[Epoch 63] Batch loss: 0.5383591055870056\n",
            "[Epoch 63] Batch loss: 0.6794828176498413\n",
            "[Epoch 63] Batch loss: 0.6577145457267761\n",
            "[Epoch 63] Batch loss: 0.6030810475349426\n",
            "[Epoch 63] Batch loss: 0.6286503076553345\n",
            "[Epoch 63] Batch loss: 0.5596877932548523\n",
            "[Epoch 63] Batch loss: 0.4958037734031677\n",
            "[Epoch 63] Batch loss: 0.586821973323822\n",
            "[Epoch 63] Batch loss: 0.7104213237762451\n",
            "[Epoch 63] Batch loss: 0.5604007244110107\n",
            "[Epoch 63] Batch loss: 0.523790717124939\n",
            "[Epoch 63] Batch loss: 0.6589030027389526\n",
            "[Epoch 63] Batch loss: 0.5407385230064392\n",
            "[Epoch 63] Batch loss: 0.6256189942359924\n",
            "[Epoch 63] Batch loss: 0.6741929054260254\n",
            "[Epoch 63] Batch loss: 0.7379770874977112\n",
            "[Epoch 63] Batch loss: 0.5593970417976379\n",
            "[Epoch 63] Batch loss: 0.6836720705032349\n",
            "[Epoch 63] Batch loss: 0.6984894275665283\n",
            "[Epoch 63] Batch loss: 0.5857689380645752\n",
            "[Epoch 63] Batch loss: 0.5522255897521973\n",
            "[Epoch 63] Batch loss: 0.6082392930984497\n",
            "[Epoch 63] Batch loss: 0.6605409383773804\n",
            "[Epoch 63] Batch loss: 0.6536436080932617\n",
            "[Epoch 63] Batch loss: 0.5117200613021851\n",
            "[Epoch 63] Batch loss: 0.5904235243797302\n",
            "[Epoch 63] Batch loss: 0.5564624071121216\n",
            "[Epoch 63] Batch loss: 0.6619462370872498\n",
            "[Epoch 63] Batch loss: 0.617952823638916\n",
            "[Epoch 63] Batch loss: 0.6693784594535828\n",
            "[Epoch 63] Batch loss: 0.6902572512626648\n",
            "[Epoch 63] Batch loss: 0.5575965046882629\n",
            "[Epoch 63] Batch loss: 0.5370216965675354\n",
            "[Epoch 63] Batch loss: 0.6287429928779602\n",
            "[Epoch 63] Batch loss: 0.6302089691162109\n",
            "[Epoch 63] Batch loss: 0.6035293340682983\n",
            "[Epoch 63] Batch loss: 0.6841347217559814\n",
            "[Epoch 63] Batch loss: 0.5002166032791138\n",
            "[Epoch 63] Batch loss: 0.5693380832672119\n",
            "[Epoch 63] Batch loss: 0.48299163579940796\n",
            "[Epoch 63] Batch loss: 0.547249436378479\n",
            "[Epoch 63] Batch loss: 0.5725674629211426\n",
            "[Epoch 63] Batch loss: 0.5332522392272949\n",
            "[Epoch 63] Batch loss: 0.6514884829521179\n",
            "[Epoch 63] Batch loss: 0.6775883436203003\n",
            "[Epoch 63] Batch loss: 0.5581119656562805\n",
            "[Epoch 63] Batch loss: 0.5635989308357239\n",
            "[Epoch 63] Batch loss: 0.5886474251747131\n",
            "[Epoch 63] Batch loss: 0.6173847913742065\n",
            "[Epoch 63] Batch loss: 0.6003462076187134\n",
            "[Epoch 63] Batch loss: 0.6133767366409302\n",
            "[Epoch 63] Batch loss: 0.5780534148216248\n",
            "[Epoch 63] Batch loss: 0.6167048811912537\n",
            "[Epoch 63] Batch loss: 0.5295836925506592\n",
            "[Epoch 63] Batch loss: 0.5809682011604309\n",
            "[Epoch 63] Batch loss: 0.49701938033103943\n",
            "[Epoch 63] Batch loss: 0.8623802661895752\n",
            "[Epoch 63] Batch loss: 0.5174268484115601\n",
            "[Epoch 63] Batch loss: 0.620012640953064\n",
            "[Epoch 63] Batch loss: 0.613452672958374\n",
            "[Epoch 63] Batch loss: 0.5217768549919128\n",
            "[Epoch 63] Batch loss: 0.6359124183654785\n",
            "[Epoch 63] Batch loss: 0.6308903098106384\n",
            "[Epoch 63] Batch loss: 0.6063008904457092\n",
            "[Epoch 63] Batch loss: 0.6466548442840576\n",
            "[Epoch 63] Batch loss: 0.5911186933517456\n",
            "[Epoch 63] Batch loss: 0.5550645589828491\n",
            "[Epoch 63] Batch loss: 0.5115612745285034\n",
            "[Epoch 63] Batch loss: 0.5722874402999878\n",
            "[Epoch 63] Batch loss: 0.5065149068832397\n",
            "[Epoch 63] Batch loss: 0.6054043769836426\n",
            "[Epoch 63] Batch loss: 0.5571396350860596\n",
            "[Epoch 63] Batch loss: 0.602852463722229\n",
            "[Epoch 63] Batch loss: 0.6126524209976196\n",
            "[Epoch 63] Batch loss: 0.5619842410087585\n",
            "[Epoch 63] Batch loss: 0.6241275668144226\n",
            "[Epoch 63] Batch loss: 0.586677074432373\n",
            "[Epoch 63] Batch loss: 0.5027636885643005\n",
            "[Epoch 63] Batch loss: 0.6538842916488647\n",
            "[Epoch 63] Batch loss: 0.7569793462753296\n",
            "[Epoch 63] Batch loss: 0.705217182636261\n",
            "[Epoch 63] Batch loss: 0.6589422821998596\n",
            "[Epoch 63] Batch loss: 0.6066420078277588\n",
            "[Epoch 63] Batch loss: 0.6101821660995483\n",
            "[Epoch 63] Batch loss: 0.49881410598754883\n",
            "[Epoch 63] Batch loss: 0.6362891793251038\n",
            "[Epoch 63] Batch loss: 0.612076461315155\n",
            "[Epoch 63] Batch loss: 0.47377172112464905\n",
            "[Epoch 63] Batch loss: 0.6956892609596252\n",
            "[Epoch 63] Batch loss: 0.5506689548492432\n",
            "[Epoch 63] Batch loss: 0.5431863069534302\n",
            "[Epoch 63] Batch loss: 0.5691057443618774\n",
            "[Epoch 63] Batch loss: 0.6108884215354919\n",
            "[Epoch 63] Batch loss: 0.5327128767967224\n",
            "[Epoch 64/100] Val loss: 1.0446\n",
            "Best saved model.\n",
            "[Epoch 64] Batch loss: 0.562659502029419\n",
            "[Epoch 64] Batch loss: 0.5709813833236694\n",
            "[Epoch 64] Batch loss: 0.5616706013679504\n",
            "[Epoch 64] Batch loss: 0.6538040041923523\n",
            "[Epoch 64] Batch loss: 0.6300674676895142\n",
            "[Epoch 64] Batch loss: 0.7140557765960693\n",
            "[Epoch 64] Batch loss: 0.5975133180618286\n",
            "[Epoch 64] Batch loss: 0.6475242972373962\n",
            "[Epoch 64] Batch loss: 0.7167104482650757\n",
            "[Epoch 64] Batch loss: 0.5624690651893616\n",
            "[Epoch 64] Batch loss: 0.5389821529388428\n",
            "[Epoch 64] Batch loss: 0.7572057247161865\n",
            "[Epoch 64] Batch loss: 0.634904682636261\n",
            "[Epoch 64] Batch loss: 0.5111430287361145\n",
            "[Epoch 64] Batch loss: 0.5278176069259644\n",
            "[Epoch 64] Batch loss: 0.5169872641563416\n",
            "[Epoch 64] Batch loss: 0.5554449558258057\n",
            "[Epoch 64] Batch loss: 0.5318104028701782\n",
            "[Epoch 64] Batch loss: 0.5885568857192993\n",
            "[Epoch 64] Batch loss: 0.7598398923873901\n",
            "[Epoch 64] Batch loss: 0.5273400545120239\n",
            "[Epoch 64] Batch loss: 0.6322749257087708\n",
            "[Epoch 64] Batch loss: 0.5793135166168213\n",
            "[Epoch 64] Batch loss: 0.5722919702529907\n",
            "[Epoch 64] Batch loss: 0.5631483197212219\n",
            "[Epoch 64] Batch loss: 0.6319910883903503\n",
            "[Epoch 64] Batch loss: 0.7095465660095215\n",
            "[Epoch 64] Batch loss: 0.5840500593185425\n",
            "[Epoch 64] Batch loss: 0.8017891645431519\n",
            "[Epoch 64] Batch loss: 0.8603129386901855\n",
            "[Epoch 64] Batch loss: 0.5859779119491577\n",
            "[Epoch 64] Batch loss: 0.49888312816619873\n",
            "[Epoch 64] Batch loss: 1.019877552986145\n",
            "[Epoch 64] Batch loss: 0.8096280097961426\n",
            "[Epoch 64] Batch loss: 0.6082285642623901\n",
            "[Epoch 64] Batch loss: 0.5371813178062439\n",
            "[Epoch 64] Batch loss: 0.5309349894523621\n",
            "[Epoch 64] Batch loss: 0.62549889087677\n",
            "[Epoch 64] Batch loss: 0.5639369487762451\n",
            "[Epoch 64] Batch loss: 0.6959349513053894\n",
            "[Epoch 64] Batch loss: 0.5189858078956604\n",
            "[Epoch 64] Batch loss: 0.540580689907074\n",
            "[Epoch 64] Batch loss: 0.5908056497573853\n",
            "[Epoch 64] Batch loss: 0.5317203402519226\n",
            "[Epoch 64] Batch loss: 0.7704781293869019\n",
            "[Epoch 64] Batch loss: 0.5556443929672241\n",
            "[Epoch 64] Batch loss: 0.5624960660934448\n",
            "[Epoch 64] Batch loss: 0.6219324469566345\n",
            "[Epoch 64] Batch loss: 0.5020627379417419\n",
            "[Epoch 64] Batch loss: 0.6732036471366882\n",
            "[Epoch 64] Batch loss: 0.5561802387237549\n",
            "[Epoch 64] Batch loss: 0.5423300266265869\n",
            "[Epoch 64] Batch loss: 0.5988785028457642\n",
            "[Epoch 64] Batch loss: 0.503131628036499\n",
            "[Epoch 64] Batch loss: 0.6354398727416992\n",
            "[Epoch 64] Batch loss: 0.5402672290802002\n",
            "[Epoch 64] Batch loss: 0.49236923456192017\n",
            "[Epoch 64] Batch loss: 0.6338473558425903\n",
            "[Epoch 64] Batch loss: 0.5479622483253479\n",
            "[Epoch 64] Batch loss: 0.6144125461578369\n",
            "[Epoch 64] Batch loss: 0.6141364574432373\n",
            "[Epoch 64] Batch loss: 0.5027756690979004\n",
            "[Epoch 64] Batch loss: 0.5629046559333801\n",
            "[Epoch 64] Batch loss: 0.5301357507705688\n",
            "[Epoch 64] Batch loss: 0.6023684144020081\n",
            "[Epoch 64] Batch loss: 0.5233526229858398\n",
            "[Epoch 64] Batch loss: 0.6817724108695984\n",
            "[Epoch 64] Batch loss: 0.5307785272598267\n",
            "[Epoch 64] Batch loss: 0.6038799285888672\n",
            "[Epoch 64] Batch loss: 0.5289133787155151\n",
            "[Epoch 64] Batch loss: 0.5762076377868652\n",
            "[Epoch 64] Batch loss: 0.5952060222625732\n",
            "[Epoch 64] Batch loss: 0.8500993251800537\n",
            "[Epoch 64] Batch loss: 0.6417406797409058\n",
            "[Epoch 64] Batch loss: 0.4812362492084503\n",
            "[Epoch 64] Batch loss: 0.5750302672386169\n",
            "[Epoch 64] Batch loss: 0.4813162088394165\n",
            "[Epoch 64] Batch loss: 0.6283954977989197\n",
            "[Epoch 64] Batch loss: 0.5520298480987549\n",
            "[Epoch 64] Batch loss: 0.6316980719566345\n",
            "[Epoch 64] Batch loss: 0.6753467917442322\n",
            "[Epoch 64] Batch loss: 0.8160925507545471\n",
            "[Epoch 64] Batch loss: 0.5100789666175842\n",
            "[Epoch 64] Batch loss: 0.49759572744369507\n",
            "[Epoch 64] Batch loss: 0.6434052586555481\n",
            "[Epoch 64] Batch loss: 0.516028642654419\n",
            "[Epoch 64] Batch loss: 0.6288465857505798\n",
            "[Epoch 64] Batch loss: 0.7211955785751343\n",
            "[Epoch 64] Batch loss: 0.6230238676071167\n",
            "[Epoch 64] Batch loss: 0.613998293876648\n",
            "[Epoch 64] Batch loss: 0.6445172429084778\n",
            "[Epoch 64] Batch loss: 0.6489933133125305\n",
            "[Epoch 64] Batch loss: 0.5450201034545898\n",
            "[Epoch 64] Batch loss: 0.5573753118515015\n",
            "[Epoch 64] Batch loss: 0.5716276168823242\n",
            "[Epoch 64] Batch loss: 0.6375899910926819\n",
            "[Epoch 64] Batch loss: 0.579882025718689\n",
            "[Epoch 64] Batch loss: 0.5549185276031494\n",
            "[Epoch 64] Batch loss: 0.5784428715705872\n",
            "[Epoch 64] Batch loss: 0.5853157043457031\n",
            "[Epoch 64] Batch loss: 0.5090332627296448\n",
            "[Epoch 64] Batch loss: 0.7654030323028564\n",
            "[Epoch 64] Batch loss: 0.6055372357368469\n",
            "[Epoch 64] Batch loss: 0.5292139649391174\n",
            "[Epoch 64] Batch loss: 0.6507281064987183\n",
            "[Epoch 64] Batch loss: 0.6547529101371765\n",
            "[Epoch 64] Batch loss: 0.7560907006263733\n",
            "[Epoch 64] Batch loss: 0.628791093826294\n",
            "[Epoch 64] Batch loss: 0.6627563238143921\n",
            "[Epoch 64] Batch loss: 0.5446133613586426\n",
            "[Epoch 64] Batch loss: 0.5869505405426025\n",
            "[Epoch 64] Batch loss: 0.6066755056381226\n",
            "[Epoch 64] Batch loss: 0.515724778175354\n",
            "[Epoch 64] Batch loss: 0.6096498966217041\n",
            "[Epoch 64] Batch loss: 0.6222670674324036\n",
            "[Epoch 64] Batch loss: 0.5284705758094788\n",
            "[Epoch 64] Batch loss: 0.7586164474487305\n",
            "[Epoch 64] Batch loss: 0.4964632987976074\n",
            "[Epoch 64] Batch loss: 0.5061302781105042\n",
            "[Epoch 64] Batch loss: 0.6880183219909668\n",
            "[Epoch 64] Batch loss: 0.6345045566558838\n",
            "[Epoch 64] Batch loss: 0.6619276404380798\n",
            "[Epoch 64] Batch loss: 0.6224552989006042\n",
            "[Epoch 64] Batch loss: 0.5450770854949951\n",
            "[Epoch 64] Batch loss: 0.8640129566192627\n",
            "[Epoch 64] Batch loss: 0.6876724362373352\n",
            "[Epoch 65/100] Val loss: 1.0291\n",
            "Best saved model.\n",
            "[Epoch 65] Batch loss: 0.4765673875808716\n",
            "[Epoch 65] Batch loss: 0.7014764547348022\n",
            "[Epoch 65] Batch loss: 0.5913422107696533\n",
            "[Epoch 65] Batch loss: 0.629011332988739\n",
            "[Epoch 65] Batch loss: 0.5663164854049683\n",
            "[Epoch 65] Batch loss: 0.6225319504737854\n",
            "[Epoch 65] Batch loss: 0.586714506149292\n",
            "[Epoch 65] Batch loss: 0.5390695333480835\n",
            "[Epoch 65] Batch loss: 0.6688724756240845\n",
            "[Epoch 65] Batch loss: 0.5286319851875305\n",
            "[Epoch 65] Batch loss: 0.6480099558830261\n",
            "[Epoch 65] Batch loss: 0.561150312423706\n",
            "[Epoch 65] Batch loss: 0.5022554397583008\n",
            "[Epoch 65] Batch loss: 0.5770631432533264\n",
            "[Epoch 65] Batch loss: 0.550176203250885\n",
            "[Epoch 65] Batch loss: 0.7417486310005188\n",
            "[Epoch 65] Batch loss: 0.6483428478240967\n",
            "[Epoch 65] Batch loss: 0.5498892068862915\n",
            "[Epoch 65] Batch loss: 0.5667957663536072\n",
            "[Epoch 65] Batch loss: 0.619560718536377\n",
            "[Epoch 65] Batch loss: 0.5539912581443787\n",
            "[Epoch 65] Batch loss: 0.6051076650619507\n",
            "[Epoch 65] Batch loss: 0.5788417458534241\n",
            "[Epoch 65] Batch loss: 0.5752979516983032\n",
            "[Epoch 65] Batch loss: 0.6563430428504944\n",
            "[Epoch 65] Batch loss: 0.6090177297592163\n",
            "[Epoch 65] Batch loss: 0.5240525603294373\n",
            "[Epoch 65] Batch loss: 0.6968477368354797\n",
            "[Epoch 65] Batch loss: 0.6077581644058228\n",
            "[Epoch 65] Batch loss: 0.619320273399353\n",
            "[Epoch 65] Batch loss: 0.5210931897163391\n",
            "[Epoch 65] Batch loss: 0.48800966143608093\n",
            "[Epoch 65] Batch loss: 0.5093815922737122\n",
            "[Epoch 65] Batch loss: 0.5209819674491882\n",
            "[Epoch 65] Batch loss: 0.5869454145431519\n",
            "[Epoch 65] Batch loss: 0.4673197865486145\n",
            "[Epoch 65] Batch loss: 0.5329227447509766\n",
            "[Epoch 65] Batch loss: 0.5890873670578003\n",
            "[Epoch 65] Batch loss: 0.7373209595680237\n",
            "[Epoch 65] Batch loss: 0.5169010162353516\n",
            "[Epoch 65] Batch loss: 0.589622974395752\n",
            "[Epoch 65] Batch loss: 0.6478811502456665\n",
            "[Epoch 65] Batch loss: 0.5573105216026306\n",
            "[Epoch 65] Batch loss: 0.5346108675003052\n",
            "[Epoch 65] Batch loss: 0.6516512632369995\n",
            "[Epoch 65] Batch loss: 0.5370835065841675\n",
            "[Epoch 65] Batch loss: 0.4926906228065491\n",
            "[Epoch 65] Batch loss: 0.4861098527908325\n",
            "[Epoch 65] Batch loss: 0.6082696318626404\n",
            "[Epoch 65] Batch loss: 0.662844717502594\n",
            "[Epoch 65] Batch loss: 0.5546197891235352\n",
            "[Epoch 65] Batch loss: 0.7502892017364502\n",
            "[Epoch 65] Batch loss: 0.5704459547996521\n",
            "[Epoch 65] Batch loss: 0.5107935667037964\n",
            "[Epoch 65] Batch loss: 0.6555238962173462\n",
            "[Epoch 65] Batch loss: 0.5743550062179565\n",
            "[Epoch 65] Batch loss: 0.5302855968475342\n",
            "[Epoch 65] Batch loss: 0.5869832038879395\n",
            "[Epoch 65] Batch loss: 0.5940810441970825\n",
            "[Epoch 65] Batch loss: 0.4401644170284271\n",
            "[Epoch 65] Batch loss: 0.7441225051879883\n",
            "[Epoch 65] Batch loss: 0.6029503345489502\n",
            "[Epoch 65] Batch loss: 0.5993886590003967\n",
            "[Epoch 65] Batch loss: 0.5313020348548889\n",
            "[Epoch 65] Batch loss: 0.5594096183776855\n",
            "[Epoch 65] Batch loss: 0.6719450950622559\n",
            "[Epoch 65] Batch loss: 0.8169982433319092\n",
            "[Epoch 65] Batch loss: 0.7123187780380249\n",
            "[Epoch 65] Batch loss: 0.6713119745254517\n",
            "[Epoch 65] Batch loss: 0.5977848768234253\n",
            "[Epoch 65] Batch loss: 0.6900129914283752\n",
            "[Epoch 65] Batch loss: 0.4871450364589691\n",
            "[Epoch 65] Batch loss: 0.6586477160453796\n",
            "[Epoch 65] Batch loss: 0.6155697107315063\n",
            "[Epoch 65] Batch loss: 0.519463837146759\n",
            "[Epoch 65] Batch loss: 0.5888065695762634\n",
            "[Epoch 65] Batch loss: 0.7090853452682495\n",
            "[Epoch 65] Batch loss: 0.5082510709762573\n",
            "[Epoch 65] Batch loss: 0.5909462571144104\n",
            "[Epoch 65] Batch loss: 0.5916633009910583\n",
            "[Epoch 65] Batch loss: 0.5568410754203796\n",
            "[Epoch 65] Batch loss: 0.595015287399292\n",
            "[Epoch 65] Batch loss: 0.727481484413147\n",
            "[Epoch 65] Batch loss: 0.49345651268959045\n",
            "[Epoch 65] Batch loss: 0.6065789461135864\n",
            "[Epoch 65] Batch loss: 0.5832845568656921\n",
            "[Epoch 65] Batch loss: 0.6292125582695007\n",
            "[Epoch 65] Batch loss: 0.6825888752937317\n",
            "[Epoch 65] Batch loss: 0.519741415977478\n",
            "[Epoch 65] Batch loss: 0.49209117889404297\n",
            "[Epoch 65] Batch loss: 0.6282565593719482\n",
            "[Epoch 65] Batch loss: 0.5488031506538391\n",
            "[Epoch 65] Batch loss: 0.5945037603378296\n",
            "[Epoch 65] Batch loss: 0.6864031553268433\n",
            "[Epoch 65] Batch loss: 0.7309689521789551\n",
            "[Epoch 65] Batch loss: 0.9037351608276367\n",
            "[Epoch 65] Batch loss: 0.5438973903656006\n",
            "[Epoch 65] Batch loss: 0.5487084984779358\n",
            "[Epoch 65] Batch loss: 0.5617821216583252\n",
            "[Epoch 65] Batch loss: 0.560378909111023\n",
            "[Epoch 65] Batch loss: 0.6991733908653259\n",
            "[Epoch 65] Batch loss: 0.6489397883415222\n",
            "[Epoch 65] Batch loss: 0.5250786542892456\n",
            "[Epoch 65] Batch loss: 0.6260649561882019\n",
            "[Epoch 65] Batch loss: 0.6282069683074951\n",
            "[Epoch 65] Batch loss: 0.6664497256278992\n",
            "[Epoch 65] Batch loss: 0.5497027635574341\n",
            "[Epoch 65] Batch loss: 0.5410140156745911\n",
            "[Epoch 65] Batch loss: 0.6796210408210754\n",
            "[Epoch 65] Batch loss: 0.7168825268745422\n",
            "[Epoch 65] Batch loss: 0.5330586433410645\n",
            "[Epoch 65] Batch loss: 0.7132612466812134\n",
            "[Epoch 65] Batch loss: 0.6094551682472229\n",
            "[Epoch 65] Batch loss: 0.4654320180416107\n",
            "[Epoch 65] Batch loss: 0.5993865132331848\n",
            "[Epoch 65] Batch loss: 0.52730393409729\n",
            "[Epoch 65] Batch loss: 0.5996401906013489\n",
            "[Epoch 65] Batch loss: 0.7634450197219849\n",
            "[Epoch 65] Batch loss: 0.7588619589805603\n",
            "[Epoch 65] Batch loss: 0.48414313793182373\n",
            "[Epoch 65] Batch loss: 0.48300695419311523\n",
            "[Epoch 65] Batch loss: 0.5652881264686584\n",
            "[Epoch 65] Batch loss: 0.5002880096435547\n",
            "[Epoch 65] Batch loss: 0.6094547510147095\n",
            "[Epoch 65] Batch loss: 0.5594449043273926\n",
            "[Epoch 65] Batch loss: 0.5922197103500366\n",
            "[Epoch 66/100] Val loss: 1.0050\n",
            "Best saved model.\n",
            "[Epoch 66] Batch loss: 0.7896143198013306\n",
            "[Epoch 66] Batch loss: 0.7303452491760254\n",
            "[Epoch 66] Batch loss: 0.5423436760902405\n",
            "[Epoch 66] Batch loss: 0.5739722847938538\n",
            "[Epoch 66] Batch loss: 0.6341816186904907\n",
            "[Epoch 66] Batch loss: 0.5707501173019409\n",
            "[Epoch 66] Batch loss: 0.6287992596626282\n",
            "[Epoch 66] Batch loss: 0.6469627618789673\n",
            "[Epoch 66] Batch loss: 0.568405270576477\n",
            "[Epoch 66] Batch loss: 0.5960657596588135\n",
            "[Epoch 66] Batch loss: 0.5025824904441833\n",
            "[Epoch 66] Batch loss: 0.5460903644561768\n",
            "[Epoch 66] Batch loss: 0.49293258786201477\n",
            "[Epoch 66] Batch loss: 0.6183434724807739\n",
            "[Epoch 66] Batch loss: 0.6646956205368042\n",
            "[Epoch 66] Batch loss: 0.5378620624542236\n",
            "[Epoch 66] Batch loss: 0.6239994168281555\n",
            "[Epoch 66] Batch loss: 0.6579267382621765\n",
            "[Epoch 66] Batch loss: 0.5068809986114502\n",
            "[Epoch 66] Batch loss: 0.609508216381073\n",
            "[Epoch 66] Batch loss: 0.49569305777549744\n",
            "[Epoch 66] Batch loss: 0.597313404083252\n",
            "[Epoch 66] Batch loss: 0.5426602363586426\n",
            "[Epoch 66] Batch loss: 0.5725449323654175\n",
            "[Epoch 66] Batch loss: 0.6598982810974121\n",
            "[Epoch 66] Batch loss: 0.492872029542923\n",
            "[Epoch 66] Batch loss: 0.5929673314094543\n",
            "[Epoch 66] Batch loss: 0.47118350863456726\n",
            "[Epoch 66] Batch loss: 0.5630956888198853\n",
            "[Epoch 66] Batch loss: 0.5247059464454651\n",
            "[Epoch 66] Batch loss: 0.5587003231048584\n",
            "[Epoch 66] Batch loss: 0.5256686210632324\n",
            "[Epoch 66] Batch loss: 0.5574135780334473\n",
            "[Epoch 66] Batch loss: 0.6725323796272278\n",
            "[Epoch 66] Batch loss: 0.5486099720001221\n",
            "[Epoch 66] Batch loss: 0.47848162055015564\n",
            "[Epoch 66] Batch loss: 0.589153528213501\n",
            "[Epoch 66] Batch loss: 0.6121857166290283\n",
            "[Epoch 66] Batch loss: 0.5767115354537964\n",
            "[Epoch 66] Batch loss: 0.6661383509635925\n",
            "[Epoch 66] Batch loss: 0.5501788258552551\n",
            "[Epoch 66] Batch loss: 0.5051563382148743\n",
            "[Epoch 66] Batch loss: 0.5544137358665466\n",
            "[Epoch 66] Batch loss: 0.6777362823486328\n",
            "[Epoch 66] Batch loss: 0.5176565647125244\n",
            "[Epoch 66] Batch loss: 0.5256987810134888\n",
            "[Epoch 66] Batch loss: 0.5429919958114624\n",
            "[Epoch 66] Batch loss: 0.6506607532501221\n",
            "[Epoch 66] Batch loss: 0.6005111932754517\n",
            "[Epoch 66] Batch loss: 0.5823489427566528\n",
            "[Epoch 66] Batch loss: 0.5044347047805786\n",
            "[Epoch 66] Batch loss: 0.5280951857566833\n",
            "[Epoch 66] Batch loss: 0.5578891038894653\n",
            "[Epoch 66] Batch loss: 0.591761589050293\n",
            "[Epoch 66] Batch loss: 0.5185891389846802\n",
            "[Epoch 66] Batch loss: 0.6339674592018127\n",
            "[Epoch 66] Batch loss: 0.6450951099395752\n",
            "[Epoch 66] Batch loss: 0.6966172456741333\n",
            "[Epoch 66] Batch loss: 0.5104234218597412\n",
            "[Epoch 66] Batch loss: 0.5417091250419617\n",
            "[Epoch 66] Batch loss: 0.5074743032455444\n",
            "[Epoch 66] Batch loss: 0.6413708925247192\n",
            "[Epoch 66] Batch loss: 0.5754706263542175\n",
            "[Epoch 66] Batch loss: 0.6492082476615906\n",
            "[Epoch 66] Batch loss: 0.7647781372070312\n",
            "[Epoch 66] Batch loss: 0.5680822134017944\n",
            "[Epoch 66] Batch loss: 0.595360517501831\n",
            "[Epoch 66] Batch loss: 0.5021635293960571\n",
            "[Epoch 66] Batch loss: 0.5092841982841492\n",
            "[Epoch 66] Batch loss: 0.6336806416511536\n",
            "[Epoch 66] Batch loss: 0.6085118651390076\n",
            "[Epoch 66] Batch loss: 0.473806232213974\n",
            "[Epoch 66] Batch loss: 0.6576038599014282\n",
            "[Epoch 66] Batch loss: 0.6521183252334595\n",
            "[Epoch 66] Batch loss: 0.6882244944572449\n",
            "[Epoch 66] Batch loss: 0.5723874568939209\n",
            "[Epoch 66] Batch loss: 0.5281248092651367\n",
            "[Epoch 66] Batch loss: 0.4921053647994995\n",
            "[Epoch 66] Batch loss: 0.784206748008728\n",
            "[Epoch 66] Batch loss: 0.6924803256988525\n",
            "[Epoch 66] Batch loss: 0.5883572697639465\n",
            "[Epoch 66] Batch loss: 0.6613391637802124\n",
            "[Epoch 66] Batch loss: 0.564841628074646\n",
            "[Epoch 66] Batch loss: 0.5173404216766357\n",
            "[Epoch 66] Batch loss: 0.6200029850006104\n",
            "[Epoch 66] Batch loss: 0.6152361035346985\n",
            "[Epoch 66] Batch loss: 0.6360910534858704\n",
            "[Epoch 66] Batch loss: 0.5171713829040527\n",
            "[Epoch 66] Batch loss: 0.5379944443702698\n",
            "[Epoch 66] Batch loss: 0.41632118821144104\n",
            "[Epoch 66] Batch loss: 0.536876380443573\n",
            "[Epoch 66] Batch loss: 0.5129894018173218\n",
            "[Epoch 66] Batch loss: 0.4887465536594391\n",
            "[Epoch 66] Batch loss: 0.5492380261421204\n",
            "[Epoch 66] Batch loss: 0.5648723244667053\n",
            "[Epoch 66] Batch loss: 0.6832346320152283\n",
            "[Epoch 66] Batch loss: 0.8809680938720703\n",
            "[Epoch 66] Batch loss: 0.4984996020793915\n",
            "[Epoch 66] Batch loss: 0.4811781048774719\n",
            "[Epoch 66] Batch loss: 0.5999355316162109\n",
            "[Epoch 66] Batch loss: 0.7143244743347168\n",
            "[Epoch 66] Batch loss: 0.8944950103759766\n",
            "[Epoch 66] Batch loss: 0.7124947905540466\n",
            "[Epoch 66] Batch loss: 0.7314795851707458\n",
            "[Epoch 66] Batch loss: 0.5462428331375122\n",
            "[Epoch 66] Batch loss: 0.5566940903663635\n",
            "[Epoch 66] Batch loss: 0.5112416744232178\n",
            "[Epoch 66] Batch loss: 0.5886720418930054\n",
            "[Epoch 66] Batch loss: 0.7530854344367981\n",
            "[Epoch 66] Batch loss: 0.6655226945877075\n",
            "[Epoch 66] Batch loss: 0.6340191960334778\n",
            "[Epoch 66] Batch loss: 0.6479390263557434\n",
            "[Epoch 66] Batch loss: 0.689153790473938\n",
            "[Epoch 66] Batch loss: 0.5381389856338501\n",
            "[Epoch 66] Batch loss: 0.5537751317024231\n",
            "[Epoch 66] Batch loss: 0.611929714679718\n",
            "[Epoch 66] Batch loss: 0.5542580485343933\n",
            "[Epoch 66] Batch loss: 0.5515897870063782\n",
            "[Epoch 66] Batch loss: 0.6727975010871887\n",
            "[Epoch 66] Batch loss: 0.5945355296134949\n",
            "[Epoch 66] Batch loss: 0.5511716604232788\n",
            "[Epoch 66] Batch loss: 0.6217724084854126\n",
            "[Epoch 66] Batch loss: 0.5274425148963928\n",
            "[Epoch 66] Batch loss: 0.5950736999511719\n",
            "[Epoch 66] Batch loss: 0.5411620140075684\n",
            "[Epoch 66] Batch loss: 0.4792722761631012\n",
            "[Epoch 67/100] Val loss: 0.9830\n",
            "Best saved model.\n",
            "[Epoch 67] Batch loss: 0.4933023154735565\n",
            "[Epoch 67] Batch loss: 0.7698471546173096\n",
            "[Epoch 67] Batch loss: 0.5947306752204895\n",
            "[Epoch 67] Batch loss: 0.5238057374954224\n",
            "[Epoch 67] Batch loss: 0.5278716683387756\n",
            "[Epoch 67] Batch loss: 0.6031321287155151\n",
            "[Epoch 67] Batch loss: 0.4882335066795349\n",
            "[Epoch 67] Batch loss: 0.49981945753097534\n",
            "[Epoch 67] Batch loss: 0.5835967659950256\n",
            "[Epoch 67] Batch loss: 0.5486499071121216\n",
            "[Epoch 67] Batch loss: 0.6015229821205139\n",
            "[Epoch 67] Batch loss: 0.49482715129852295\n",
            "[Epoch 67] Batch loss: 0.600858211517334\n",
            "[Epoch 67] Batch loss: 0.5441994071006775\n",
            "[Epoch 67] Batch loss: 0.5965601205825806\n",
            "[Epoch 67] Batch loss: 0.7228034138679504\n",
            "[Epoch 67] Batch loss: 0.6011428833007812\n",
            "[Epoch 67] Batch loss: 0.5473600625991821\n",
            "[Epoch 67] Batch loss: 0.6893973350524902\n",
            "[Epoch 67] Batch loss: 0.524939775466919\n",
            "[Epoch 67] Batch loss: 0.6408666968345642\n",
            "[Epoch 67] Batch loss: 0.6419954299926758\n",
            "[Epoch 67] Batch loss: 0.7625656127929688\n",
            "[Epoch 67] Batch loss: 0.5097053050994873\n",
            "[Epoch 67] Batch loss: 0.5991224646568298\n",
            "[Epoch 67] Batch loss: 0.4711664319038391\n",
            "[Epoch 67] Batch loss: 0.626335859298706\n",
            "[Epoch 67] Batch loss: 0.5982426404953003\n",
            "[Epoch 67] Batch loss: 0.6131104230880737\n",
            "[Epoch 67] Batch loss: 0.4713885188102722\n",
            "[Epoch 67] Batch loss: 0.5689688324928284\n",
            "[Epoch 67] Batch loss: 0.68377286195755\n",
            "[Epoch 67] Batch loss: 0.6206637620925903\n",
            "[Epoch 67] Batch loss: 0.6120980978012085\n",
            "[Epoch 67] Batch loss: 0.5829064249992371\n",
            "[Epoch 67] Batch loss: 0.5247521996498108\n",
            "[Epoch 67] Batch loss: 0.5723690986633301\n",
            "[Epoch 67] Batch loss: 0.6255268454551697\n",
            "[Epoch 67] Batch loss: 0.5601133108139038\n",
            "[Epoch 67] Batch loss: 0.4708542227745056\n",
            "[Epoch 67] Batch loss: 0.5249902009963989\n",
            "[Epoch 67] Batch loss: 0.5835833549499512\n",
            "[Epoch 67] Batch loss: 0.5629563927650452\n",
            "[Epoch 67] Batch loss: 0.5062232613563538\n",
            "[Epoch 67] Batch loss: 0.6171493530273438\n",
            "[Epoch 67] Batch loss: 0.6727370619773865\n",
            "[Epoch 67] Batch loss: 0.5688247084617615\n",
            "[Epoch 67] Batch loss: 0.5835430026054382\n",
            "[Epoch 67] Batch loss: 0.5526590347290039\n",
            "[Epoch 67] Batch loss: 0.6654675006866455\n",
            "[Epoch 67] Batch loss: 0.5361531972885132\n",
            "[Epoch 67] Batch loss: 0.6653660535812378\n",
            "[Epoch 67] Batch loss: 0.5079243779182434\n",
            "[Epoch 67] Batch loss: 0.7444838881492615\n",
            "[Epoch 67] Batch loss: 0.5771376490592957\n",
            "[Epoch 67] Batch loss: 0.6440328359603882\n",
            "[Epoch 67] Batch loss: 0.5552424788475037\n",
            "[Epoch 67] Batch loss: 0.5878136157989502\n",
            "[Epoch 67] Batch loss: 0.7055410146713257\n",
            "[Epoch 67] Batch loss: 0.5864785313606262\n",
            "[Epoch 67] Batch loss: 0.5616322159767151\n",
            "[Epoch 67] Batch loss: 0.6028850078582764\n",
            "[Epoch 67] Batch loss: 0.49546805024147034\n",
            "[Epoch 67] Batch loss: 0.5588061213493347\n",
            "[Epoch 67] Batch loss: 0.6226887702941895\n",
            "[Epoch 67] Batch loss: 0.4548415243625641\n",
            "[Epoch 67] Batch loss: 0.5018478631973267\n",
            "[Epoch 67] Batch loss: 0.5288056135177612\n",
            "[Epoch 67] Batch loss: 0.5067930221557617\n",
            "[Epoch 67] Batch loss: 0.538459062576294\n",
            "[Epoch 67] Batch loss: 0.5990610122680664\n",
            "[Epoch 67] Batch loss: 0.5299238562583923\n",
            "[Epoch 67] Batch loss: 0.5355269908905029\n",
            "[Epoch 67] Batch loss: 0.5818353891372681\n",
            "[Epoch 67] Batch loss: 0.5127627849578857\n",
            "[Epoch 67] Batch loss: 0.6423562169075012\n",
            "[Epoch 67] Batch loss: 0.46575742959976196\n",
            "[Epoch 67] Batch loss: 0.5042451024055481\n",
            "[Epoch 67] Batch loss: 0.5418291091918945\n",
            "[Epoch 67] Batch loss: 0.6825964450836182\n",
            "[Epoch 67] Batch loss: 0.5408479571342468\n",
            "[Epoch 67] Batch loss: 0.6369951367378235\n",
            "[Epoch 67] Batch loss: 0.790125846862793\n",
            "[Epoch 67] Batch loss: 0.5748440027236938\n",
            "[Epoch 67] Batch loss: 0.5875180959701538\n",
            "[Epoch 67] Batch loss: 0.5257636308670044\n",
            "[Epoch 67] Batch loss: 0.5211077928543091\n",
            "[Epoch 67] Batch loss: 0.6884027123451233\n",
            "[Epoch 67] Batch loss: 0.7205797433853149\n",
            "[Epoch 67] Batch loss: 0.4929051399230957\n",
            "[Epoch 67] Batch loss: 0.6549568176269531\n",
            "[Epoch 67] Batch loss: 0.610961377620697\n",
            "[Epoch 67] Batch loss: 0.6167573928833008\n",
            "[Epoch 67] Batch loss: 0.5633775591850281\n",
            "[Epoch 67] Batch loss: 0.5551183223724365\n",
            "[Epoch 67] Batch loss: 0.5150852799415588\n",
            "[Epoch 67] Batch loss: 0.5223909616470337\n",
            "[Epoch 67] Batch loss: 0.6577067375183105\n",
            "[Epoch 67] Batch loss: 0.48731234669685364\n",
            "[Epoch 67] Batch loss: 0.4859486520290375\n",
            "[Epoch 67] Batch loss: 0.5104702711105347\n",
            "[Epoch 67] Batch loss: 0.5157663226127625\n",
            "[Epoch 67] Batch loss: 0.8028237819671631\n",
            "[Epoch 67] Batch loss: 0.5161647796630859\n",
            "[Epoch 67] Batch loss: 0.591063916683197\n",
            "[Epoch 67] Batch loss: 0.632074236869812\n",
            "[Epoch 67] Batch loss: 0.5449233055114746\n",
            "[Epoch 67] Batch loss: 0.541889488697052\n",
            "[Epoch 67] Batch loss: 0.47036218643188477\n",
            "[Epoch 67] Batch loss: 0.5751387476921082\n",
            "[Epoch 67] Batch loss: 0.5894972085952759\n",
            "[Epoch 67] Batch loss: 0.5112696290016174\n",
            "[Epoch 67] Batch loss: 0.5107450485229492\n",
            "[Epoch 67] Batch loss: 0.601130485534668\n",
            "[Epoch 67] Batch loss: 0.6947108507156372\n",
            "[Epoch 67] Batch loss: 0.5430795550346375\n",
            "[Epoch 67] Batch loss: 0.5265902280807495\n",
            "[Epoch 67] Batch loss: 0.540586531162262\n",
            "[Epoch 67] Batch loss: 0.5549846291542053\n",
            "[Epoch 67] Batch loss: 0.49421799182891846\n",
            "[Epoch 67] Batch loss: 0.6000857353210449\n",
            "[Epoch 67] Batch loss: 0.5149631500244141\n",
            "[Epoch 67] Batch loss: 0.46070098876953125\n",
            "[Epoch 67] Batch loss: 0.49270927906036377\n",
            "[Epoch 67] Batch loss: 0.6043811440467834\n",
            "[Epoch 67] Batch loss: 0.9086753129959106\n",
            "[Epoch 68/100] Val loss: 0.9670\n",
            "Best saved model.\n",
            "[Epoch 68] Batch loss: 0.5260820388793945\n",
            "[Epoch 68] Batch loss: 0.6707251071929932\n",
            "[Epoch 68] Batch loss: 0.5353724360466003\n",
            "[Epoch 68] Batch loss: 0.5593390464782715\n",
            "[Epoch 68] Batch loss: 0.4544011056423187\n",
            "[Epoch 68] Batch loss: 0.5428610444068909\n",
            "[Epoch 68] Batch loss: 0.5648835897445679\n",
            "[Epoch 68] Batch loss: 0.49038249254226685\n",
            "[Epoch 68] Batch loss: 0.4938160181045532\n",
            "[Epoch 68] Batch loss: 0.518682062625885\n",
            "[Epoch 68] Batch loss: 0.7206063270568848\n",
            "[Epoch 68] Batch loss: 0.5435455441474915\n",
            "[Epoch 68] Batch loss: 0.5925508737564087\n",
            "[Epoch 68] Batch loss: 0.5839924216270447\n",
            "[Epoch 68] Batch loss: 0.47677916288375854\n",
            "[Epoch 68] Batch loss: 0.7338300943374634\n",
            "[Epoch 68] Batch loss: 0.6477077007293701\n",
            "[Epoch 68] Batch loss: 0.4707752466201782\n",
            "[Epoch 68] Batch loss: 0.5895410776138306\n",
            "[Epoch 68] Batch loss: 0.5646190643310547\n",
            "[Epoch 68] Batch loss: 0.4978460371494293\n",
            "[Epoch 68] Batch loss: 0.5448713898658752\n",
            "[Epoch 68] Batch loss: 0.6284421682357788\n",
            "[Epoch 68] Batch loss: 0.6813181638717651\n",
            "[Epoch 68] Batch loss: 0.4558005928993225\n",
            "[Epoch 68] Batch loss: 0.5094344615936279\n",
            "[Epoch 68] Batch loss: 0.49099528789520264\n",
            "[Epoch 68] Batch loss: 0.5950671434402466\n",
            "[Epoch 68] Batch loss: 0.7013170719146729\n",
            "[Epoch 68] Batch loss: 0.4890742897987366\n",
            "[Epoch 68] Batch loss: 0.44985508918762207\n",
            "[Epoch 68] Batch loss: 0.6876172423362732\n",
            "[Epoch 68] Batch loss: 0.8456822633743286\n",
            "[Epoch 68] Batch loss: 0.5146389007568359\n",
            "[Epoch 68] Batch loss: 0.5644978880882263\n",
            "[Epoch 68] Batch loss: 0.5522608757019043\n",
            "[Epoch 68] Batch loss: 0.6817998886108398\n",
            "[Epoch 68] Batch loss: 0.4993566572666168\n",
            "[Epoch 68] Batch loss: 0.5786199569702148\n",
            "[Epoch 68] Batch loss: 0.5547013282775879\n",
            "[Epoch 68] Batch loss: 0.820259153842926\n",
            "[Epoch 68] Batch loss: 0.5590766668319702\n",
            "[Epoch 68] Batch loss: 0.45783132314682007\n",
            "[Epoch 68] Batch loss: 0.5508460998535156\n",
            "[Epoch 68] Batch loss: 0.6132309436798096\n",
            "[Epoch 68] Batch loss: 0.477379709482193\n",
            "[Epoch 68] Batch loss: 0.5687249898910522\n",
            "[Epoch 68] Batch loss: 0.5144525766372681\n",
            "[Epoch 68] Batch loss: 0.5438195466995239\n",
            "[Epoch 68] Batch loss: 0.545579195022583\n",
            "[Epoch 68] Batch loss: 0.5947461724281311\n",
            "[Epoch 68] Batch loss: 0.5901757478713989\n",
            "[Epoch 68] Batch loss: 0.4403429627418518\n",
            "[Epoch 68] Batch loss: 0.4741913080215454\n",
            "[Epoch 68] Batch loss: 0.5525729656219482\n",
            "[Epoch 68] Batch loss: 0.50517338514328\n",
            "[Epoch 68] Batch loss: 0.6120102405548096\n",
            "[Epoch 68] Batch loss: 0.5129259824752808\n",
            "[Epoch 68] Batch loss: 0.57741779088974\n",
            "[Epoch 68] Batch loss: 0.5342362523078918\n",
            "[Epoch 68] Batch loss: 0.5083425641059875\n",
            "[Epoch 68] Batch loss: 0.6013205647468567\n",
            "[Epoch 68] Batch loss: 0.5977578163146973\n",
            "[Epoch 68] Batch loss: 0.5402045249938965\n",
            "[Epoch 68] Batch loss: 0.5317398309707642\n",
            "[Epoch 68] Batch loss: 0.5624595880508423\n",
            "[Epoch 68] Batch loss: 0.5208894610404968\n",
            "[Epoch 68] Batch loss: 0.4951289892196655\n",
            "[Epoch 68] Batch loss: 0.5392863750457764\n",
            "[Epoch 68] Batch loss: 0.47473299503326416\n",
            "[Epoch 68] Batch loss: 0.5416332483291626\n",
            "[Epoch 68] Batch loss: 0.48052671551704407\n",
            "[Epoch 68] Batch loss: 0.520651638507843\n",
            "[Epoch 68] Batch loss: 0.5315727591514587\n",
            "[Epoch 68] Batch loss: 0.5702738165855408\n",
            "[Epoch 68] Batch loss: 0.5595157742500305\n",
            "[Epoch 68] Batch loss: 0.6636136174201965\n",
            "[Epoch 68] Batch loss: 0.4994252920150757\n",
            "[Epoch 68] Batch loss: 0.5598907470703125\n",
            "[Epoch 68] Batch loss: 0.6863787770271301\n",
            "[Epoch 68] Batch loss: 0.5424022674560547\n",
            "[Epoch 68] Batch loss: 0.57807856798172\n",
            "[Epoch 68] Batch loss: 0.6550701856613159\n",
            "[Epoch 68] Batch loss: 0.512424111366272\n",
            "[Epoch 68] Batch loss: 0.5352739691734314\n",
            "[Epoch 68] Batch loss: 0.5744869112968445\n",
            "[Epoch 68] Batch loss: 0.591762363910675\n",
            "[Epoch 68] Batch loss: 0.7600234746932983\n",
            "[Epoch 68] Batch loss: 0.6308020353317261\n",
            "[Epoch 68] Batch loss: 0.6130188703536987\n",
            "[Epoch 68] Batch loss: 0.577041745185852\n",
            "[Epoch 68] Batch loss: 0.6974247097969055\n",
            "[Epoch 68] Batch loss: 0.500022292137146\n",
            "[Epoch 68] Batch loss: 0.4819287359714508\n",
            "[Epoch 68] Batch loss: 0.6647960543632507\n",
            "[Epoch 68] Batch loss: 0.5323579907417297\n",
            "[Epoch 68] Batch loss: 0.5762192606925964\n",
            "[Epoch 68] Batch loss: 0.5571266412734985\n",
            "[Epoch 68] Batch loss: 0.5763786435127258\n",
            "[Epoch 68] Batch loss: 0.580467939376831\n",
            "[Epoch 68] Batch loss: 0.6294083595275879\n",
            "[Epoch 68] Batch loss: 0.5052253007888794\n",
            "[Epoch 68] Batch loss: 0.6463010311126709\n",
            "[Epoch 68] Batch loss: 0.5866166353225708\n",
            "[Epoch 68] Batch loss: 0.6273202300071716\n",
            "[Epoch 68] Batch loss: 0.5377672910690308\n",
            "[Epoch 68] Batch loss: 0.6007053256034851\n",
            "[Epoch 68] Batch loss: 0.5293395519256592\n",
            "[Epoch 68] Batch loss: 0.473503053188324\n",
            "[Epoch 68] Batch loss: 0.5343363285064697\n",
            "[Epoch 68] Batch loss: 0.43659380078315735\n",
            "[Epoch 68] Batch loss: 0.6303418278694153\n",
            "[Epoch 68] Batch loss: 0.5773451924324036\n",
            "[Epoch 68] Batch loss: 0.5253403186798096\n",
            "[Epoch 68] Batch loss: 0.6021523475646973\n",
            "[Epoch 68] Batch loss: 0.5546957850456238\n",
            "[Epoch 68] Batch loss: 0.5934743881225586\n",
            "[Epoch 68] Batch loss: 0.5348023176193237\n",
            "[Epoch 68] Batch loss: 0.49944132566452026\n",
            "[Epoch 68] Batch loss: 0.6040065288543701\n",
            "[Epoch 68] Batch loss: 0.5216154456138611\n",
            "[Epoch 68] Batch loss: 0.601593554019928\n",
            "[Epoch 68] Batch loss: 0.6192305088043213\n",
            "[Epoch 68] Batch loss: 0.5000405311584473\n",
            "[Epoch 68] Batch loss: 0.7856467366218567\n",
            "[Epoch 68] Batch loss: 0.636120617389679\n",
            "[Epoch 69/100] Val loss: 0.9501\n",
            "Best saved model.\n",
            "[Epoch 69] Batch loss: 0.5850389003753662\n",
            "[Epoch 69] Batch loss: 0.5238335728645325\n",
            "[Epoch 69] Batch loss: 0.5499091744422913\n",
            "[Epoch 69] Batch loss: 0.5373873114585876\n",
            "[Epoch 69] Batch loss: 0.5523867607116699\n",
            "[Epoch 69] Batch loss: 0.5164504051208496\n",
            "[Epoch 69] Batch loss: 0.7067539095878601\n",
            "[Epoch 69] Batch loss: 0.5452021956443787\n",
            "[Epoch 69] Batch loss: 0.5420876741409302\n",
            "[Epoch 69] Batch loss: 0.452944815158844\n",
            "[Epoch 69] Batch loss: 0.5590671896934509\n",
            "[Epoch 69] Batch loss: 0.617192804813385\n",
            "[Epoch 69] Batch loss: 0.4922153651714325\n",
            "[Epoch 69] Batch loss: 0.6226181983947754\n",
            "[Epoch 69] Batch loss: 0.4679526686668396\n",
            "[Epoch 69] Batch loss: 0.5795924663543701\n",
            "[Epoch 69] Batch loss: 0.4824730157852173\n",
            "[Epoch 69] Batch loss: 0.6001591682434082\n",
            "[Epoch 69] Batch loss: 0.6597360372543335\n",
            "[Epoch 69] Batch loss: 0.6109695434570312\n",
            "[Epoch 69] Batch loss: 0.5045586228370667\n",
            "[Epoch 69] Batch loss: 0.5810422897338867\n",
            "[Epoch 69] Batch loss: 0.7271993160247803\n",
            "[Epoch 69] Batch loss: 0.46439865231513977\n",
            "[Epoch 69] Batch loss: 0.545989453792572\n",
            "[Epoch 69] Batch loss: 0.5055970549583435\n",
            "[Epoch 69] Batch loss: 0.5451139807701111\n",
            "[Epoch 69] Batch loss: 0.4691479206085205\n",
            "[Epoch 69] Batch loss: 0.564914345741272\n",
            "[Epoch 69] Batch loss: 0.5536905527114868\n",
            "[Epoch 69] Batch loss: 0.635819673538208\n",
            "[Epoch 69] Batch loss: 0.5547808408737183\n",
            "[Epoch 69] Batch loss: 0.6659207940101624\n",
            "[Epoch 69] Batch loss: 0.5473552942276001\n",
            "[Epoch 69] Batch loss: 0.5654966235160828\n",
            "[Epoch 69] Batch loss: 0.5726367235183716\n",
            "[Epoch 69] Batch loss: 0.4390431046485901\n",
            "[Epoch 69] Batch loss: 0.5502118468284607\n",
            "[Epoch 69] Batch loss: 0.4707932472229004\n",
            "[Epoch 69] Batch loss: 0.6889521479606628\n",
            "[Epoch 69] Batch loss: 0.6023516654968262\n",
            "[Epoch 69] Batch loss: 0.5235949754714966\n",
            "[Epoch 69] Batch loss: 0.5449455976486206\n",
            "[Epoch 69] Batch loss: 0.5011564493179321\n",
            "[Epoch 69] Batch loss: 0.559063196182251\n",
            "[Epoch 69] Batch loss: 0.5464637279510498\n",
            "[Epoch 69] Batch loss: 0.45202741026878357\n",
            "[Epoch 69] Batch loss: 0.6595278978347778\n",
            "[Epoch 69] Batch loss: 0.48784947395324707\n",
            "[Epoch 69] Batch loss: 0.467938095331192\n",
            "[Epoch 69] Batch loss: 0.503360390663147\n",
            "[Epoch 69] Batch loss: 0.5068378448486328\n",
            "[Epoch 69] Batch loss: 0.7408620119094849\n",
            "[Epoch 69] Batch loss: 0.6376968026161194\n",
            "[Epoch 69] Batch loss: 0.46557921171188354\n",
            "[Epoch 69] Batch loss: 0.6322533488273621\n",
            "[Epoch 69] Batch loss: 0.4591538906097412\n",
            "[Epoch 69] Batch loss: 0.5540058612823486\n",
            "[Epoch 69] Batch loss: 0.654389500617981\n",
            "[Epoch 69] Batch loss: 0.5897775888442993\n",
            "[Epoch 69] Batch loss: 0.5697236657142639\n",
            "[Epoch 69] Batch loss: 0.5594044923782349\n",
            "[Epoch 69] Batch loss: 0.455658882856369\n",
            "[Epoch 69] Batch loss: 0.5220756530761719\n",
            "[Epoch 69] Batch loss: 0.47779810428619385\n",
            "[Epoch 69] Batch loss: 0.6317418813705444\n",
            "[Epoch 69] Batch loss: 0.5767855644226074\n",
            "[Epoch 69] Batch loss: 0.5372203588485718\n",
            "[Epoch 69] Batch loss: 0.557443380355835\n",
            "[Epoch 69] Batch loss: 0.6219531297683716\n",
            "[Epoch 69] Batch loss: 0.4698431193828583\n",
            "[Epoch 69] Batch loss: 0.5399943590164185\n",
            "[Epoch 69] Batch loss: 0.5854419469833374\n",
            "[Epoch 69] Batch loss: 0.5376164317131042\n",
            "[Epoch 69] Batch loss: 0.5162771344184875\n",
            "[Epoch 69] Batch loss: 0.6145530343055725\n",
            "[Epoch 69] Batch loss: 0.5591843128204346\n",
            "[Epoch 69] Batch loss: 0.5943768620491028\n",
            "[Epoch 69] Batch loss: 0.6971383094787598\n",
            "[Epoch 69] Batch loss: 0.6030049920082092\n",
            "[Epoch 69] Batch loss: 0.6575153470039368\n",
            "[Epoch 69] Batch loss: 0.5561860203742981\n",
            "[Epoch 69] Batch loss: 0.6765580177307129\n",
            "[Epoch 69] Batch loss: 0.5555672645568848\n",
            "[Epoch 69] Batch loss: 0.5768789052963257\n",
            "[Epoch 69] Batch loss: 0.6233392953872681\n",
            "[Epoch 69] Batch loss: 0.5145490169525146\n",
            "[Epoch 69] Batch loss: 0.605549156665802\n",
            "[Epoch 69] Batch loss: 0.5580044388771057\n",
            "[Epoch 69] Batch loss: 0.4904400408267975\n",
            "[Epoch 69] Batch loss: 0.5171564817428589\n",
            "[Epoch 69] Batch loss: 0.5315864086151123\n",
            "[Epoch 69] Batch loss: 0.7491294145584106\n",
            "[Epoch 69] Batch loss: 0.4414639472961426\n",
            "[Epoch 69] Batch loss: 0.5382780432701111\n",
            "[Epoch 69] Batch loss: 0.4912967085838318\n",
            "[Epoch 69] Batch loss: 0.5284823179244995\n",
            "[Epoch 69] Batch loss: 0.6236563324928284\n",
            "[Epoch 69] Batch loss: 0.5330771803855896\n",
            "[Epoch 69] Batch loss: 0.613709032535553\n",
            "[Epoch 69] Batch loss: 0.6581659317016602\n",
            "[Epoch 69] Batch loss: 0.6113938689231873\n",
            "[Epoch 69] Batch loss: 0.543621301651001\n",
            "[Epoch 69] Batch loss: 0.6315443515777588\n",
            "[Epoch 69] Batch loss: 0.5222592353820801\n",
            "[Epoch 69] Batch loss: 0.7082940340042114\n",
            "[Epoch 69] Batch loss: 0.47052502632141113\n",
            "[Epoch 69] Batch loss: 0.45534658432006836\n",
            "[Epoch 69] Batch loss: 0.5196084976196289\n",
            "[Epoch 69] Batch loss: 0.6174479126930237\n",
            "[Epoch 69] Batch loss: 0.5084035992622375\n",
            "[Epoch 69] Batch loss: 0.5498031377792358\n",
            "[Epoch 69] Batch loss: 0.47637104988098145\n",
            "[Epoch 69] Batch loss: 0.48827555775642395\n",
            "[Epoch 69] Batch loss: 0.45365703105926514\n",
            "[Epoch 69] Batch loss: 0.5488248467445374\n",
            "[Epoch 69] Batch loss: 0.5332621335983276\n",
            "[Epoch 69] Batch loss: 0.5539907813072205\n",
            "[Epoch 69] Batch loss: 0.6344790458679199\n",
            "[Epoch 69] Batch loss: 0.5237329006195068\n",
            "[Epoch 69] Batch loss: 0.5386390686035156\n",
            "[Epoch 69] Batch loss: 0.5234367847442627\n",
            "[Epoch 69] Batch loss: 0.5954160094261169\n",
            "[Epoch 69] Batch loss: 0.5145574808120728\n",
            "[Epoch 69] Batch loss: 0.7663348913192749\n",
            "[Epoch 69] Batch loss: 0.4792235791683197\n",
            "[Epoch 70/100] Val loss: 0.9291\n",
            "Best saved model.\n",
            "[Epoch 70] Batch loss: 0.5423092842102051\n",
            "[Epoch 70] Batch loss: 0.4858628213405609\n",
            "[Epoch 70] Batch loss: 0.5422276854515076\n",
            "[Epoch 70] Batch loss: 0.6059880256652832\n",
            "[Epoch 70] Batch loss: 0.5303823947906494\n",
            "[Epoch 70] Batch loss: 0.4946818947792053\n",
            "[Epoch 70] Batch loss: 0.6588510274887085\n",
            "[Epoch 70] Batch loss: 0.5324850082397461\n",
            "[Epoch 70] Batch loss: 0.6274430751800537\n",
            "[Epoch 70] Batch loss: 0.47264137864112854\n",
            "[Epoch 70] Batch loss: 0.5223069190979004\n",
            "[Epoch 70] Batch loss: 0.5186602473258972\n",
            "[Epoch 70] Batch loss: 0.5944129824638367\n",
            "[Epoch 70] Batch loss: 0.5669161081314087\n",
            "[Epoch 70] Batch loss: 0.5842203497886658\n",
            "[Epoch 70] Batch loss: 0.5263409614562988\n",
            "[Epoch 70] Batch loss: 0.49200668931007385\n",
            "[Epoch 70] Batch loss: 0.663449764251709\n",
            "[Epoch 70] Batch loss: 0.5620346069335938\n",
            "[Epoch 70] Batch loss: 0.5974327921867371\n",
            "[Epoch 70] Batch loss: 0.6760104894638062\n",
            "[Epoch 70] Batch loss: 0.5881532430648804\n",
            "[Epoch 70] Batch loss: 0.605648398399353\n",
            "[Epoch 70] Batch loss: 0.6584604382514954\n",
            "[Epoch 70] Batch loss: 0.5540611743927002\n",
            "[Epoch 70] Batch loss: 0.5319677591323853\n",
            "[Epoch 70] Batch loss: 0.5245543122291565\n",
            "[Epoch 70] Batch loss: 0.4846654236316681\n",
            "[Epoch 70] Batch loss: 0.5364680886268616\n",
            "[Epoch 70] Batch loss: 0.45957881212234497\n",
            "[Epoch 70] Batch loss: 0.540859043598175\n",
            "[Epoch 70] Batch loss: 0.46948689222335815\n",
            "[Epoch 70] Batch loss: 0.5112226009368896\n",
            "[Epoch 70] Batch loss: 0.46033975481987\n",
            "[Epoch 70] Batch loss: 0.5029065012931824\n",
            "[Epoch 70] Batch loss: 0.5532753467559814\n",
            "[Epoch 70] Batch loss: 0.5296505689620972\n",
            "[Epoch 70] Batch loss: 0.6017298698425293\n",
            "[Epoch 70] Batch loss: 0.5450441837310791\n",
            "[Epoch 70] Batch loss: 0.5003854632377625\n",
            "[Epoch 70] Batch loss: 0.6513962745666504\n",
            "[Epoch 70] Batch loss: 0.47610628604888916\n",
            "[Epoch 70] Batch loss: 0.512531578540802\n",
            "[Epoch 70] Batch loss: 0.5046258568763733\n",
            "[Epoch 70] Batch loss: 0.6068079471588135\n",
            "[Epoch 70] Batch loss: 0.5214254260063171\n",
            "[Epoch 70] Batch loss: 0.5616192817687988\n",
            "[Epoch 70] Batch loss: 0.5187615752220154\n",
            "[Epoch 70] Batch loss: 0.5220330357551575\n",
            "[Epoch 70] Batch loss: 0.5724461078643799\n",
            "[Epoch 70] Batch loss: 0.5761066675186157\n",
            "[Epoch 70] Batch loss: 0.5728926062583923\n",
            "[Epoch 70] Batch loss: 0.4749878942966461\n",
            "[Epoch 70] Batch loss: 0.6712893843650818\n",
            "[Epoch 70] Batch loss: 0.6522455215454102\n",
            "[Epoch 70] Batch loss: 0.4471668601036072\n",
            "[Epoch 70] Batch loss: 0.6069867610931396\n",
            "[Epoch 70] Batch loss: 0.5187013149261475\n",
            "[Epoch 70] Batch loss: 0.5798203945159912\n",
            "[Epoch 70] Batch loss: 0.4590126872062683\n",
            "[Epoch 70] Batch loss: 0.48088133335113525\n",
            "[Epoch 70] Batch loss: 0.5022449493408203\n",
            "[Epoch 70] Batch loss: 0.559127688407898\n",
            "[Epoch 70] Batch loss: 0.4466075599193573\n",
            "[Epoch 70] Batch loss: 0.65070641040802\n",
            "[Epoch 70] Batch loss: 0.5905084609985352\n",
            "[Epoch 70] Batch loss: 0.5181007385253906\n",
            "[Epoch 70] Batch loss: 0.43598970770835876\n",
            "[Epoch 70] Batch loss: 0.6100533604621887\n",
            "[Epoch 70] Batch loss: 0.46994224190711975\n",
            "[Epoch 70] Batch loss: 0.6250088810920715\n",
            "[Epoch 70] Batch loss: 0.5194042325019836\n",
            "[Epoch 70] Batch loss: 0.5688985586166382\n",
            "[Epoch 70] Batch loss: 0.6560661196708679\n",
            "[Epoch 70] Batch loss: 0.5466838479042053\n",
            "[Epoch 70] Batch loss: 0.538843035697937\n",
            "[Epoch 70] Batch loss: 0.5730295181274414\n",
            "[Epoch 70] Batch loss: 0.6154470443725586\n",
            "[Epoch 70] Batch loss: 0.5011259317398071\n",
            "[Epoch 70] Batch loss: 0.5917724370956421\n",
            "[Epoch 70] Batch loss: 0.553569495677948\n",
            "[Epoch 70] Batch loss: 0.6261913180351257\n",
            "[Epoch 70] Batch loss: 0.5680161714553833\n",
            "[Epoch 70] Batch loss: 0.4944344162940979\n",
            "[Epoch 70] Batch loss: 0.566955029964447\n",
            "[Epoch 70] Batch loss: 0.4580512046813965\n",
            "[Epoch 70] Batch loss: 0.8092653155326843\n",
            "[Epoch 70] Batch loss: 0.49217861890792847\n",
            "[Epoch 70] Batch loss: 0.5469214916229248\n",
            "[Epoch 70] Batch loss: 0.6163256168365479\n",
            "[Epoch 70] Batch loss: 0.5628690719604492\n",
            "[Epoch 70] Batch loss: 0.4905149042606354\n",
            "[Epoch 70] Batch loss: 0.58123379945755\n",
            "[Epoch 70] Batch loss: 0.6732757091522217\n",
            "[Epoch 70] Batch loss: 0.6809579730033875\n",
            "[Epoch 70] Batch loss: 0.5562609434127808\n",
            "[Epoch 70] Batch loss: 0.49376142024993896\n",
            "[Epoch 70] Batch loss: 0.5056555867195129\n",
            "[Epoch 70] Batch loss: 0.5059762001037598\n",
            "[Epoch 70] Batch loss: 0.5727373957633972\n",
            "[Epoch 70] Batch loss: 0.527927815914154\n",
            "[Epoch 70] Batch loss: 0.6789337992668152\n",
            "[Epoch 70] Batch loss: 0.6016366481781006\n",
            "[Epoch 70] Batch loss: 0.5832501649856567\n",
            "[Epoch 70] Batch loss: 0.516373872756958\n",
            "[Epoch 70] Batch loss: 0.5794358849525452\n",
            "[Epoch 70] Batch loss: 0.6202521920204163\n",
            "[Epoch 70] Batch loss: 0.5721939206123352\n",
            "[Epoch 70] Batch loss: 0.4992992579936981\n",
            "[Epoch 70] Batch loss: 0.5178914070129395\n",
            "[Epoch 70] Batch loss: 0.4297982156276703\n",
            "[Epoch 70] Batch loss: 0.520832896232605\n",
            "[Epoch 70] Batch loss: 0.49997490644454956\n",
            "[Epoch 70] Batch loss: 0.6766195893287659\n",
            "[Epoch 70] Batch loss: 0.5639702677726746\n",
            "[Epoch 70] Batch loss: 0.58134526014328\n",
            "[Epoch 70] Batch loss: 0.5173698663711548\n",
            "[Epoch 70] Batch loss: 0.50933837890625\n",
            "[Epoch 70] Batch loss: 0.535576343536377\n",
            "[Epoch 70] Batch loss: 0.5284856557846069\n",
            "[Epoch 70] Batch loss: 0.6044414639472961\n",
            "[Epoch 70] Batch loss: 0.5363540053367615\n",
            "[Epoch 70] Batch loss: 0.45995187759399414\n",
            "[Epoch 70] Batch loss: 0.6077360510826111\n",
            "[Epoch 70] Batch loss: 0.5166918039321899\n",
            "[Epoch 70] Batch loss: 0.45445680618286133\n",
            "[Epoch 71/100] Val loss: 0.9189\n",
            "Best saved model.\n",
            "[Epoch 71] Batch loss: 0.5709923505783081\n",
            "[Epoch 71] Batch loss: 0.5501331090927124\n",
            "[Epoch 71] Batch loss: 0.5478347539901733\n",
            "[Epoch 71] Batch loss: 0.5450985431671143\n",
            "[Epoch 71] Batch loss: 0.49681222438812256\n",
            "[Epoch 71] Batch loss: 0.5653766989707947\n",
            "[Epoch 71] Batch loss: 0.4826631546020508\n",
            "[Epoch 71] Batch loss: 0.5895297527313232\n",
            "[Epoch 71] Batch loss: 0.5748261213302612\n",
            "[Epoch 71] Batch loss: 0.6425564885139465\n",
            "[Epoch 71] Batch loss: 0.5157721638679504\n",
            "[Epoch 71] Batch loss: 0.5131651163101196\n",
            "[Epoch 71] Batch loss: 0.5312346816062927\n",
            "[Epoch 71] Batch loss: 0.42088940739631653\n",
            "[Epoch 71] Batch loss: 0.4858207702636719\n",
            "[Epoch 71] Batch loss: 0.5659201145172119\n",
            "[Epoch 71] Batch loss: 0.6673431396484375\n",
            "[Epoch 71] Batch loss: 0.5932229161262512\n",
            "[Epoch 71] Batch loss: 0.5145322680473328\n",
            "[Epoch 71] Batch loss: 0.5614420175552368\n",
            "[Epoch 71] Batch loss: 0.5582056641578674\n",
            "[Epoch 71] Batch loss: 0.5179657936096191\n",
            "[Epoch 71] Batch loss: 0.4767136871814728\n",
            "[Epoch 71] Batch loss: 0.497979074716568\n",
            "[Epoch 71] Batch loss: 0.5617881417274475\n",
            "[Epoch 71] Batch loss: 0.6287236213684082\n",
            "[Epoch 71] Batch loss: 0.5618716478347778\n",
            "[Epoch 71] Batch loss: 0.5030692219734192\n",
            "[Epoch 71] Batch loss: 0.45372259616851807\n",
            "[Epoch 71] Batch loss: 0.5657346248626709\n",
            "[Epoch 71] Batch loss: 0.5672671794891357\n",
            "[Epoch 71] Batch loss: 0.5052250623703003\n",
            "[Epoch 71] Batch loss: 0.5633313059806824\n",
            "[Epoch 71] Batch loss: 0.5722247958183289\n",
            "[Epoch 71] Batch loss: 0.6057606935501099\n",
            "[Epoch 71] Batch loss: 0.49995654821395874\n",
            "[Epoch 71] Batch loss: 0.5668312311172485\n",
            "[Epoch 71] Batch loss: 0.6117082238197327\n",
            "[Epoch 71] Batch loss: 0.5589485168457031\n",
            "[Epoch 71] Batch loss: 0.5106981992721558\n",
            "[Epoch 71] Batch loss: 0.5193796753883362\n",
            "[Epoch 71] Batch loss: 0.6484869122505188\n",
            "[Epoch 71] Batch loss: 0.47024330496788025\n",
            "[Epoch 71] Batch loss: 0.5666273236274719\n",
            "[Epoch 71] Batch loss: 0.5814489722251892\n",
            "[Epoch 71] Batch loss: 0.5745238661766052\n",
            "[Epoch 71] Batch loss: 0.5611128807067871\n",
            "[Epoch 71] Batch loss: 0.47355204820632935\n",
            "[Epoch 71] Batch loss: 0.5976033210754395\n",
            "[Epoch 71] Batch loss: 0.5119142532348633\n",
            "[Epoch 71] Batch loss: 0.5166753530502319\n",
            "[Epoch 71] Batch loss: 0.4764554798603058\n",
            "[Epoch 71] Batch loss: 0.5627052783966064\n",
            "[Epoch 71] Batch loss: 0.5565882325172424\n",
            "[Epoch 71] Batch loss: 0.4936513900756836\n",
            "[Epoch 71] Batch loss: 0.530466616153717\n",
            "[Epoch 71] Batch loss: 0.44006025791168213\n",
            "[Epoch 71] Batch loss: 0.43531423807144165\n",
            "[Epoch 71] Batch loss: 0.5333231687545776\n",
            "[Epoch 71] Batch loss: 0.45566388964653015\n",
            "[Epoch 71] Batch loss: 0.604121208190918\n",
            "[Epoch 71] Batch loss: 0.5505901575088501\n",
            "[Epoch 71] Batch loss: 0.6013442277908325\n",
            "[Epoch 71] Batch loss: 0.5151088833808899\n",
            "[Epoch 71] Batch loss: 0.6674287915229797\n",
            "[Epoch 71] Batch loss: 0.528928816318512\n",
            "[Epoch 71] Batch loss: 0.5829468965530396\n",
            "[Epoch 71] Batch loss: 0.5521645545959473\n",
            "[Epoch 71] Batch loss: 0.44864094257354736\n",
            "[Epoch 71] Batch loss: 0.4935205280780792\n",
            "[Epoch 71] Batch loss: 0.45047399401664734\n",
            "[Epoch 71] Batch loss: 0.5190416574478149\n",
            "[Epoch 71] Batch loss: 0.5288457274436951\n",
            "[Epoch 71] Batch loss: 0.5356601476669312\n",
            "[Epoch 71] Batch loss: 0.43004852533340454\n",
            "[Epoch 71] Batch loss: 0.6692412495613098\n",
            "[Epoch 71] Batch loss: 0.6278485655784607\n",
            "[Epoch 71] Batch loss: 0.5096315145492554\n",
            "[Epoch 71] Batch loss: 0.48908111453056335\n",
            "[Epoch 71] Batch loss: 0.4241671860218048\n",
            "[Epoch 71] Batch loss: 0.5466613173484802\n",
            "[Epoch 71] Batch loss: 0.5194080471992493\n",
            "[Epoch 71] Batch loss: 0.5579699277877808\n",
            "[Epoch 71] Batch loss: 0.6480531692504883\n",
            "[Epoch 71] Batch loss: 0.6380791664123535\n",
            "[Epoch 71] Batch loss: 0.6445783376693726\n",
            "[Epoch 71] Batch loss: 0.46849292516708374\n",
            "[Epoch 71] Batch loss: 0.5891963243484497\n",
            "[Epoch 71] Batch loss: 0.47784966230392456\n",
            "[Epoch 71] Batch loss: 0.4941485822200775\n",
            "[Epoch 71] Batch loss: 0.5355433821678162\n",
            "[Epoch 71] Batch loss: 0.77475506067276\n",
            "[Epoch 71] Batch loss: 0.6734293103218079\n",
            "[Epoch 71] Batch loss: 0.49125751852989197\n",
            "[Epoch 71] Batch loss: 0.5651644468307495\n",
            "[Epoch 71] Batch loss: 0.5214515924453735\n",
            "[Epoch 71] Batch loss: 0.4776896834373474\n",
            "[Epoch 71] Batch loss: 0.561274528503418\n",
            "[Epoch 71] Batch loss: 0.5694618821144104\n",
            "[Epoch 71] Batch loss: 0.46915143728256226\n",
            "[Epoch 71] Batch loss: 0.55062335729599\n",
            "[Epoch 71] Batch loss: 0.5168170928955078\n",
            "[Epoch 71] Batch loss: 0.6171184778213501\n",
            "[Epoch 71] Batch loss: 0.4701097309589386\n",
            "[Epoch 71] Batch loss: 0.5790833830833435\n",
            "[Epoch 71] Batch loss: 0.5492730140686035\n",
            "[Epoch 71] Batch loss: 0.6122528314590454\n",
            "[Epoch 71] Batch loss: 0.5461260080337524\n",
            "[Epoch 71] Batch loss: 0.5532168745994568\n",
            "[Epoch 71] Batch loss: 0.5508687496185303\n",
            "[Epoch 71] Batch loss: 0.5178306102752686\n",
            "[Epoch 71] Batch loss: 0.43811875581741333\n",
            "[Epoch 71] Batch loss: 0.48396503925323486\n",
            "[Epoch 71] Batch loss: 0.549394428730011\n",
            "[Epoch 71] Batch loss: 0.47305604815483093\n",
            "[Epoch 71] Batch loss: 0.5087996125221252\n",
            "[Epoch 71] Batch loss: 0.41513490676879883\n",
            "[Epoch 71] Batch loss: 0.5333389639854431\n",
            "[Epoch 71] Batch loss: 0.5166385173797607\n",
            "[Epoch 71] Batch loss: 0.5210264921188354\n",
            "[Epoch 71] Batch loss: 0.5463260412216187\n",
            "[Epoch 71] Batch loss: 0.5341023206710815\n",
            "[Epoch 71] Batch loss: 0.4162420332431793\n",
            "[Epoch 71] Batch loss: 0.5926966667175293\n",
            "[Epoch 71] Batch loss: 0.5608762502670288\n",
            "[Epoch 71] Batch loss: 0.4635530710220337\n",
            "[Epoch 72/100] Val loss: 0.9048\n",
            "Best saved model.\n",
            "[Epoch 72] Batch loss: 0.4948270916938782\n",
            "[Epoch 72] Batch loss: 0.602489709854126\n",
            "[Epoch 72] Batch loss: 0.5098434686660767\n",
            "[Epoch 72] Batch loss: 0.6309874653816223\n",
            "[Epoch 72] Batch loss: 0.5320998430252075\n",
            "[Epoch 72] Batch loss: 0.5841297507286072\n",
            "[Epoch 72] Batch loss: 0.5130763053894043\n",
            "[Epoch 72] Batch loss: 0.6229553818702698\n",
            "[Epoch 72] Batch loss: 0.5873329043388367\n",
            "[Epoch 72] Batch loss: 0.5773211121559143\n",
            "[Epoch 72] Batch loss: 0.6323456764221191\n",
            "[Epoch 72] Batch loss: 0.4476199150085449\n",
            "[Epoch 72] Batch loss: 0.5461474657058716\n",
            "[Epoch 72] Batch loss: 0.44663065671920776\n",
            "[Epoch 72] Batch loss: 0.5389856100082397\n",
            "[Epoch 72] Batch loss: 0.4243772327899933\n",
            "[Epoch 72] Batch loss: 0.5778588652610779\n",
            "[Epoch 72] Batch loss: 0.5848137140274048\n",
            "[Epoch 72] Batch loss: 0.5663352608680725\n",
            "[Epoch 72] Batch loss: 0.48148128390312195\n",
            "[Epoch 72] Batch loss: 0.5317148566246033\n",
            "[Epoch 72] Batch loss: 0.7173174619674683\n",
            "[Epoch 72] Batch loss: 0.47927194833755493\n",
            "[Epoch 72] Batch loss: 0.5138628482818604\n",
            "[Epoch 72] Batch loss: 0.5146409273147583\n",
            "[Epoch 72] Batch loss: 0.47677358984947205\n",
            "[Epoch 72] Batch loss: 0.43851539492607117\n",
            "[Epoch 72] Batch loss: 0.5240839719772339\n",
            "[Epoch 72] Batch loss: 0.6026073694229126\n",
            "[Epoch 72] Batch loss: 0.6785618662834167\n",
            "[Epoch 72] Batch loss: 0.5191556215286255\n",
            "[Epoch 72] Batch loss: 0.4885575771331787\n",
            "[Epoch 72] Batch loss: 0.5608358979225159\n",
            "[Epoch 72] Batch loss: 0.5559428930282593\n",
            "[Epoch 72] Batch loss: 0.4656003713607788\n",
            "[Epoch 72] Batch loss: 0.5754914283752441\n",
            "[Epoch 72] Batch loss: 0.47285065054893494\n",
            "[Epoch 72] Batch loss: 0.44799602031707764\n",
            "[Epoch 72] Batch loss: 0.45653191208839417\n",
            "[Epoch 72] Batch loss: 0.6531442403793335\n",
            "[Epoch 72] Batch loss: 0.4690665602684021\n",
            "[Epoch 72] Batch loss: 0.553528904914856\n",
            "[Epoch 72] Batch loss: 0.5850343108177185\n",
            "[Epoch 72] Batch loss: 0.521142303943634\n",
            "[Epoch 72] Batch loss: 0.5055956244468689\n",
            "[Epoch 72] Batch loss: 0.4558752775192261\n",
            "[Epoch 72] Batch loss: 0.4913662374019623\n",
            "[Epoch 72] Batch loss: 0.5712506771087646\n",
            "[Epoch 72] Batch loss: 0.46114277839660645\n",
            "[Epoch 72] Batch loss: 0.47592997550964355\n",
            "[Epoch 72] Batch loss: 0.6033135652542114\n",
            "[Epoch 72] Batch loss: 0.5430132746696472\n",
            "[Epoch 72] Batch loss: 0.4344722330570221\n",
            "[Epoch 72] Batch loss: 0.5991319417953491\n",
            "[Epoch 72] Batch loss: 0.5183373689651489\n",
            "[Epoch 72] Batch loss: 0.6166570782661438\n",
            "[Epoch 72] Batch loss: 0.619221031665802\n",
            "[Epoch 72] Batch loss: 0.46448200941085815\n",
            "[Epoch 72] Batch loss: 0.5253056287765503\n",
            "[Epoch 72] Batch loss: 0.43628016114234924\n",
            "[Epoch 72] Batch loss: 0.5425873398780823\n",
            "[Epoch 72] Batch loss: 0.5476133823394775\n",
            "[Epoch 72] Batch loss: 0.5954636335372925\n",
            "[Epoch 72] Batch loss: 0.5058882236480713\n",
            "[Epoch 72] Batch loss: 0.4576309621334076\n",
            "[Epoch 72] Batch loss: 0.4772869944572449\n",
            "[Epoch 72] Batch loss: 0.7416526675224304\n",
            "[Epoch 72] Batch loss: 0.5271077156066895\n",
            "[Epoch 72] Batch loss: 0.5979106426239014\n",
            "[Epoch 72] Batch loss: 0.5162783861160278\n",
            "[Epoch 72] Batch loss: 0.4855839014053345\n",
            "[Epoch 72] Batch loss: 0.4674890637397766\n",
            "[Epoch 72] Batch loss: 0.5829564332962036\n",
            "[Epoch 72] Batch loss: 0.9885052442550659\n",
            "[Epoch 72] Batch loss: 0.49577611684799194\n",
            "[Epoch 72] Batch loss: 0.4551570415496826\n",
            "[Epoch 72] Batch loss: 0.5084655284881592\n",
            "[Epoch 72] Batch loss: 0.46216267347335815\n",
            "[Epoch 72] Batch loss: 0.5463102459907532\n",
            "[Epoch 72] Batch loss: 0.4713931381702423\n",
            "[Epoch 72] Batch loss: 0.5333652496337891\n",
            "[Epoch 72] Batch loss: 0.5642091631889343\n",
            "[Epoch 72] Batch loss: 0.45997512340545654\n",
            "[Epoch 72] Batch loss: 0.4552261233329773\n",
            "[Epoch 72] Batch loss: 0.5003562569618225\n",
            "[Epoch 72] Batch loss: 0.5544134974479675\n",
            "[Epoch 72] Batch loss: 0.5484424233436584\n",
            "[Epoch 72] Batch loss: 0.539651095867157\n",
            "[Epoch 72] Batch loss: 0.5640770196914673\n",
            "[Epoch 72] Batch loss: 0.6162625551223755\n",
            "[Epoch 72] Batch loss: 0.45189735293388367\n",
            "[Epoch 72] Batch loss: 0.5127561688423157\n",
            "[Epoch 72] Batch loss: 0.4820247292518616\n",
            "[Epoch 72] Batch loss: 0.6106023788452148\n",
            "[Epoch 72] Batch loss: 0.5267854332923889\n",
            "[Epoch 72] Batch loss: 0.575253427028656\n",
            "[Epoch 72] Batch loss: 0.593339204788208\n",
            "[Epoch 72] Batch loss: 0.48712366819381714\n",
            "[Epoch 72] Batch loss: 0.49610379338264465\n",
            "[Epoch 72] Batch loss: 0.590657651424408\n",
            "[Epoch 72] Batch loss: 0.5259713530540466\n",
            "[Epoch 72] Batch loss: 0.4892367720603943\n",
            "[Epoch 72] Batch loss: 0.6490238904953003\n",
            "[Epoch 72] Batch loss: 0.4992091655731201\n",
            "[Epoch 72] Batch loss: 0.5482470989227295\n",
            "[Epoch 72] Batch loss: 0.4247123599052429\n",
            "[Epoch 72] Batch loss: 0.5247349143028259\n",
            "[Epoch 72] Batch loss: 0.7357805371284485\n",
            "[Epoch 72] Batch loss: 0.5485500693321228\n",
            "[Epoch 72] Batch loss: 0.6773279309272766\n",
            "[Epoch 72] Batch loss: 0.5765733122825623\n",
            "[Epoch 72] Batch loss: 0.5273882746696472\n",
            "[Epoch 72] Batch loss: 0.5267938375473022\n",
            "[Epoch 72] Batch loss: 0.5319699048995972\n",
            "[Epoch 72] Batch loss: 0.599708080291748\n",
            "[Epoch 72] Batch loss: 0.647630512714386\n",
            "[Epoch 72] Batch loss: 0.4934062957763672\n",
            "[Epoch 72] Batch loss: 0.4894018769264221\n",
            "[Epoch 72] Batch loss: 0.4552159309387207\n",
            "[Epoch 72] Batch loss: 0.5093411803245544\n",
            "[Epoch 72] Batch loss: 0.5033776164054871\n",
            "[Epoch 72] Batch loss: 0.5960781574249268\n",
            "[Epoch 72] Batch loss: 0.5385791659355164\n",
            "[Epoch 72] Batch loss: 0.5051421523094177\n",
            "[Epoch 72] Batch loss: 0.6437512636184692\n",
            "[Epoch 72] Batch loss: 0.4543791115283966\n",
            "[Epoch 73/100] Val loss: 0.8926\n",
            "Best saved model.\n",
            "[Epoch 73] Batch loss: 0.6261305809020996\n",
            "[Epoch 73] Batch loss: 0.5345252156257629\n",
            "[Epoch 73] Batch loss: 0.5461704730987549\n",
            "[Epoch 73] Batch loss: 0.47789639234542847\n",
            "[Epoch 73] Batch loss: 0.6054261326789856\n",
            "[Epoch 73] Batch loss: 0.4894489049911499\n",
            "[Epoch 73] Batch loss: 0.46094799041748047\n",
            "[Epoch 73] Batch loss: 0.5873497724533081\n",
            "[Epoch 73] Batch loss: 0.5128769874572754\n",
            "[Epoch 73] Batch loss: 0.6987370848655701\n",
            "[Epoch 73] Batch loss: 0.49239107966423035\n",
            "[Epoch 73] Batch loss: 0.5081287622451782\n",
            "[Epoch 73] Batch loss: 0.5678495764732361\n",
            "[Epoch 73] Batch loss: 0.48747798800468445\n",
            "[Epoch 73] Batch loss: 0.5444626808166504\n",
            "[Epoch 73] Batch loss: 0.5013960003852844\n",
            "[Epoch 73] Batch loss: 0.48803964257240295\n",
            "[Epoch 73] Batch loss: 0.4737119674682617\n",
            "[Epoch 73] Batch loss: 0.47110116481781006\n",
            "[Epoch 73] Batch loss: 0.5294951796531677\n",
            "[Epoch 73] Batch loss: 0.5033613443374634\n",
            "[Epoch 73] Batch loss: 0.4574236571788788\n",
            "[Epoch 73] Batch loss: 0.6341007351875305\n",
            "[Epoch 73] Batch loss: 0.6320472359657288\n",
            "[Epoch 73] Batch loss: 0.5048988461494446\n",
            "[Epoch 73] Batch loss: 0.5896331667900085\n",
            "[Epoch 73] Batch loss: 0.4774411618709564\n",
            "[Epoch 73] Batch loss: 0.6707836389541626\n",
            "[Epoch 73] Batch loss: 0.5368021130561829\n",
            "[Epoch 73] Batch loss: 0.5102805495262146\n",
            "[Epoch 73] Batch loss: 0.4811570346355438\n",
            "[Epoch 73] Batch loss: 0.48798733949661255\n",
            "[Epoch 73] Batch loss: 0.5102509260177612\n",
            "[Epoch 73] Batch loss: 0.5830352902412415\n",
            "[Epoch 73] Batch loss: 0.5894184112548828\n",
            "[Epoch 73] Batch loss: 0.4868614077568054\n",
            "[Epoch 73] Batch loss: 0.5460214018821716\n",
            "[Epoch 73] Batch loss: 0.4718514084815979\n",
            "[Epoch 73] Batch loss: 0.5441645383834839\n",
            "[Epoch 73] Batch loss: 0.4280102252960205\n",
            "[Epoch 73] Batch loss: 0.514915943145752\n",
            "[Epoch 73] Batch loss: 0.6193268299102783\n",
            "[Epoch 73] Batch loss: 0.5801502466201782\n",
            "[Epoch 73] Batch loss: 0.47128942608833313\n",
            "[Epoch 73] Batch loss: 0.4467845857143402\n",
            "[Epoch 73] Batch loss: 0.45183026790618896\n",
            "[Epoch 73] Batch loss: 0.5103685855865479\n",
            "[Epoch 73] Batch loss: 0.623797595500946\n",
            "[Epoch 73] Batch loss: 0.4793783128261566\n",
            "[Epoch 73] Batch loss: 0.5816212892532349\n",
            "[Epoch 73] Batch loss: 0.5477572083473206\n",
            "[Epoch 73] Batch loss: 0.6625903844833374\n",
            "[Epoch 73] Batch loss: 0.49835196137428284\n",
            "[Epoch 73] Batch loss: 0.4433724284172058\n",
            "[Epoch 73] Batch loss: 0.5301644206047058\n",
            "[Epoch 73] Batch loss: 0.5716383457183838\n",
            "[Epoch 73] Batch loss: 0.5702620148658752\n",
            "[Epoch 73] Batch loss: 0.4491960108280182\n",
            "[Epoch 73] Batch loss: 0.41276633739471436\n",
            "[Epoch 73] Batch loss: 0.5340830683708191\n",
            "[Epoch 73] Batch loss: 0.7356101274490356\n",
            "[Epoch 73] Batch loss: 0.4753201901912689\n",
            "[Epoch 73] Batch loss: 0.5400972962379456\n",
            "[Epoch 73] Batch loss: 0.43387115001678467\n",
            "[Epoch 73] Batch loss: 0.5570673942565918\n",
            "[Epoch 73] Batch loss: 0.4558428227901459\n",
            "[Epoch 73] Batch loss: 0.6596955060958862\n",
            "[Epoch 73] Batch loss: 0.5058740377426147\n",
            "[Epoch 73] Batch loss: 0.4808747172355652\n",
            "[Epoch 73] Batch loss: 0.4883473515510559\n",
            "[Epoch 73] Batch loss: 0.5890983939170837\n",
            "[Epoch 73] Batch loss: 0.5523173213005066\n",
            "[Epoch 73] Batch loss: 0.5570223331451416\n",
            "[Epoch 73] Batch loss: 0.521145224571228\n",
            "[Epoch 73] Batch loss: 0.5225141644477844\n",
            "[Epoch 73] Batch loss: 0.5173631906509399\n",
            "[Epoch 73] Batch loss: 0.5640944838523865\n",
            "[Epoch 73] Batch loss: 0.5316044092178345\n",
            "[Epoch 73] Batch loss: 0.4868476986885071\n",
            "[Epoch 73] Batch loss: 0.5078527927398682\n",
            "[Epoch 73] Batch loss: 0.5322999954223633\n",
            "[Epoch 73] Batch loss: 0.54372239112854\n",
            "[Epoch 73] Batch loss: 0.45963889360427856\n",
            "[Epoch 73] Batch loss: 0.511771559715271\n",
            "[Epoch 73] Batch loss: 0.5850487947463989\n",
            "[Epoch 73] Batch loss: 0.5078367590904236\n",
            "[Epoch 73] Batch loss: 0.4315173923969269\n",
            "[Epoch 73] Batch loss: 0.4935815930366516\n",
            "[Epoch 73] Batch loss: 0.5496045351028442\n",
            "[Epoch 73] Batch loss: 0.5682369470596313\n",
            "[Epoch 73] Batch loss: 0.48701411485671997\n",
            "[Epoch 73] Batch loss: 0.4277563989162445\n",
            "[Epoch 73] Batch loss: 0.45936185121536255\n",
            "[Epoch 73] Batch loss: 0.5103064179420471\n",
            "[Epoch 73] Batch loss: 0.48842406272888184\n",
            "[Epoch 73] Batch loss: 0.44624558091163635\n",
            "[Epoch 73] Batch loss: 0.5309196710586548\n",
            "[Epoch 73] Batch loss: 0.5614610314369202\n",
            "[Epoch 73] Batch loss: 0.5527573227882385\n",
            "[Epoch 73] Batch loss: 0.5844509601593018\n",
            "[Epoch 73] Batch loss: 0.5549584627151489\n",
            "[Epoch 73] Batch loss: 0.5402055382728577\n",
            "[Epoch 73] Batch loss: 0.4824318289756775\n",
            "[Epoch 73] Batch loss: 0.6500745415687561\n",
            "[Epoch 73] Batch loss: 0.51972895860672\n",
            "[Epoch 73] Batch loss: 0.5222668051719666\n",
            "[Epoch 73] Batch loss: 0.7606390118598938\n",
            "[Epoch 73] Batch loss: 0.5587334632873535\n",
            "[Epoch 73] Batch loss: 0.6424321532249451\n",
            "[Epoch 73] Batch loss: 0.5677536725997925\n",
            "[Epoch 73] Batch loss: 0.5521959066390991\n",
            "[Epoch 73] Batch loss: 0.4663543999195099\n",
            "[Epoch 73] Batch loss: 0.3944234848022461\n",
            "[Epoch 73] Batch loss: 0.45533251762390137\n",
            "[Epoch 73] Batch loss: 0.5761886239051819\n",
            "[Epoch 73] Batch loss: 0.4230385720729828\n",
            "[Epoch 73] Batch loss: 0.6157517433166504\n",
            "[Epoch 73] Batch loss: 0.5017638802528381\n",
            "[Epoch 73] Batch loss: 0.5495679378509521\n",
            "[Epoch 73] Batch loss: 0.4574269652366638\n",
            "[Epoch 73] Batch loss: 0.4738589823246002\n",
            "[Epoch 73] Batch loss: 0.6261133551597595\n",
            "[Epoch 73] Batch loss: 0.44751089811325073\n",
            "[Epoch 73] Batch loss: 0.6140522956848145\n",
            "[Epoch 73] Batch loss: 0.5155097842216492\n",
            "[Epoch 73] Batch loss: 0.5798021554946899\n",
            "[Epoch 74/100] Val loss: 0.8903\n",
            "Best saved model.\n",
            "[Epoch 74] Batch loss: 0.4899795949459076\n",
            "[Epoch 74] Batch loss: 0.45963194966316223\n",
            "[Epoch 74] Batch loss: 0.5135703086853027\n",
            "[Epoch 74] Batch loss: 0.4688294529914856\n",
            "[Epoch 74] Batch loss: 0.43007954955101013\n",
            "[Epoch 74] Batch loss: 0.5337518453598022\n",
            "[Epoch 74] Batch loss: 0.4672931432723999\n",
            "[Epoch 74] Batch loss: 0.4731132984161377\n",
            "[Epoch 74] Batch loss: 0.46153396368026733\n",
            "[Epoch 74] Batch loss: 0.5771887302398682\n",
            "[Epoch 74] Batch loss: 0.5110638737678528\n",
            "[Epoch 74] Batch loss: 0.5751723647117615\n",
            "[Epoch 74] Batch loss: 0.47159287333488464\n",
            "[Epoch 74] Batch loss: 0.4343871474266052\n",
            "[Epoch 74] Batch loss: 0.5107885003089905\n",
            "[Epoch 74] Batch loss: 0.5990180373191833\n",
            "[Epoch 74] Batch loss: 0.4791930913925171\n",
            "[Epoch 74] Batch loss: 0.47059208154678345\n",
            "[Epoch 74] Batch loss: 0.5707789659500122\n",
            "[Epoch 74] Batch loss: 0.5334521532058716\n",
            "[Epoch 74] Batch loss: 0.4879608750343323\n",
            "[Epoch 74] Batch loss: 0.5032016634941101\n",
            "[Epoch 74] Batch loss: 0.5692957043647766\n",
            "[Epoch 74] Batch loss: 0.6624936461448669\n",
            "[Epoch 74] Batch loss: 0.4702872037887573\n",
            "[Epoch 74] Batch loss: 0.45833924412727356\n",
            "[Epoch 74] Batch loss: 0.5858547687530518\n",
            "[Epoch 74] Batch loss: 0.5141394734382629\n",
            "[Epoch 74] Batch loss: 0.4997265040874481\n",
            "[Epoch 74] Batch loss: 0.5479037165641785\n",
            "[Epoch 74] Batch loss: 0.55947345495224\n",
            "[Epoch 74] Batch loss: 0.5174041986465454\n",
            "[Epoch 74] Batch loss: 0.49691498279571533\n",
            "[Epoch 74] Batch loss: 0.4941866397857666\n",
            "[Epoch 74] Batch loss: 0.5609164834022522\n",
            "[Epoch 74] Batch loss: 0.5030519366264343\n",
            "[Epoch 74] Batch loss: 0.5298588275909424\n",
            "[Epoch 74] Batch loss: 0.5478352308273315\n",
            "[Epoch 74] Batch loss: 0.5190669298171997\n",
            "[Epoch 74] Batch loss: 0.4750446379184723\n",
            "[Epoch 74] Batch loss: 0.44981443881988525\n",
            "[Epoch 74] Batch loss: 0.5104128122329712\n",
            "[Epoch 74] Batch loss: 0.6642751693725586\n",
            "[Epoch 74] Batch loss: 0.3994109034538269\n",
            "[Epoch 74] Batch loss: 0.5651034712791443\n",
            "[Epoch 74] Batch loss: 0.5042669773101807\n",
            "[Epoch 74] Batch loss: 0.5822764039039612\n",
            "[Epoch 74] Batch loss: 0.4722983241081238\n",
            "[Epoch 74] Batch loss: 0.5336637496948242\n",
            "[Epoch 74] Batch loss: 0.5037391185760498\n",
            "[Epoch 74] Batch loss: 0.5693905353546143\n",
            "[Epoch 74] Batch loss: 0.5370087623596191\n",
            "[Epoch 74] Batch loss: 0.745154082775116\n",
            "[Epoch 74] Batch loss: 0.5516438484191895\n",
            "[Epoch 74] Batch loss: 0.48724260926246643\n",
            "[Epoch 74] Batch loss: 0.5781239867210388\n",
            "[Epoch 74] Batch loss: 0.50790935754776\n",
            "[Epoch 74] Batch loss: 0.4717170298099518\n",
            "[Epoch 74] Batch loss: 0.4791533052921295\n",
            "[Epoch 74] Batch loss: 0.44752055406570435\n",
            "[Epoch 74] Batch loss: 0.45273301005363464\n",
            "[Epoch 74] Batch loss: 0.4895328879356384\n",
            "[Epoch 74] Batch loss: 0.4242093563079834\n",
            "[Epoch 74] Batch loss: 0.5874539613723755\n",
            "[Epoch 74] Batch loss: 0.619624674320221\n",
            "[Epoch 74] Batch loss: 0.4810754060745239\n",
            "[Epoch 74] Batch loss: 0.5342564582824707\n",
            "[Epoch 74] Batch loss: 0.5083835124969482\n",
            "[Epoch 74] Batch loss: 0.7347617745399475\n",
            "[Epoch 74] Batch loss: 0.5238525867462158\n",
            "[Epoch 74] Batch loss: 0.5418481230735779\n",
            "[Epoch 74] Batch loss: 0.4623957574367523\n",
            "[Epoch 74] Batch loss: 0.48488613963127136\n",
            "[Epoch 74] Batch loss: 0.49142229557037354\n",
            "[Epoch 74] Batch loss: 0.4197826385498047\n",
            "[Epoch 74] Batch loss: 0.47167402505874634\n",
            "[Epoch 74] Batch loss: 0.48094433546066284\n",
            "[Epoch 74] Batch loss: 0.47471535205841064\n",
            "[Epoch 74] Batch loss: 0.49210742115974426\n",
            "[Epoch 74] Batch loss: 0.6135550141334534\n",
            "[Epoch 74] Batch loss: 0.5410348773002625\n",
            "[Epoch 74] Batch loss: 0.5004349946975708\n",
            "[Epoch 74] Batch loss: 0.5027424097061157\n",
            "[Epoch 74] Batch loss: 0.5148048400878906\n",
            "[Epoch 74] Batch loss: 0.5437679290771484\n",
            "[Epoch 74] Batch loss: 0.4651494324207306\n",
            "[Epoch 74] Batch loss: 0.5227376222610474\n",
            "[Epoch 74] Batch loss: 0.5417546629905701\n",
            "[Epoch 74] Batch loss: 0.5202664732933044\n",
            "[Epoch 74] Batch loss: 0.45749011635780334\n",
            "[Epoch 74] Batch loss: 0.4602013826370239\n",
            "[Epoch 74] Batch loss: 0.7077909111976624\n",
            "[Epoch 74] Batch loss: 0.469965398311615\n",
            "[Epoch 74] Batch loss: 0.4529194235801697\n",
            "[Epoch 74] Batch loss: 0.4721072316169739\n",
            "[Epoch 74] Batch loss: 0.5400674343109131\n",
            "[Epoch 74] Batch loss: 0.49886631965637207\n",
            "[Epoch 74] Batch loss: 0.5662700533866882\n",
            "[Epoch 74] Batch loss: 0.47718966007232666\n",
            "[Epoch 74] Batch loss: 0.4110867381095886\n",
            "[Epoch 74] Batch loss: 0.5546270608901978\n",
            "[Epoch 74] Batch loss: 0.6544437408447266\n",
            "[Epoch 74] Batch loss: 0.45725753903388977\n",
            "[Epoch 74] Batch loss: 0.560808002948761\n",
            "[Epoch 74] Batch loss: 0.48111391067504883\n",
            "[Epoch 74] Batch loss: 0.4517652988433838\n",
            "[Epoch 74] Batch loss: 0.5958020687103271\n",
            "[Epoch 74] Batch loss: 0.4761328399181366\n",
            "[Epoch 74] Batch loss: 0.4888554513454437\n",
            "[Epoch 74] Batch loss: 0.45162567496299744\n",
            "[Epoch 74] Batch loss: 0.5726580619812012\n",
            "[Epoch 74] Batch loss: 0.513578474521637\n",
            "[Epoch 74] Batch loss: 0.4256322979927063\n",
            "[Epoch 74] Batch loss: 0.486878365278244\n",
            "[Epoch 74] Batch loss: 0.5184775590896606\n",
            "[Epoch 74] Batch loss: 0.5675277709960938\n",
            "[Epoch 74] Batch loss: 0.5538965463638306\n",
            "[Epoch 74] Batch loss: 0.6012772917747498\n",
            "[Epoch 74] Batch loss: 0.4680545926094055\n",
            "[Epoch 74] Batch loss: 0.5009351372718811\n",
            "[Epoch 74] Batch loss: 0.54435795545578\n",
            "[Epoch 74] Batch loss: 0.5251625180244446\n",
            "[Epoch 74] Batch loss: 0.5098984241485596\n",
            "[Epoch 74] Batch loss: 0.5266438722610474\n",
            "[Epoch 74] Batch loss: 0.3885631859302521\n",
            "[Epoch 74] Batch loss: 0.6072117686271667\n",
            "[Epoch 75/100] Val loss: 0.8785\n",
            "Best saved model.\n",
            "[Epoch 75] Batch loss: 0.5264903903007507\n",
            "[Epoch 75] Batch loss: 0.47188717126846313\n",
            "[Epoch 75] Batch loss: 0.5166175365447998\n",
            "[Epoch 75] Batch loss: 0.3850977420806885\n",
            "[Epoch 75] Batch loss: 0.5642920136451721\n",
            "[Epoch 75] Batch loss: 0.4845571219921112\n",
            "[Epoch 75] Batch loss: 0.5983337163925171\n",
            "[Epoch 75] Batch loss: 0.5546368360519409\n",
            "[Epoch 75] Batch loss: 0.5162787437438965\n",
            "[Epoch 75] Batch loss: 0.5074976086616516\n",
            "[Epoch 75] Batch loss: 0.45076170563697815\n",
            "[Epoch 75] Batch loss: 0.5440487861633301\n",
            "[Epoch 75] Batch loss: 0.5841982960700989\n",
            "[Epoch 75] Batch loss: 0.4704018533229828\n",
            "[Epoch 75] Batch loss: 0.5871964693069458\n",
            "[Epoch 75] Batch loss: 0.5198510885238647\n",
            "[Epoch 75] Batch loss: 0.4838205873966217\n",
            "[Epoch 75] Batch loss: 0.5357879400253296\n",
            "[Epoch 75] Batch loss: 0.5534178614616394\n",
            "[Epoch 75] Batch loss: 0.5245542526245117\n",
            "[Epoch 75] Batch loss: 0.48474305868148804\n",
            "[Epoch 75] Batch loss: 0.5704488754272461\n",
            "[Epoch 75] Batch loss: 0.5269703269004822\n",
            "[Epoch 75] Batch loss: 0.5216169953346252\n",
            "[Epoch 75] Batch loss: 0.4674457311630249\n",
            "[Epoch 75] Batch loss: 0.4953349828720093\n",
            "[Epoch 75] Batch loss: 0.5461878180503845\n",
            "[Epoch 75] Batch loss: 0.5914114713668823\n",
            "[Epoch 75] Batch loss: 0.5451036691665649\n",
            "[Epoch 75] Batch loss: 0.5929027795791626\n",
            "[Epoch 75] Batch loss: 0.48778998851776123\n",
            "[Epoch 75] Batch loss: 0.4750032424926758\n",
            "[Epoch 75] Batch loss: 0.5313214063644409\n",
            "[Epoch 75] Batch loss: 0.5022712349891663\n",
            "[Epoch 75] Batch loss: 0.7083296179771423\n",
            "[Epoch 75] Batch loss: 0.5349847674369812\n",
            "[Epoch 75] Batch loss: 0.46217650175094604\n",
            "[Epoch 75] Batch loss: 0.60911625623703\n",
            "[Epoch 75] Batch loss: 0.5419241189956665\n",
            "[Epoch 75] Batch loss: 0.5546457171440125\n",
            "[Epoch 75] Batch loss: 0.5990163087844849\n",
            "[Epoch 75] Batch loss: 0.5125075578689575\n",
            "[Epoch 75] Batch loss: 0.45214658975601196\n",
            "[Epoch 75] Batch loss: 0.4456411302089691\n",
            "[Epoch 75] Batch loss: 0.44112786650657654\n",
            "[Epoch 75] Batch loss: 0.44333702325820923\n",
            "[Epoch 75] Batch loss: 0.5487614870071411\n",
            "[Epoch 75] Batch loss: 0.6070118546485901\n",
            "[Epoch 75] Batch loss: 0.5502342581748962\n",
            "[Epoch 75] Batch loss: 0.5351336002349854\n",
            "[Epoch 75] Batch loss: 0.4197946786880493\n",
            "[Epoch 75] Batch loss: 0.5450495481491089\n",
            "[Epoch 75] Batch loss: 0.4386499524116516\n",
            "[Epoch 75] Batch loss: 0.5135653614997864\n",
            "[Epoch 75] Batch loss: 0.5068504214286804\n",
            "[Epoch 75] Batch loss: 0.6288555860519409\n",
            "[Epoch 75] Batch loss: 0.41195598244667053\n",
            "[Epoch 75] Batch loss: 0.617647647857666\n",
            "[Epoch 75] Batch loss: 0.5335792303085327\n",
            "[Epoch 75] Batch loss: 0.5044183731079102\n",
            "[Epoch 75] Batch loss: 0.4527062177658081\n",
            "[Epoch 75] Batch loss: 0.4606834948062897\n",
            "[Epoch 75] Batch loss: 0.5617339611053467\n",
            "[Epoch 75] Batch loss: 0.4954845905303955\n",
            "[Epoch 75] Batch loss: 0.4468032121658325\n",
            "[Epoch 75] Batch loss: 0.49031680822372437\n",
            "[Epoch 75] Batch loss: 0.48009389638900757\n",
            "[Epoch 75] Batch loss: 0.49270907044410706\n",
            "[Epoch 75] Batch loss: 0.5099614262580872\n",
            "[Epoch 75] Batch loss: 0.3912627100944519\n",
            "[Epoch 75] Batch loss: 0.5796031951904297\n",
            "[Epoch 75] Batch loss: 0.6529965400695801\n",
            "[Epoch 75] Batch loss: 0.4542998671531677\n",
            "[Epoch 75] Batch loss: 0.4996834397315979\n",
            "[Epoch 75] Batch loss: 0.5314847230911255\n",
            "[Epoch 75] Batch loss: 0.5814021825790405\n",
            "[Epoch 75] Batch loss: 0.5287117958068848\n",
            "[Epoch 75] Batch loss: 0.4625313878059387\n",
            "[Epoch 75] Batch loss: 0.5039921402931213\n",
            "[Epoch 75] Batch loss: 0.5427578687667847\n",
            "[Epoch 75] Batch loss: 0.49539098143577576\n",
            "[Epoch 75] Batch loss: 0.4231366217136383\n",
            "[Epoch 75] Batch loss: 0.47908881306648254\n",
            "[Epoch 75] Batch loss: 0.5359572768211365\n",
            "[Epoch 75] Batch loss: 0.537342369556427\n",
            "[Epoch 75] Batch loss: 0.5558320879936218\n",
            "[Epoch 75] Batch loss: 0.5045289397239685\n",
            "[Epoch 75] Batch loss: 0.4837670624256134\n",
            "[Epoch 75] Batch loss: 0.4350668489933014\n",
            "[Epoch 75] Batch loss: 0.41477325558662415\n",
            "[Epoch 75] Batch loss: 0.5116793513298035\n",
            "[Epoch 75] Batch loss: 0.46566903591156006\n",
            "[Epoch 75] Batch loss: 0.44387879967689514\n",
            "[Epoch 75] Batch loss: 0.39996337890625\n",
            "[Epoch 75] Batch loss: 0.39328092336654663\n",
            "[Epoch 75] Batch loss: 0.572515070438385\n",
            "[Epoch 75] Batch loss: 0.4960218369960785\n",
            "[Epoch 75] Batch loss: 0.49766770005226135\n",
            "[Epoch 75] Batch loss: 0.49120455980300903\n",
            "[Epoch 75] Batch loss: 0.5224997997283936\n",
            "[Epoch 75] Batch loss: 0.6420394778251648\n",
            "[Epoch 75] Batch loss: 0.3947233557701111\n",
            "[Epoch 75] Batch loss: 0.39250460267066956\n",
            "[Epoch 75] Batch loss: 0.5431910157203674\n",
            "[Epoch 75] Batch loss: 0.5187811255455017\n",
            "[Epoch 75] Batch loss: 0.5040487051010132\n",
            "[Epoch 75] Batch loss: 0.48191893100738525\n",
            "[Epoch 75] Batch loss: 0.43588191270828247\n",
            "[Epoch 75] Batch loss: 0.4550037384033203\n",
            "[Epoch 75] Batch loss: 0.5136319994926453\n",
            "[Epoch 75] Batch loss: 0.4923778176307678\n",
            "[Epoch 75] Batch loss: 0.552330493927002\n",
            "[Epoch 75] Batch loss: 0.48571982979774475\n",
            "[Epoch 75] Batch loss: 0.5377557873725891\n",
            "[Epoch 75] Batch loss: 0.4872414767742157\n",
            "[Epoch 75] Batch loss: 0.6011347770690918\n",
            "[Epoch 75] Batch loss: 0.5808861255645752\n",
            "[Epoch 75] Batch loss: 0.47438275814056396\n",
            "[Epoch 75] Batch loss: 0.4919569194316864\n",
            "[Epoch 75] Batch loss: 0.6545048356056213\n",
            "[Epoch 75] Batch loss: 0.5939206480979919\n",
            "[Epoch 75] Batch loss: 0.4628087878227234\n",
            "[Epoch 75] Batch loss: 0.46065831184387207\n",
            "[Epoch 75] Batch loss: 0.42786905169487\n",
            "[Epoch 75] Batch loss: 0.4781860113143921\n",
            "[Epoch 75] Batch loss: 0.45826125144958496\n",
            "[Epoch 76/100] Val loss: 0.8645\n",
            "Best saved model.\n",
            "[Epoch 76] Batch loss: 0.5008822679519653\n",
            "[Epoch 76] Batch loss: 0.4913659393787384\n",
            "[Epoch 76] Batch loss: 0.4499227702617645\n",
            "[Epoch 76] Batch loss: 0.5973591804504395\n",
            "[Epoch 76] Batch loss: 0.4630635380744934\n",
            "[Epoch 76] Batch loss: 0.4667266309261322\n",
            "[Epoch 76] Batch loss: 0.5535710453987122\n",
            "[Epoch 76] Batch loss: 0.5324814915657043\n",
            "[Epoch 76] Batch loss: 0.535672664642334\n",
            "[Epoch 76] Batch loss: 0.5191262364387512\n",
            "[Epoch 76] Batch loss: 0.46056076884269714\n",
            "[Epoch 76] Batch loss: 0.5399723052978516\n",
            "[Epoch 76] Batch loss: 0.5021630525588989\n",
            "[Epoch 76] Batch loss: 0.4160412549972534\n",
            "[Epoch 76] Batch loss: 0.5105674862861633\n",
            "[Epoch 76] Batch loss: 0.4385085105895996\n",
            "[Epoch 76] Batch loss: 0.54471355676651\n",
            "[Epoch 76] Batch loss: 0.44371894001960754\n",
            "[Epoch 76] Batch loss: 0.5499259233474731\n",
            "[Epoch 76] Batch loss: 0.4974991977214813\n",
            "[Epoch 76] Batch loss: 0.4378087818622589\n",
            "[Epoch 76] Batch loss: 0.49917107820510864\n",
            "[Epoch 76] Batch loss: 0.5679293870925903\n",
            "[Epoch 76] Batch loss: 0.4303337335586548\n",
            "[Epoch 76] Batch loss: 0.436595618724823\n",
            "[Epoch 76] Batch loss: 0.3949979841709137\n",
            "[Epoch 76] Batch loss: 0.44340789318084717\n",
            "[Epoch 76] Batch loss: 0.47840961813926697\n",
            "[Epoch 76] Batch loss: 0.6264535188674927\n",
            "[Epoch 76] Batch loss: 0.5106902122497559\n",
            "[Epoch 76] Batch loss: 0.46560585498809814\n",
            "[Epoch 76] Batch loss: 0.4732266366481781\n",
            "[Epoch 76] Batch loss: 0.5703544616699219\n",
            "[Epoch 76] Batch loss: 0.6182698607444763\n",
            "[Epoch 76] Batch loss: 0.3776257038116455\n",
            "[Epoch 76] Batch loss: 0.4473567605018616\n",
            "[Epoch 76] Batch loss: 0.43683335185050964\n",
            "[Epoch 76] Batch loss: 0.5257642269134521\n",
            "[Epoch 76] Batch loss: 0.5803812742233276\n",
            "[Epoch 76] Batch loss: 0.46787703037261963\n",
            "[Epoch 76] Batch loss: 0.4394351840019226\n",
            "[Epoch 76] Batch loss: 0.41142737865448\n",
            "[Epoch 76] Batch loss: 0.48178839683532715\n",
            "[Epoch 76] Batch loss: 0.5428786873817444\n",
            "[Epoch 76] Batch loss: 0.43118131160736084\n",
            "[Epoch 76] Batch loss: 0.49977970123291016\n",
            "[Epoch 76] Batch loss: 0.42021775245666504\n",
            "[Epoch 76] Batch loss: 0.4387463927268982\n",
            "[Epoch 76] Batch loss: 0.5904301404953003\n",
            "[Epoch 76] Batch loss: 0.4278906285762787\n",
            "[Epoch 76] Batch loss: 0.7105187773704529\n",
            "[Epoch 76] Batch loss: 0.6675440073013306\n",
            "[Epoch 76] Batch loss: 0.48246198892593384\n",
            "[Epoch 76] Batch loss: 0.5430570840835571\n",
            "[Epoch 76] Batch loss: 0.616772472858429\n",
            "[Epoch 76] Batch loss: 0.48381537199020386\n",
            "[Epoch 76] Batch loss: 0.48377907276153564\n",
            "[Epoch 76] Batch loss: 0.48283734917640686\n",
            "[Epoch 76] Batch loss: 0.4189298152923584\n",
            "[Epoch 76] Batch loss: 0.6131952404975891\n",
            "[Epoch 76] Batch loss: 0.4687418043613434\n",
            "[Epoch 76] Batch loss: 0.5174579620361328\n",
            "[Epoch 76] Batch loss: 0.6130527257919312\n",
            "[Epoch 76] Batch loss: 0.5022150874137878\n",
            "[Epoch 76] Batch loss: 0.5120580792427063\n",
            "[Epoch 76] Batch loss: 0.5997492671012878\n",
            "[Epoch 76] Batch loss: 0.5051560401916504\n",
            "[Epoch 76] Batch loss: 0.47967690229415894\n",
            "[Epoch 76] Batch loss: 0.41314196586608887\n",
            "[Epoch 76] Batch loss: 0.5030618906021118\n",
            "[Epoch 76] Batch loss: 0.529643177986145\n",
            "[Epoch 76] Batch loss: 0.5968292951583862\n",
            "[Epoch 76] Batch loss: 0.49725228548049927\n",
            "[Epoch 76] Batch loss: 0.5063748955726624\n",
            "[Epoch 76] Batch loss: 0.48059991002082825\n",
            "[Epoch 76] Batch loss: 0.44244384765625\n",
            "[Epoch 76] Batch loss: 0.4625574052333832\n",
            "[Epoch 76] Batch loss: 0.46427446603775024\n",
            "[Epoch 76] Batch loss: 0.4636403024196625\n",
            "[Epoch 76] Batch loss: 0.5024774670600891\n",
            "[Epoch 76] Batch loss: 0.5734390616416931\n",
            "[Epoch 76] Batch loss: 0.5737833380699158\n",
            "[Epoch 76] Batch loss: 0.46661242842674255\n",
            "[Epoch 76] Batch loss: 0.48185765743255615\n",
            "[Epoch 76] Batch loss: 0.5336573719978333\n",
            "[Epoch 76] Batch loss: 0.5128392577171326\n",
            "[Epoch 76] Batch loss: 0.4143638610839844\n",
            "[Epoch 76] Batch loss: 0.4552580416202545\n",
            "[Epoch 76] Batch loss: 0.4449554979801178\n",
            "[Epoch 76] Batch loss: 0.6955279111862183\n",
            "[Epoch 76] Batch loss: 0.5434025526046753\n",
            "[Epoch 76] Batch loss: 0.48743799328804016\n",
            "[Epoch 76] Batch loss: 0.607003927230835\n",
            "[Epoch 76] Batch loss: 0.5304821729660034\n",
            "[Epoch 76] Batch loss: 0.5382938981056213\n",
            "[Epoch 76] Batch loss: 0.4830332398414612\n",
            "[Epoch 76] Batch loss: 0.520243227481842\n",
            "[Epoch 76] Batch loss: 0.6362088322639465\n",
            "[Epoch 76] Batch loss: 0.4862256348133087\n",
            "[Epoch 76] Batch loss: 0.49610260128974915\n",
            "[Epoch 76] Batch loss: 0.4516119956970215\n",
            "[Epoch 76] Batch loss: 0.52751624584198\n",
            "[Epoch 76] Batch loss: 0.4603782296180725\n",
            "[Epoch 76] Batch loss: 0.4614974856376648\n",
            "[Epoch 76] Batch loss: 0.667915403842926\n",
            "[Epoch 76] Batch loss: 0.6396438479423523\n",
            "[Epoch 76] Batch loss: 0.5187991857528687\n",
            "[Epoch 76] Batch loss: 0.5534370541572571\n",
            "[Epoch 76] Batch loss: 0.5365222096443176\n",
            "[Epoch 76] Batch loss: 0.4905029237270355\n",
            "[Epoch 76] Batch loss: 0.5062674880027771\n",
            "[Epoch 76] Batch loss: 0.511619508266449\n",
            "[Epoch 76] Batch loss: 0.436771959066391\n",
            "[Epoch 76] Batch loss: 0.4662895202636719\n",
            "[Epoch 76] Batch loss: 0.5059497952461243\n",
            "[Epoch 76] Batch loss: 0.48630037903785706\n",
            "[Epoch 76] Batch loss: 0.4500877261161804\n",
            "[Epoch 76] Batch loss: 0.5057797431945801\n",
            "[Epoch 76] Batch loss: 0.4350086748600006\n",
            "[Epoch 76] Batch loss: 0.5651609301567078\n",
            "[Epoch 76] Batch loss: 0.5598404407501221\n",
            "[Epoch 76] Batch loss: 0.49064159393310547\n",
            "[Epoch 76] Batch loss: 0.5495496988296509\n",
            "[Epoch 76] Batch loss: 0.5070740580558777\n",
            "[Epoch 76] Batch loss: 0.44765058159828186\n",
            "[Epoch 76] Batch loss: 0.5058013200759888\n",
            "[Epoch 77/100] Val loss: 0.8463\n",
            "Best saved model.\n",
            "[Epoch 77] Batch loss: 0.49163058400154114\n",
            "[Epoch 77] Batch loss: 0.486366868019104\n",
            "[Epoch 77] Batch loss: 0.4923120141029358\n",
            "[Epoch 77] Batch loss: 0.487994909286499\n",
            "[Epoch 77] Batch loss: 0.4444393217563629\n",
            "[Epoch 77] Batch loss: 0.47420012950897217\n",
            "[Epoch 77] Batch loss: 0.5159424543380737\n",
            "[Epoch 77] Batch loss: 0.5090302228927612\n",
            "[Epoch 77] Batch loss: 0.4253133237361908\n",
            "[Epoch 77] Batch loss: 0.5723732709884644\n",
            "[Epoch 77] Batch loss: 0.4999118447303772\n",
            "[Epoch 77] Batch loss: 0.528917133808136\n",
            "[Epoch 77] Batch loss: 0.48721569776535034\n",
            "[Epoch 77] Batch loss: 0.4785887598991394\n",
            "[Epoch 77] Batch loss: 0.47163939476013184\n",
            "[Epoch 77] Batch loss: 0.43212735652923584\n",
            "[Epoch 77] Batch loss: 0.42113566398620605\n",
            "[Epoch 77] Batch loss: 0.39593127369880676\n",
            "[Epoch 77] Batch loss: 0.5926567316055298\n",
            "[Epoch 77] Batch loss: 0.46786195039749146\n",
            "[Epoch 77] Batch loss: 0.47362130880355835\n",
            "[Epoch 77] Batch loss: 0.47224438190460205\n",
            "[Epoch 77] Batch loss: 0.5195497870445251\n",
            "[Epoch 77] Batch loss: 0.3798419237136841\n",
            "[Epoch 77] Batch loss: 0.6057647466659546\n",
            "[Epoch 77] Batch loss: 0.45814040303230286\n",
            "[Epoch 77] Batch loss: 0.5111519694328308\n",
            "[Epoch 77] Batch loss: 0.48302727937698364\n",
            "[Epoch 77] Batch loss: 0.4764072895050049\n",
            "[Epoch 77] Batch loss: 0.5272241234779358\n",
            "[Epoch 77] Batch loss: 0.5197186470031738\n",
            "[Epoch 77] Batch loss: 0.4966124892234802\n",
            "[Epoch 77] Batch loss: 0.41052761673927307\n",
            "[Epoch 77] Batch loss: 0.5317654609680176\n",
            "[Epoch 77] Batch loss: 0.45934006571769714\n",
            "[Epoch 77] Batch loss: 0.4572469890117645\n",
            "[Epoch 77] Batch loss: 0.43667036294937134\n",
            "[Epoch 77] Batch loss: 0.5068857669830322\n",
            "[Epoch 77] Batch loss: 0.49885931611061096\n",
            "[Epoch 77] Batch loss: 0.4517061114311218\n",
            "[Epoch 77] Batch loss: 0.5629108548164368\n",
            "[Epoch 77] Batch loss: 0.533083975315094\n",
            "[Epoch 77] Batch loss: 0.5659258365631104\n",
            "[Epoch 77] Batch loss: 0.5847578644752502\n",
            "[Epoch 77] Batch loss: 0.570598304271698\n",
            "[Epoch 77] Batch loss: 0.4402744472026825\n",
            "[Epoch 77] Batch loss: 0.4917176365852356\n",
            "[Epoch 77] Batch loss: 0.5259773135185242\n",
            "[Epoch 77] Batch loss: 0.4668125510215759\n",
            "[Epoch 77] Batch loss: 0.483790785074234\n",
            "[Epoch 77] Batch loss: 0.4520191550254822\n",
            "[Epoch 77] Batch loss: 0.4199308156967163\n",
            "[Epoch 77] Batch loss: 0.48878228664398193\n",
            "[Epoch 77] Batch loss: 0.4348328411579132\n",
            "[Epoch 77] Batch loss: 0.5157613158226013\n",
            "[Epoch 77] Batch loss: 0.4869363307952881\n",
            "[Epoch 77] Batch loss: 0.53172367811203\n",
            "[Epoch 77] Batch loss: 0.4239441156387329\n",
            "[Epoch 77] Batch loss: 0.4400951564311981\n",
            "[Epoch 77] Batch loss: 0.4922381639480591\n",
            "[Epoch 77] Batch loss: 0.5165483951568604\n",
            "[Epoch 77] Batch loss: 0.5119885206222534\n",
            "[Epoch 77] Batch loss: 0.5421029329299927\n",
            "[Epoch 77] Batch loss: 0.5840149521827698\n",
            "[Epoch 77] Batch loss: 0.42915770411491394\n",
            "[Epoch 77] Batch loss: 0.5466930866241455\n",
            "[Epoch 77] Batch loss: 0.5204787254333496\n",
            "[Epoch 77] Batch loss: 0.4916244149208069\n",
            "[Epoch 77] Batch loss: 0.5043949484825134\n",
            "[Epoch 77] Batch loss: 0.4941779375076294\n",
            "[Epoch 77] Batch loss: 0.44022008776664734\n",
            "[Epoch 77] Batch loss: 0.5316817760467529\n",
            "[Epoch 77] Batch loss: 0.5095083117485046\n",
            "[Epoch 77] Batch loss: 0.617271363735199\n",
            "[Epoch 77] Batch loss: 0.5903844833374023\n",
            "[Epoch 77] Batch loss: 0.5174135565757751\n",
            "[Epoch 77] Batch loss: 0.5155709981918335\n",
            "[Epoch 77] Batch loss: 0.5067062973976135\n",
            "[Epoch 77] Batch loss: 0.4289819300174713\n",
            "[Epoch 77] Batch loss: 0.5975952744483948\n",
            "[Epoch 77] Batch loss: 0.5262603759765625\n",
            "[Epoch 77] Batch loss: 0.5254099369049072\n",
            "[Epoch 77] Batch loss: 0.6352096199989319\n",
            "[Epoch 77] Batch loss: 0.4428980350494385\n",
            "[Epoch 77] Batch loss: 0.40636250376701355\n",
            "[Epoch 77] Batch loss: 0.4968978762626648\n",
            "[Epoch 77] Batch loss: 0.5635054707527161\n",
            "[Epoch 77] Batch loss: 0.602903425693512\n",
            "[Epoch 77] Batch loss: 0.48067188262939453\n",
            "[Epoch 77] Batch loss: 0.45806050300598145\n",
            "[Epoch 77] Batch loss: 0.5753522515296936\n",
            "[Epoch 77] Batch loss: 0.4266481101512909\n",
            "[Epoch 77] Batch loss: 0.5492466688156128\n",
            "[Epoch 77] Batch loss: 0.4845367968082428\n",
            "[Epoch 77] Batch loss: 0.6305510401725769\n",
            "[Epoch 77] Batch loss: 0.48234614729881287\n",
            "[Epoch 77] Batch loss: 0.5858842730522156\n",
            "[Epoch 77] Batch loss: 0.6882591843605042\n",
            "[Epoch 77] Batch loss: 0.38842523097991943\n",
            "[Epoch 77] Batch loss: 0.44343021512031555\n",
            "[Epoch 77] Batch loss: 0.4885888397693634\n",
            "[Epoch 77] Batch loss: 0.45406562089920044\n",
            "[Epoch 77] Batch loss: 0.656967043876648\n",
            "[Epoch 77] Batch loss: 0.5861133337020874\n",
            "[Epoch 77] Batch loss: 0.6537901163101196\n",
            "[Epoch 77] Batch loss: 0.4725058972835541\n",
            "[Epoch 77] Batch loss: 0.48580217361450195\n",
            "[Epoch 77] Batch loss: 0.48160430788993835\n",
            "[Epoch 77] Batch loss: 0.41519588232040405\n",
            "[Epoch 77] Batch loss: 0.47566208243370056\n",
            "[Epoch 77] Batch loss: 0.5161837339401245\n",
            "[Epoch 77] Batch loss: 0.46524736285209656\n",
            "[Epoch 77] Batch loss: 0.46339911222457886\n",
            "[Epoch 77] Batch loss: 0.4927370548248291\n",
            "[Epoch 77] Batch loss: 0.5268001556396484\n",
            "[Epoch 77] Batch loss: 0.5130419135093689\n",
            "[Epoch 77] Batch loss: 0.6661299467086792\n",
            "[Epoch 77] Batch loss: 0.5490618944168091\n",
            "[Epoch 77] Batch loss: 0.5320799350738525\n",
            "[Epoch 77] Batch loss: 0.4753088355064392\n",
            "[Epoch 77] Batch loss: 0.6244135499000549\n",
            "[Epoch 77] Batch loss: 0.5184649229049683\n",
            "[Epoch 77] Batch loss: 0.5065912008285522\n",
            "[Epoch 77] Batch loss: 0.5621875524520874\n",
            "[Epoch 77] Batch loss: 0.4946058690547943\n",
            "[Epoch 77] Batch loss: 0.7457801103591919\n",
            "[Epoch 78/100] Val loss: 0.8412\n",
            "Best saved model.\n",
            "[Epoch 78] Batch loss: 0.5041312575340271\n",
            "[Epoch 78] Batch loss: 0.5074082612991333\n",
            "[Epoch 78] Batch loss: 0.4564528167247772\n",
            "[Epoch 78] Batch loss: 0.4722074568271637\n",
            "[Epoch 78] Batch loss: 0.4877382218837738\n",
            "[Epoch 78] Batch loss: 0.5130668878555298\n",
            "[Epoch 78] Batch loss: 0.46395647525787354\n",
            "[Epoch 78] Batch loss: 0.42953088879585266\n",
            "[Epoch 78] Batch loss: 0.42452898621559143\n",
            "[Epoch 78] Batch loss: 0.47920864820480347\n",
            "[Epoch 78] Batch loss: 0.41582605242729187\n",
            "[Epoch 78] Batch loss: 0.467026948928833\n",
            "[Epoch 78] Batch loss: 0.6190004348754883\n",
            "[Epoch 78] Batch loss: 0.49306318163871765\n",
            "[Epoch 78] Batch loss: 0.4939696192741394\n",
            "[Epoch 78] Batch loss: 0.4407721757888794\n",
            "[Epoch 78] Batch loss: 0.6000414490699768\n",
            "[Epoch 78] Batch loss: 0.4091717004776001\n",
            "[Epoch 78] Batch loss: 0.48854580521583557\n",
            "[Epoch 78] Batch loss: 0.5038315057754517\n",
            "[Epoch 78] Batch loss: 0.519083559513092\n",
            "[Epoch 78] Batch loss: 0.45309752225875854\n",
            "[Epoch 78] Batch loss: 0.49547937512397766\n",
            "[Epoch 78] Batch loss: 0.5036302208900452\n",
            "[Epoch 78] Batch loss: 0.4609259068965912\n",
            "[Epoch 78] Batch loss: 0.4575853943824768\n",
            "[Epoch 78] Batch loss: 0.6166335344314575\n",
            "[Epoch 78] Batch loss: 0.4941086173057556\n",
            "[Epoch 78] Batch loss: 0.41692784428596497\n",
            "[Epoch 78] Batch loss: 0.5726315379142761\n",
            "[Epoch 78] Batch loss: 0.6540711522102356\n",
            "[Epoch 78] Batch loss: 0.47976022958755493\n",
            "[Epoch 78] Batch loss: 0.45434436202049255\n",
            "[Epoch 78] Batch loss: 0.5428977608680725\n",
            "[Epoch 78] Batch loss: 0.46527382731437683\n",
            "[Epoch 78] Batch loss: 0.5182112455368042\n",
            "[Epoch 78] Batch loss: 0.6567819118499756\n",
            "[Epoch 78] Batch loss: 0.5528604984283447\n",
            "[Epoch 78] Batch loss: 0.5685511231422424\n",
            "[Epoch 78] Batch loss: 0.5598998069763184\n",
            "[Epoch 78] Batch loss: 0.4855303168296814\n",
            "[Epoch 78] Batch loss: 0.47182875871658325\n",
            "[Epoch 78] Batch loss: 0.41872113943099976\n",
            "[Epoch 78] Batch loss: 0.5059017539024353\n",
            "[Epoch 78] Batch loss: 0.5537731051445007\n",
            "[Epoch 78] Batch loss: 0.44709300994873047\n",
            "[Epoch 78] Batch loss: 0.6902262568473816\n",
            "[Epoch 78] Batch loss: 0.43447059392929077\n",
            "[Epoch 78] Batch loss: 0.4807281196117401\n",
            "[Epoch 78] Batch loss: 0.45930856466293335\n",
            "[Epoch 78] Batch loss: 0.44841670989990234\n",
            "[Epoch 78] Batch loss: 0.4101211130619049\n",
            "[Epoch 78] Batch loss: 0.416269987821579\n",
            "[Epoch 78] Batch loss: 0.573517382144928\n",
            "[Epoch 78] Batch loss: 0.48020946979522705\n",
            "[Epoch 78] Batch loss: 0.5183094143867493\n",
            "[Epoch 78] Batch loss: 0.49322131276130676\n",
            "[Epoch 78] Batch loss: 0.600895881652832\n",
            "[Epoch 78] Batch loss: 0.511991024017334\n",
            "[Epoch 78] Batch loss: 0.6301766037940979\n",
            "[Epoch 78] Batch loss: 0.44765418767929077\n",
            "[Epoch 78] Batch loss: 0.4851365089416504\n",
            "[Epoch 78] Batch loss: 0.5635149478912354\n",
            "[Epoch 78] Batch loss: 0.48224350810050964\n",
            "[Epoch 78] Batch loss: 0.535718560218811\n",
            "[Epoch 78] Batch loss: 0.4872632622718811\n",
            "[Epoch 78] Batch loss: 0.4709257185459137\n",
            "[Epoch 78] Batch loss: 0.49063944816589355\n",
            "[Epoch 78] Batch loss: 0.45880013704299927\n",
            "[Epoch 78] Batch loss: 0.5267495512962341\n",
            "[Epoch 78] Batch loss: 0.5477157831192017\n",
            "[Epoch 78] Batch loss: 0.4599113464355469\n",
            "[Epoch 78] Batch loss: 0.5552435517311096\n",
            "[Epoch 78] Batch loss: 0.5172872543334961\n",
            "[Epoch 78] Batch loss: 0.5020955204963684\n",
            "[Epoch 78] Batch loss: 0.5584580898284912\n",
            "[Epoch 78] Batch loss: 0.5451563000679016\n",
            "[Epoch 78] Batch loss: 0.5200853943824768\n",
            "[Epoch 78] Batch loss: 0.4980929493904114\n",
            "[Epoch 78] Batch loss: 0.43273210525512695\n",
            "[Epoch 78] Batch loss: 0.3855222761631012\n",
            "[Epoch 78] Batch loss: 0.4695686399936676\n",
            "[Epoch 78] Batch loss: 0.5460398197174072\n",
            "[Epoch 78] Batch loss: 0.5084908604621887\n",
            "[Epoch 78] Batch loss: 0.5586233735084534\n",
            "[Epoch 78] Batch loss: 0.4649500846862793\n",
            "[Epoch 78] Batch loss: 0.4927419424057007\n",
            "[Epoch 78] Batch loss: 0.47628819942474365\n",
            "[Epoch 78] Batch loss: 0.47359272837638855\n",
            "[Epoch 78] Batch loss: 0.41559523344039917\n",
            "[Epoch 78] Batch loss: 0.4860813319683075\n",
            "[Epoch 78] Batch loss: 0.45838066935539246\n",
            "[Epoch 78] Batch loss: 0.4276650846004486\n",
            "[Epoch 78] Batch loss: 0.4540412127971649\n",
            "[Epoch 78] Batch loss: 0.5937698483467102\n",
            "[Epoch 78] Batch loss: 0.43054020404815674\n",
            "[Epoch 78] Batch loss: 0.5584447979927063\n",
            "[Epoch 78] Batch loss: 0.467682421207428\n",
            "[Epoch 78] Batch loss: 0.46365121006965637\n",
            "[Epoch 78] Batch loss: 0.5062904357910156\n",
            "[Epoch 78] Batch loss: 0.48271554708480835\n",
            "[Epoch 78] Batch loss: 0.5005374550819397\n",
            "[Epoch 78] Batch loss: 0.42306241393089294\n",
            "[Epoch 78] Batch loss: 0.5967624187469482\n",
            "[Epoch 78] Batch loss: 0.4612703323364258\n",
            "[Epoch 78] Batch loss: 0.5488437414169312\n",
            "[Epoch 78] Batch loss: 0.4569467008113861\n",
            "[Epoch 78] Batch loss: 0.5156503319740295\n",
            "[Epoch 78] Batch loss: 0.5081020593643188\n",
            "[Epoch 78] Batch loss: 0.42883196473121643\n",
            "[Epoch 78] Batch loss: 0.4840101897716522\n",
            "[Epoch 78] Batch loss: 0.37680384516716003\n",
            "[Epoch 78] Batch loss: 0.4887521266937256\n",
            "[Epoch 78] Batch loss: 0.5195707082748413\n",
            "[Epoch 78] Batch loss: 0.4312647581100464\n",
            "[Epoch 78] Batch loss: 0.4213360548019409\n",
            "[Epoch 78] Batch loss: 0.5711353421211243\n",
            "[Epoch 78] Batch loss: 0.4713057279586792\n",
            "[Epoch 78] Batch loss: 0.5033194422721863\n",
            "[Epoch 78] Batch loss: 0.4055022597312927\n",
            "[Epoch 78] Batch loss: 0.48336249589920044\n",
            "[Epoch 78] Batch loss: 0.5208439230918884\n",
            "[Epoch 78] Batch loss: 0.44752660393714905\n",
            "[Epoch 78] Batch loss: 0.5042068958282471\n",
            "[Epoch 78] Batch loss: 0.42047226428985596\n",
            "[Epoch 78] Batch loss: 0.3794982433319092\n",
            "[Epoch 79/100] Val loss: 0.8345\n",
            "Best saved model.\n",
            "[Epoch 79] Batch loss: 0.6292850971221924\n",
            "[Epoch 79] Batch loss: 0.514210045337677\n",
            "[Epoch 79] Batch loss: 0.5046190023422241\n",
            "[Epoch 79] Batch loss: 0.5087305307388306\n",
            "[Epoch 79] Batch loss: 0.45212501287460327\n",
            "[Epoch 79] Batch loss: 0.5424003005027771\n",
            "[Epoch 79] Batch loss: 0.46626511216163635\n",
            "[Epoch 79] Batch loss: 0.43777990341186523\n",
            "[Epoch 79] Batch loss: 0.48225313425064087\n",
            "[Epoch 79] Batch loss: 0.5335061550140381\n",
            "[Epoch 79] Batch loss: 0.48505502939224243\n",
            "[Epoch 79] Batch loss: 0.5206414461135864\n",
            "[Epoch 79] Batch loss: 0.6821470260620117\n",
            "[Epoch 79] Batch loss: 0.5275885462760925\n",
            "[Epoch 79] Batch loss: 0.49761679768562317\n",
            "[Epoch 79] Batch loss: 0.5899330973625183\n",
            "[Epoch 79] Batch loss: 0.4660918116569519\n",
            "[Epoch 79] Batch loss: 0.423331618309021\n",
            "[Epoch 79] Batch loss: 0.5027832984924316\n",
            "[Epoch 79] Batch loss: 0.42088398337364197\n",
            "[Epoch 79] Batch loss: 0.5769257545471191\n",
            "[Epoch 79] Batch loss: 0.40522313117980957\n",
            "[Epoch 79] Batch loss: 0.5456111431121826\n",
            "[Epoch 79] Batch loss: 0.4622778296470642\n",
            "[Epoch 79] Batch loss: 0.4441606402397156\n",
            "[Epoch 79] Batch loss: 0.43637725710868835\n",
            "[Epoch 79] Batch loss: 0.46258097887039185\n",
            "[Epoch 79] Batch loss: 0.42698514461517334\n",
            "[Epoch 79] Batch loss: 0.407559871673584\n",
            "[Epoch 79] Batch loss: 0.4194020628929138\n",
            "[Epoch 79] Batch loss: 0.4604782164096832\n",
            "[Epoch 79] Batch loss: 0.5292389988899231\n",
            "[Epoch 79] Batch loss: 0.40889546275138855\n",
            "[Epoch 79] Batch loss: 0.41066253185272217\n",
            "[Epoch 79] Batch loss: 0.43299609422683716\n",
            "[Epoch 79] Batch loss: 0.4328795373439789\n",
            "[Epoch 79] Batch loss: 0.4544353783130646\n",
            "[Epoch 79] Batch loss: 0.4541301131248474\n",
            "[Epoch 79] Batch loss: 0.5755694508552551\n",
            "[Epoch 79] Batch loss: 0.535234272480011\n",
            "[Epoch 79] Batch loss: 0.47537699341773987\n",
            "[Epoch 79] Batch loss: 0.46865513920783997\n",
            "[Epoch 79] Batch loss: 0.4689306318759918\n",
            "[Epoch 79] Batch loss: 0.506962239742279\n",
            "[Epoch 79] Batch loss: 0.5606489777565002\n",
            "[Epoch 79] Batch loss: 0.44444018602371216\n",
            "[Epoch 79] Batch loss: 0.5367456078529358\n",
            "[Epoch 79] Batch loss: 0.4654850363731384\n",
            "[Epoch 79] Batch loss: 0.5558960437774658\n",
            "[Epoch 79] Batch loss: 0.5239250659942627\n",
            "[Epoch 79] Batch loss: 0.4093238413333893\n",
            "[Epoch 79] Batch loss: 0.4630080759525299\n",
            "[Epoch 79] Batch loss: 0.532208263874054\n",
            "[Epoch 79] Batch loss: 0.46901458501815796\n",
            "[Epoch 79] Batch loss: 0.7013729214668274\n",
            "[Epoch 79] Batch loss: 0.46393904089927673\n",
            "[Epoch 79] Batch loss: 0.5019336938858032\n",
            "[Epoch 79] Batch loss: 0.4603841304779053\n",
            "[Epoch 79] Batch loss: 0.444336473941803\n",
            "[Epoch 79] Batch loss: 0.49292364716529846\n",
            "[Epoch 79] Batch loss: 0.5937008261680603\n",
            "[Epoch 79] Batch loss: 0.4959667921066284\n",
            "[Epoch 79] Batch loss: 0.6128672957420349\n",
            "[Epoch 79] Batch loss: 0.5355098247528076\n",
            "[Epoch 79] Batch loss: 0.50144362449646\n",
            "[Epoch 79] Batch loss: 0.4765883684158325\n",
            "[Epoch 79] Batch loss: 0.5019204616546631\n",
            "[Epoch 79] Batch loss: 0.37289997935295105\n",
            "[Epoch 79] Batch loss: 0.5341849327087402\n",
            "[Epoch 79] Batch loss: 0.4551660120487213\n",
            "[Epoch 79] Batch loss: 0.4520264267921448\n",
            "[Epoch 79] Batch loss: 0.595259964466095\n",
            "[Epoch 79] Batch loss: 0.5294197797775269\n",
            "[Epoch 79] Batch loss: 0.5090850591659546\n",
            "[Epoch 79] Batch loss: 0.5359418392181396\n",
            "[Epoch 79] Batch loss: 0.5628916025161743\n",
            "[Epoch 79] Batch loss: 0.4557551145553589\n",
            "[Epoch 79] Batch loss: 0.5551795363426208\n",
            "[Epoch 79] Batch loss: 0.41215649247169495\n",
            "[Epoch 79] Batch loss: 0.5183217525482178\n",
            "[Epoch 79] Batch loss: 0.41703805327415466\n",
            "[Epoch 79] Batch loss: 0.5442875623703003\n",
            "[Epoch 79] Batch loss: 0.4744725823402405\n",
            "[Epoch 79] Batch loss: 0.5166463851928711\n",
            "[Epoch 79] Batch loss: 0.5316745042800903\n",
            "[Epoch 79] Batch loss: 0.5091310143470764\n",
            "[Epoch 79] Batch loss: 0.40703049302101135\n",
            "[Epoch 79] Batch loss: 0.4258732795715332\n",
            "[Epoch 79] Batch loss: 0.5210922360420227\n",
            "[Epoch 79] Batch loss: 0.3950619101524353\n",
            "[Epoch 79] Batch loss: 0.6035272479057312\n",
            "[Epoch 79] Batch loss: 0.4775054156780243\n",
            "[Epoch 79] Batch loss: 0.43935105204582214\n",
            "[Epoch 79] Batch loss: 0.5353749990463257\n",
            "[Epoch 79] Batch loss: 0.5054236650466919\n",
            "[Epoch 79] Batch loss: 0.463913232088089\n",
            "[Epoch 79] Batch loss: 0.47323286533355713\n",
            "[Epoch 79] Batch loss: 0.5595188140869141\n",
            "[Epoch 79] Batch loss: 0.47514957189559937\n",
            "[Epoch 79] Batch loss: 0.47043493390083313\n",
            "[Epoch 79] Batch loss: 0.44712895154953003\n",
            "[Epoch 79] Batch loss: 0.48015522956848145\n",
            "[Epoch 79] Batch loss: 0.44226568937301636\n",
            "[Epoch 79] Batch loss: 0.47823435068130493\n",
            "[Epoch 79] Batch loss: 0.5406421422958374\n",
            "[Epoch 79] Batch loss: 0.47944408655166626\n",
            "[Epoch 79] Batch loss: 0.4448484778404236\n",
            "[Epoch 79] Batch loss: 0.47907334566116333\n",
            "[Epoch 79] Batch loss: 0.41797319054603577\n",
            "[Epoch 79] Batch loss: 0.4778307378292084\n",
            "[Epoch 79] Batch loss: 0.48321038484573364\n",
            "[Epoch 79] Batch loss: 0.4949546456336975\n",
            "[Epoch 79] Batch loss: 0.44905081391334534\n",
            "[Epoch 79] Batch loss: 0.4182327389717102\n",
            "[Epoch 79] Batch loss: 0.5471822619438171\n",
            "[Epoch 79] Batch loss: 0.4697265028953552\n",
            "[Epoch 79] Batch loss: 0.46234118938446045\n",
            "[Epoch 79] Batch loss: 0.5674387216567993\n",
            "[Epoch 79] Batch loss: 0.4430882930755615\n",
            "[Epoch 79] Batch loss: 0.4357641935348511\n",
            "[Epoch 79] Batch loss: 0.5938194394111633\n",
            "[Epoch 79] Batch loss: 0.45825496315956116\n",
            "[Epoch 79] Batch loss: 0.591593325138092\n",
            "[Epoch 79] Batch loss: 0.5608387589454651\n",
            "[Epoch 79] Batch loss: 0.4409728944301605\n",
            "[Epoch 79] Batch loss: 0.4624764025211334\n",
            "[Epoch 80/100] Val loss: 0.8291\n",
            "Best saved model.\n",
            "[Epoch 80] Batch loss: 0.5455328226089478\n",
            "[Epoch 80] Batch loss: 0.420307993888855\n",
            "[Epoch 80] Batch loss: 0.5455818772315979\n",
            "[Epoch 80] Batch loss: 0.49164530634880066\n",
            "[Epoch 80] Batch loss: 0.4743850827217102\n",
            "[Epoch 80] Batch loss: 0.49685731530189514\n",
            "[Epoch 80] Batch loss: 0.5001441240310669\n",
            "[Epoch 80] Batch loss: 0.3876084089279175\n",
            "[Epoch 80] Batch loss: 0.5325906276702881\n",
            "[Epoch 80] Batch loss: 0.44767889380455017\n",
            "[Epoch 80] Batch loss: 0.587173342704773\n",
            "[Epoch 80] Batch loss: 0.5029984712600708\n",
            "[Epoch 80] Batch loss: 0.4639485776424408\n",
            "[Epoch 80] Batch loss: 0.4366452991962433\n",
            "[Epoch 80] Batch loss: 0.5076319575309753\n",
            "[Epoch 80] Batch loss: 0.3744775950908661\n",
            "[Epoch 80] Batch loss: 0.35584592819213867\n",
            "[Epoch 80] Batch loss: 0.4259122610092163\n",
            "[Epoch 80] Batch loss: 0.43415629863739014\n",
            "[Epoch 80] Batch loss: 0.42873501777648926\n",
            "[Epoch 80] Batch loss: 0.6181443929672241\n",
            "[Epoch 80] Batch loss: 0.42217692732810974\n",
            "[Epoch 80] Batch loss: 0.45307281613349915\n",
            "[Epoch 80] Batch loss: 0.47402089834213257\n",
            "[Epoch 80] Batch loss: 0.40448427200317383\n",
            "[Epoch 80] Batch loss: 0.4664224684238434\n",
            "[Epoch 80] Batch loss: 0.5137426853179932\n",
            "[Epoch 80] Batch loss: 0.457950234413147\n",
            "[Epoch 80] Batch loss: 0.4374946355819702\n",
            "[Epoch 80] Batch loss: 0.5226793885231018\n",
            "[Epoch 80] Batch loss: 0.4625435471534729\n",
            "[Epoch 80] Batch loss: 0.43789273500442505\n",
            "[Epoch 80] Batch loss: 0.4541412889957428\n",
            "[Epoch 80] Batch loss: 0.419249951839447\n",
            "[Epoch 80] Batch loss: 0.4345269203186035\n",
            "[Epoch 80] Batch loss: 0.46089017391204834\n",
            "[Epoch 80] Batch loss: 0.5412811636924744\n",
            "[Epoch 80] Batch loss: 0.49136751890182495\n",
            "[Epoch 80] Batch loss: 0.44137880206108093\n",
            "[Epoch 80] Batch loss: 0.5100332498550415\n",
            "[Epoch 80] Batch loss: 0.4627613425254822\n",
            "[Epoch 80] Batch loss: 0.47943776845932007\n",
            "[Epoch 80] Batch loss: 0.4759770333766937\n",
            "[Epoch 80] Batch loss: 0.509274959564209\n",
            "[Epoch 80] Batch loss: 0.4810464382171631\n",
            "[Epoch 80] Batch loss: 0.48457297682762146\n",
            "[Epoch 80] Batch loss: 0.5339744687080383\n",
            "[Epoch 80] Batch loss: 0.5081255435943604\n",
            "[Epoch 80] Batch loss: 0.5868210792541504\n",
            "[Epoch 80] Batch loss: 0.4530777633190155\n",
            "[Epoch 80] Batch loss: 0.4543600380420685\n",
            "[Epoch 80] Batch loss: 0.4683893024921417\n",
            "[Epoch 80] Batch loss: 0.44928666949272156\n",
            "[Epoch 80] Batch loss: 0.44021540880203247\n",
            "[Epoch 80] Batch loss: 0.5285210013389587\n",
            "[Epoch 80] Batch loss: 0.4487401247024536\n",
            "[Epoch 80] Batch loss: 0.512103259563446\n",
            "[Epoch 80] Batch loss: 0.47325974702835083\n",
            "[Epoch 80] Batch loss: 0.5302157402038574\n",
            "[Epoch 80] Batch loss: 0.41460055112838745\n",
            "[Epoch 80] Batch loss: 0.44832855463027954\n",
            "[Epoch 80] Batch loss: 0.5054086446762085\n",
            "[Epoch 80] Batch loss: 0.5476788282394409\n",
            "[Epoch 80] Batch loss: 0.48821932077407837\n",
            "[Epoch 80] Batch loss: 0.43831491470336914\n",
            "[Epoch 80] Batch loss: 0.51407790184021\n",
            "[Epoch 80] Batch loss: 0.4928578734397888\n",
            "[Epoch 80] Batch loss: 0.547895610332489\n",
            "[Epoch 80] Batch loss: 0.5311522483825684\n",
            "[Epoch 80] Batch loss: 0.5385847091674805\n",
            "[Epoch 80] Batch loss: 0.4781686067581177\n",
            "[Epoch 80] Batch loss: 0.5122120976448059\n",
            "[Epoch 80] Batch loss: 0.4089275002479553\n",
            "[Epoch 80] Batch loss: 0.5305700302124023\n",
            "[Epoch 80] Batch loss: 0.4569900333881378\n",
            "[Epoch 80] Batch loss: 0.43758541345596313\n",
            "[Epoch 80] Batch loss: 0.5744335651397705\n",
            "[Epoch 80] Batch loss: 0.5472816228866577\n",
            "[Epoch 80] Batch loss: 0.47166821360588074\n",
            "[Epoch 80] Batch loss: 0.524018406867981\n",
            "[Epoch 80] Batch loss: 0.4853152632713318\n",
            "[Epoch 80] Batch loss: 0.4223395586013794\n",
            "[Epoch 80] Batch loss: 0.46638238430023193\n",
            "[Epoch 80] Batch loss: 0.4599144756793976\n",
            "[Epoch 80] Batch loss: 0.524728000164032\n",
            "[Epoch 80] Batch loss: 0.4870721399784088\n",
            "[Epoch 80] Batch loss: 0.40980684757232666\n",
            "[Epoch 80] Batch loss: 0.48174890875816345\n",
            "[Epoch 80] Batch loss: 0.4810919463634491\n",
            "[Epoch 80] Batch loss: 0.518829345703125\n",
            "[Epoch 80] Batch loss: 0.5130603313446045\n",
            "[Epoch 80] Batch loss: 0.46774598956108093\n",
            "[Epoch 80] Batch loss: 0.5792194604873657\n",
            "[Epoch 80] Batch loss: 0.4879239797592163\n",
            "[Epoch 80] Batch loss: 0.6000627279281616\n",
            "[Epoch 80] Batch loss: 0.4253990948200226\n",
            "[Epoch 80] Batch loss: 0.44720277190208435\n",
            "[Epoch 80] Batch loss: 0.49174964427948\n",
            "[Epoch 80] Batch loss: 0.38941141963005066\n",
            "[Epoch 80] Batch loss: 0.46306630969047546\n",
            "[Epoch 80] Batch loss: 0.4487360119819641\n",
            "[Epoch 80] Batch loss: 0.4697341322898865\n",
            "[Epoch 80] Batch loss: 0.473213255405426\n",
            "[Epoch 80] Batch loss: 0.45313623547554016\n",
            "[Epoch 80] Batch loss: 0.548815131187439\n",
            "[Epoch 80] Batch loss: 0.4737232029438019\n",
            "[Epoch 80] Batch loss: 0.5020745992660522\n",
            "[Epoch 80] Batch loss: 0.4337320923805237\n",
            "[Epoch 80] Batch loss: 0.5417026877403259\n",
            "[Epoch 80] Batch loss: 0.4596318006515503\n",
            "[Epoch 80] Batch loss: 0.43819427490234375\n",
            "[Epoch 80] Batch loss: 0.48730966448783875\n",
            "[Epoch 80] Batch loss: 0.511398434638977\n",
            "[Epoch 80] Batch loss: 0.4335533082485199\n",
            "[Epoch 80] Batch loss: 0.44435954093933105\n",
            "[Epoch 80] Batch loss: 0.5001434683799744\n",
            "[Epoch 80] Batch loss: 0.5672347545623779\n",
            "[Epoch 80] Batch loss: 0.4165405035018921\n",
            "[Epoch 80] Batch loss: 0.4549761414527893\n",
            "[Epoch 80] Batch loss: 0.42097434401512146\n",
            "[Epoch 80] Batch loss: 0.574104905128479\n",
            "[Epoch 80] Batch loss: 0.48649144172668457\n",
            "[Epoch 80] Batch loss: 0.5673877000808716\n",
            "[Epoch 80] Batch loss: 0.47802573442459106\n",
            "[Epoch 80] Batch loss: 0.48239192366600037\n",
            "[Epoch 80] Batch loss: 0.4613358676433563\n",
            "[Epoch 81/100] Val loss: 0.8183\n",
            "Best saved model.\n",
            "[Epoch 81] Batch loss: 0.4933077096939087\n",
            "[Epoch 81] Batch loss: 0.4495695233345032\n",
            "[Epoch 81] Batch loss: 0.5576884150505066\n",
            "[Epoch 81] Batch loss: 0.4974419176578522\n",
            "[Epoch 81] Batch loss: 0.4821629822254181\n",
            "[Epoch 81] Batch loss: 0.49449700117111206\n",
            "[Epoch 81] Batch loss: 0.47117358446121216\n",
            "[Epoch 81] Batch loss: 0.39038991928100586\n",
            "[Epoch 81] Batch loss: 0.4141611158847809\n",
            "[Epoch 81] Batch loss: 0.4652218818664551\n",
            "[Epoch 81] Batch loss: 0.4742470383644104\n",
            "[Epoch 81] Batch loss: 0.481966108083725\n",
            "[Epoch 81] Batch loss: 0.4274113178253174\n",
            "[Epoch 81] Batch loss: 0.44915324449539185\n",
            "[Epoch 81] Batch loss: 0.4753192961215973\n",
            "[Epoch 81] Batch loss: 0.4797361195087433\n",
            "[Epoch 81] Batch loss: 0.4837309718132019\n",
            "[Epoch 81] Batch loss: 0.4135630428791046\n",
            "[Epoch 81] Batch loss: 0.5573201775550842\n",
            "[Epoch 81] Batch loss: 0.5530350208282471\n",
            "[Epoch 81] Batch loss: 0.432835191488266\n",
            "[Epoch 81] Batch loss: 0.4227833151817322\n",
            "[Epoch 81] Batch loss: 0.4082601070404053\n",
            "[Epoch 81] Batch loss: 0.5003361701965332\n",
            "[Epoch 81] Batch loss: 0.3802766501903534\n",
            "[Epoch 81] Batch loss: 0.4493054449558258\n",
            "[Epoch 81] Batch loss: 0.5347365140914917\n",
            "[Epoch 81] Batch loss: 0.38885605335235596\n",
            "[Epoch 81] Batch loss: 0.3749177157878876\n",
            "[Epoch 81] Batch loss: 0.45606333017349243\n",
            "[Epoch 81] Batch loss: 0.5233204364776611\n",
            "[Epoch 81] Batch loss: 0.4413781464099884\n",
            "[Epoch 81] Batch loss: 0.5046476721763611\n",
            "[Epoch 81] Batch loss: 0.5745644569396973\n",
            "[Epoch 81] Batch loss: 0.44583070278167725\n",
            "[Epoch 81] Batch loss: 0.5144246816635132\n",
            "[Epoch 81] Batch loss: 0.4777204394340515\n",
            "[Epoch 81] Batch loss: 0.4869155287742615\n",
            "[Epoch 81] Batch loss: 0.530356764793396\n",
            "[Epoch 81] Batch loss: 0.46903520822525024\n",
            "[Epoch 81] Batch loss: 0.5605381727218628\n",
            "[Epoch 81] Batch loss: 0.5141490697860718\n",
            "[Epoch 81] Batch loss: 0.5140290260314941\n",
            "[Epoch 81] Batch loss: 0.4715374708175659\n",
            "[Epoch 81] Batch loss: 0.45876240730285645\n",
            "[Epoch 81] Batch loss: 0.4617542326450348\n",
            "[Epoch 81] Batch loss: 0.47743961215019226\n",
            "[Epoch 81] Batch loss: 0.44613879919052124\n",
            "[Epoch 81] Batch loss: 0.48697835206985474\n",
            "[Epoch 81] Batch loss: 0.4873873293399811\n",
            "[Epoch 81] Batch loss: 0.45478466153144836\n",
            "[Epoch 81] Batch loss: 0.494516521692276\n",
            "[Epoch 81] Batch loss: 0.4649145007133484\n",
            "[Epoch 81] Batch loss: 0.39764100313186646\n",
            "[Epoch 81] Batch loss: 0.4500730335712433\n",
            "[Epoch 81] Batch loss: 0.5103163719177246\n",
            "[Epoch 81] Batch loss: 0.4567984938621521\n",
            "[Epoch 81] Batch loss: 0.38131001591682434\n",
            "[Epoch 81] Batch loss: 0.5040276050567627\n",
            "[Epoch 81] Batch loss: 0.4156566560268402\n",
            "[Epoch 81] Batch loss: 0.476884126663208\n",
            "[Epoch 81] Batch loss: 0.4869656562805176\n",
            "[Epoch 81] Batch loss: 0.4326092600822449\n",
            "[Epoch 81] Batch loss: 0.506815493106842\n",
            "[Epoch 81] Batch loss: 0.46102169156074524\n",
            "[Epoch 81] Batch loss: 0.4533296525478363\n",
            "[Epoch 81] Batch loss: 0.441693514585495\n",
            "[Epoch 81] Batch loss: 0.4411391019821167\n",
            "[Epoch 81] Batch loss: 0.4471966028213501\n",
            "[Epoch 81] Batch loss: 0.4956631660461426\n",
            "[Epoch 81] Batch loss: 0.5205976963043213\n",
            "[Epoch 81] Batch loss: 0.4219396114349365\n",
            "[Epoch 81] Batch loss: 0.5142427682876587\n",
            "[Epoch 81] Batch loss: 0.40881070494651794\n",
            "[Epoch 81] Batch loss: 0.5461758375167847\n",
            "[Epoch 81] Batch loss: 0.4766920208930969\n",
            "[Epoch 81] Batch loss: 0.4309312403202057\n",
            "[Epoch 81] Batch loss: 0.3555326461791992\n",
            "[Epoch 81] Batch loss: 0.44461768865585327\n",
            "[Epoch 81] Batch loss: 0.5076954364776611\n",
            "[Epoch 81] Batch loss: 0.5649297833442688\n",
            "[Epoch 81] Batch loss: 0.5114394426345825\n",
            "[Epoch 81] Batch loss: 0.46631836891174316\n",
            "[Epoch 81] Batch loss: 0.46322914958000183\n",
            "[Epoch 81] Batch loss: 0.43877673149108887\n",
            "[Epoch 81] Batch loss: 0.4599077105522156\n",
            "[Epoch 81] Batch loss: 0.4922943413257599\n",
            "[Epoch 81] Batch loss: 0.5577706694602966\n",
            "[Epoch 81] Batch loss: 0.4700947403907776\n",
            "[Epoch 81] Batch loss: 0.45442506670951843\n",
            "[Epoch 81] Batch loss: 0.4157911539077759\n",
            "[Epoch 81] Batch loss: 0.5011826753616333\n",
            "[Epoch 81] Batch loss: 0.4870221018791199\n",
            "[Epoch 81] Batch loss: 0.5420240759849548\n",
            "[Epoch 81] Batch loss: 0.5078279376029968\n",
            "[Epoch 81] Batch loss: 0.4959179759025574\n",
            "[Epoch 81] Batch loss: 0.4812402129173279\n",
            "[Epoch 81] Batch loss: 0.43422654271125793\n",
            "[Epoch 81] Batch loss: 0.4921320080757141\n",
            "[Epoch 81] Batch loss: 0.49381473660469055\n",
            "[Epoch 81] Batch loss: 0.5320347547531128\n",
            "[Epoch 81] Batch loss: 0.41955313086509705\n",
            "[Epoch 81] Batch loss: 0.510543942451477\n",
            "[Epoch 81] Batch loss: 0.4906668961048126\n",
            "[Epoch 81] Batch loss: 0.5234779715538025\n",
            "[Epoch 81] Batch loss: 0.46479108929634094\n",
            "[Epoch 81] Batch loss: 0.4979466199874878\n",
            "[Epoch 81] Batch loss: 0.47516435384750366\n",
            "[Epoch 81] Batch loss: 0.5432465076446533\n",
            "[Epoch 81] Batch loss: 0.4557415544986725\n",
            "[Epoch 81] Batch loss: 0.4851606786251068\n",
            "[Epoch 81] Batch loss: 0.422980397939682\n",
            "[Epoch 81] Batch loss: 0.4670536518096924\n",
            "[Epoch 81] Batch loss: 0.4243118166923523\n",
            "[Epoch 81] Batch loss: 0.43497928977012634\n",
            "[Epoch 81] Batch loss: 0.4999101161956787\n",
            "[Epoch 81] Batch loss: 0.4800715148448944\n",
            "[Epoch 81] Batch loss: 0.5575559735298157\n",
            "[Epoch 81] Batch loss: 0.4544717073440552\n",
            "[Epoch 81] Batch loss: 0.43776988983154297\n",
            "[Epoch 81] Batch loss: 0.4689972400665283\n",
            "[Epoch 81] Batch loss: 0.44693589210510254\n",
            "[Epoch 81] Batch loss: 0.4630500376224518\n",
            "[Epoch 81] Batch loss: 0.5333889722824097\n",
            "[Epoch 81] Batch loss: 0.41514667868614197\n",
            "[Epoch 81] Batch loss: 0.5097833275794983\n",
            "[Epoch 82/100] Val loss: 0.8081\n",
            "Best saved model.\n",
            "[Epoch 82] Batch loss: 0.4225033223628998\n",
            "[Epoch 82] Batch loss: 0.4876444339752197\n",
            "[Epoch 82] Batch loss: 0.35163414478302\n",
            "[Epoch 82] Batch loss: 0.446504682302475\n",
            "[Epoch 82] Batch loss: 0.46437785029411316\n",
            "[Epoch 82] Batch loss: 0.5326666235923767\n",
            "[Epoch 82] Batch loss: 0.40762874484062195\n",
            "[Epoch 82] Batch loss: 0.3844199776649475\n",
            "[Epoch 82] Batch loss: 0.4657363295555115\n",
            "[Epoch 82] Batch loss: 0.43585652112960815\n",
            "[Epoch 82] Batch loss: 0.490259051322937\n",
            "[Epoch 82] Batch loss: 0.4834396541118622\n",
            "[Epoch 82] Batch loss: 0.508713960647583\n",
            "[Epoch 82] Batch loss: 0.5029784440994263\n",
            "[Epoch 82] Batch loss: 0.5298454761505127\n",
            "[Epoch 82] Batch loss: 0.5073627233505249\n",
            "[Epoch 82] Batch loss: 0.46268710494041443\n",
            "[Epoch 82] Batch loss: 0.43106895685195923\n",
            "[Epoch 82] Batch loss: 0.3850208818912506\n",
            "[Epoch 82] Batch loss: 0.47004806995391846\n",
            "[Epoch 82] Batch loss: 0.48188552260398865\n",
            "[Epoch 82] Batch loss: 0.5905028581619263\n",
            "[Epoch 82] Batch loss: 0.4561523199081421\n",
            "[Epoch 82] Batch loss: 0.5799185633659363\n",
            "[Epoch 82] Batch loss: 0.5601200461387634\n",
            "[Epoch 82] Batch loss: 0.5104342699050903\n",
            "[Epoch 82] Batch loss: 0.4735547602176666\n",
            "[Epoch 82] Batch loss: 0.48190581798553467\n",
            "[Epoch 82] Batch loss: 0.574393630027771\n",
            "[Epoch 82] Batch loss: 0.5164527893066406\n",
            "[Epoch 82] Batch loss: 0.4078238606452942\n",
            "[Epoch 82] Batch loss: 0.45900216698646545\n",
            "[Epoch 82] Batch loss: 0.4257563054561615\n",
            "[Epoch 82] Batch loss: 0.4638051390647888\n",
            "[Epoch 82] Batch loss: 0.569570779800415\n",
            "[Epoch 82] Batch loss: 0.43680205941200256\n",
            "[Epoch 82] Batch loss: 0.478354275226593\n",
            "[Epoch 82] Batch loss: 0.5361096858978271\n",
            "[Epoch 82] Batch loss: 0.5106654167175293\n",
            "[Epoch 82] Batch loss: 0.47203701734542847\n",
            "[Epoch 82] Batch loss: 0.4105813801288605\n",
            "[Epoch 82] Batch loss: 0.44163182377815247\n",
            "[Epoch 82] Batch loss: 0.4721457362174988\n",
            "[Epoch 82] Batch loss: 0.4051286578178406\n",
            "[Epoch 82] Batch loss: 0.5101240277290344\n",
            "[Epoch 82] Batch loss: 0.4038732051849365\n",
            "[Epoch 82] Batch loss: 0.4548913836479187\n",
            "[Epoch 82] Batch loss: 0.47688475251197815\n",
            "[Epoch 82] Batch loss: 0.4482976794242859\n",
            "[Epoch 82] Batch loss: 0.5138522982597351\n",
            "[Epoch 82] Batch loss: 0.41360270977020264\n",
            "[Epoch 82] Batch loss: 0.4249890148639679\n",
            "[Epoch 82] Batch loss: 0.46915778517723083\n",
            "[Epoch 82] Batch loss: 0.42792072892189026\n",
            "[Epoch 82] Batch loss: 0.4607980251312256\n",
            "[Epoch 82] Batch loss: 0.4015786051750183\n",
            "[Epoch 82] Batch loss: 0.42489203810691833\n",
            "[Epoch 82] Batch loss: 0.4513922929763794\n",
            "[Epoch 82] Batch loss: 0.44982385635375977\n",
            "[Epoch 82] Batch loss: 0.49748408794403076\n",
            "[Epoch 82] Batch loss: 0.46490344405174255\n",
            "[Epoch 82] Batch loss: 0.5183228254318237\n",
            "[Epoch 82] Batch loss: 0.39336544275283813\n",
            "[Epoch 82] Batch loss: 0.46854618191719055\n",
            "[Epoch 82] Batch loss: 0.4523758888244629\n",
            "[Epoch 82] Batch loss: 0.445017546415329\n",
            "[Epoch 82] Batch loss: 0.4849770665168762\n",
            "[Epoch 82] Batch loss: 0.518880307674408\n",
            "[Epoch 82] Batch loss: 0.3740452229976654\n",
            "[Epoch 82] Batch loss: 0.4746841788291931\n",
            "[Epoch 82] Batch loss: 0.46572473645210266\n",
            "[Epoch 82] Batch loss: 0.5312760472297668\n",
            "[Epoch 82] Batch loss: 0.40590721368789673\n",
            "[Epoch 82] Batch loss: 0.45146462321281433\n",
            "[Epoch 82] Batch loss: 0.47089022397994995\n",
            "[Epoch 82] Batch loss: 0.5243951082229614\n",
            "[Epoch 82] Batch loss: 0.485716313123703\n",
            "[Epoch 82] Batch loss: 0.5312355160713196\n",
            "[Epoch 82] Batch loss: 0.49270087480545044\n",
            "[Epoch 82] Batch loss: 0.5333537459373474\n",
            "[Epoch 82] Batch loss: 0.5759068131446838\n",
            "[Epoch 82] Batch loss: 0.4783129394054413\n",
            "[Epoch 82] Batch loss: 0.5269230008125305\n",
            "[Epoch 82] Batch loss: 0.514829158782959\n",
            "[Epoch 82] Batch loss: 0.46848198771476746\n",
            "[Epoch 82] Batch loss: 0.45821601152420044\n",
            "[Epoch 82] Batch loss: 0.44949978590011597\n",
            "[Epoch 82] Batch loss: 0.4300011396408081\n",
            "[Epoch 82] Batch loss: 0.42059868574142456\n",
            "[Epoch 82] Batch loss: 0.5934033989906311\n",
            "[Epoch 82] Batch loss: 0.43108296394348145\n",
            "[Epoch 82] Batch loss: 0.5136398673057556\n",
            "[Epoch 82] Batch loss: 0.5937698483467102\n",
            "[Epoch 82] Batch loss: 0.4626897871494293\n",
            "[Epoch 82] Batch loss: 0.3959558606147766\n",
            "[Epoch 82] Batch loss: 0.5094087719917297\n",
            "[Epoch 82] Batch loss: 0.5996497869491577\n",
            "[Epoch 82] Batch loss: 0.4285222589969635\n",
            "[Epoch 82] Batch loss: 0.4880686104297638\n",
            "[Epoch 82] Batch loss: 0.5022165775299072\n",
            "[Epoch 82] Batch loss: 0.40684953331947327\n",
            "[Epoch 82] Batch loss: 0.5375573039054871\n",
            "[Epoch 82] Batch loss: 0.38432568311691284\n",
            "[Epoch 82] Batch loss: 0.4721088111400604\n",
            "[Epoch 82] Batch loss: 0.46011197566986084\n",
            "[Epoch 82] Batch loss: 0.44985273480415344\n",
            "[Epoch 82] Batch loss: 0.4288898706436157\n",
            "[Epoch 82] Batch loss: 0.4750158488750458\n",
            "[Epoch 82] Batch loss: 0.41518861055374146\n",
            "[Epoch 82] Batch loss: 0.4701566696166992\n",
            "[Epoch 82] Batch loss: 0.42812448740005493\n",
            "[Epoch 82] Batch loss: 0.6093611717224121\n",
            "[Epoch 82] Batch loss: 0.41372552514076233\n",
            "[Epoch 82] Batch loss: 0.4487115740776062\n",
            "[Epoch 82] Batch loss: 0.47387948632240295\n",
            "[Epoch 82] Batch loss: 0.40832626819610596\n",
            "[Epoch 82] Batch loss: 0.47724637389183044\n",
            "[Epoch 82] Batch loss: 0.49201706051826477\n",
            "[Epoch 82] Batch loss: 0.4262271225452423\n",
            "[Epoch 82] Batch loss: 0.4648388624191284\n",
            "[Epoch 82] Batch loss: 0.46832984685897827\n",
            "[Epoch 82] Batch loss: 0.46532484889030457\n",
            "[Epoch 82] Batch loss: 0.5111777186393738\n",
            "[Epoch 82] Batch loss: 0.40316638350486755\n",
            "[Epoch 82] Batch loss: 0.38293367624282837\n",
            "[Epoch 82] Batch loss: 0.7316341400146484\n",
            "[Epoch 83/100] Val loss: 0.8042\n",
            "Best saved model.\n",
            "[Epoch 83] Batch loss: 0.5698043704032898\n",
            "[Epoch 83] Batch loss: 0.4440179467201233\n",
            "[Epoch 83] Batch loss: 0.40942078828811646\n",
            "[Epoch 83] Batch loss: 0.43744131922721863\n",
            "[Epoch 83] Batch loss: 0.4790659248828888\n",
            "[Epoch 83] Batch loss: 0.4563445448875427\n",
            "[Epoch 83] Batch loss: 0.3984587788581848\n",
            "[Epoch 83] Batch loss: 0.5475021004676819\n",
            "[Epoch 83] Batch loss: 0.49869659543037415\n",
            "[Epoch 83] Batch loss: 0.4668893814086914\n",
            "[Epoch 83] Batch loss: 0.48187506198883057\n",
            "[Epoch 83] Batch loss: 0.4905948042869568\n",
            "[Epoch 83] Batch loss: 0.5027911067008972\n",
            "[Epoch 83] Batch loss: 0.5331103801727295\n",
            "[Epoch 83] Batch loss: 0.4145863354206085\n",
            "[Epoch 83] Batch loss: 0.612091600894928\n",
            "[Epoch 83] Batch loss: 0.47805148363113403\n",
            "[Epoch 83] Batch loss: 0.486709862947464\n",
            "[Epoch 83] Batch loss: 0.4417954087257385\n",
            "[Epoch 83] Batch loss: 0.45240750908851624\n",
            "[Epoch 83] Batch loss: 0.40959978103637695\n",
            "[Epoch 83] Batch loss: 0.4458058476448059\n",
            "[Epoch 83] Batch loss: 0.462536484003067\n",
            "[Epoch 83] Batch loss: 0.5326911211013794\n",
            "[Epoch 83] Batch loss: 0.44496238231658936\n",
            "[Epoch 83] Batch loss: 0.43016597628593445\n",
            "[Epoch 83] Batch loss: 0.4583148658275604\n",
            "[Epoch 83] Batch loss: 0.548622727394104\n",
            "[Epoch 83] Batch loss: 0.48607560992240906\n",
            "[Epoch 83] Batch loss: 0.5382627844810486\n",
            "[Epoch 83] Batch loss: 0.43323814868927\n",
            "[Epoch 83] Batch loss: 0.546172559261322\n",
            "[Epoch 83] Batch loss: 0.46982336044311523\n",
            "[Epoch 83] Batch loss: 0.4264845848083496\n",
            "[Epoch 83] Batch loss: 0.4690915048122406\n",
            "[Epoch 83] Batch loss: 0.42989078164100647\n",
            "[Epoch 83] Batch loss: 0.5402296185493469\n",
            "[Epoch 83] Batch loss: 0.4215470552444458\n",
            "[Epoch 83] Batch loss: 0.38222095370292664\n",
            "[Epoch 83] Batch loss: 0.42336520552635193\n",
            "[Epoch 83] Batch loss: 0.4688586890697479\n",
            "[Epoch 83] Batch loss: 0.5218420028686523\n",
            "[Epoch 83] Batch loss: 0.4892406463623047\n",
            "[Epoch 83] Batch loss: 0.44694530963897705\n",
            "[Epoch 83] Batch loss: 0.4592548906803131\n",
            "[Epoch 83] Batch loss: 0.4132448732852936\n",
            "[Epoch 83] Batch loss: 0.4639008045196533\n",
            "[Epoch 83] Batch loss: 0.47854697704315186\n",
            "[Epoch 83] Batch loss: 0.49378401041030884\n",
            "[Epoch 83] Batch loss: 0.5012996196746826\n",
            "[Epoch 83] Batch loss: 0.5200498700141907\n",
            "[Epoch 83] Batch loss: 0.46275845170021057\n",
            "[Epoch 83] Batch loss: 0.42746254801750183\n",
            "[Epoch 83] Batch loss: 0.6221075654029846\n",
            "[Epoch 83] Batch loss: 0.4487510919570923\n",
            "[Epoch 83] Batch loss: 0.4492413103580475\n",
            "[Epoch 83] Batch loss: 0.3230598568916321\n",
            "[Epoch 83] Batch loss: 0.5007861852645874\n",
            "[Epoch 83] Batch loss: 0.486579567193985\n",
            "[Epoch 83] Batch loss: 0.4927166700363159\n",
            "[Epoch 83] Batch loss: 0.512839674949646\n",
            "[Epoch 83] Batch loss: 0.6066384315490723\n",
            "[Epoch 83] Batch loss: 0.44234180450439453\n",
            "[Epoch 83] Batch loss: 0.6123539805412292\n",
            "[Epoch 83] Batch loss: 0.40399691462516785\n",
            "[Epoch 83] Batch loss: 0.4678058326244354\n",
            "[Epoch 83] Batch loss: 0.5568364262580872\n",
            "[Epoch 83] Batch loss: 0.460971474647522\n",
            "[Epoch 83] Batch loss: 0.423018217086792\n",
            "[Epoch 83] Batch loss: 0.4587879776954651\n",
            "[Epoch 83] Batch loss: 0.4103701710700989\n",
            "[Epoch 83] Batch loss: 0.4543917179107666\n",
            "[Epoch 83] Batch loss: 0.5626311898231506\n",
            "[Epoch 83] Batch loss: 0.4621613621711731\n",
            "[Epoch 83] Batch loss: 0.4371497333049774\n",
            "[Epoch 83] Batch loss: 0.5050671100616455\n",
            "[Epoch 83] Batch loss: 0.485329270362854\n",
            "[Epoch 83] Batch loss: 0.45888692140579224\n",
            "[Epoch 83] Batch loss: 0.5209367871284485\n",
            "[Epoch 83] Batch loss: 0.4952108561992645\n",
            "[Epoch 83] Batch loss: 0.4139791429042816\n",
            "[Epoch 83] Batch loss: 0.47976166009902954\n",
            "[Epoch 83] Batch loss: 0.4605342149734497\n",
            "[Epoch 83] Batch loss: 0.45344915986061096\n",
            "[Epoch 83] Batch loss: 0.4902956187725067\n",
            "[Epoch 83] Batch loss: 0.4007398784160614\n",
            "[Epoch 83] Batch loss: 0.49367383122444153\n",
            "[Epoch 83] Batch loss: 0.4094081521034241\n",
            "[Epoch 83] Batch loss: 0.5852516889572144\n",
            "[Epoch 83] Batch loss: 0.46992620825767517\n",
            "[Epoch 83] Batch loss: 0.4401325583457947\n",
            "[Epoch 83] Batch loss: 0.4701564311981201\n",
            "[Epoch 83] Batch loss: 0.45424389839172363\n",
            "[Epoch 83] Batch loss: 0.47259050607681274\n",
            "[Epoch 83] Batch loss: 0.45968320965766907\n",
            "[Epoch 83] Batch loss: 0.4826904535293579\n",
            "[Epoch 83] Batch loss: 0.4309447109699249\n",
            "[Epoch 83] Batch loss: 0.5391603708267212\n",
            "[Epoch 83] Batch loss: 0.5430037379264832\n",
            "[Epoch 83] Batch loss: 0.49294906854629517\n",
            "[Epoch 83] Batch loss: 0.44321244955062866\n",
            "[Epoch 83] Batch loss: 0.49335598945617676\n",
            "[Epoch 83] Batch loss: 0.43830427527427673\n",
            "[Epoch 83] Batch loss: 0.4489273428916931\n",
            "[Epoch 83] Batch loss: 0.4574330449104309\n",
            "[Epoch 83] Batch loss: 0.4580674469470978\n",
            "[Epoch 83] Batch loss: 0.4807646870613098\n",
            "[Epoch 83] Batch loss: 0.4238881468772888\n",
            "[Epoch 83] Batch loss: 0.44160956144332886\n",
            "[Epoch 83] Batch loss: 0.41840481758117676\n",
            "[Epoch 83] Batch loss: 0.39427849650382996\n",
            "[Epoch 83] Batch loss: 0.47740665078163147\n",
            "[Epoch 83] Batch loss: 0.3975110948085785\n",
            "[Epoch 83] Batch loss: 0.496620237827301\n",
            "[Epoch 83] Batch loss: 0.5095399618148804\n",
            "[Epoch 83] Batch loss: 0.38036176562309265\n",
            "[Epoch 83] Batch loss: 0.41145411133766174\n",
            "[Epoch 83] Batch loss: 0.5649702548980713\n",
            "[Epoch 83] Batch loss: 0.45427119731903076\n",
            "[Epoch 83] Batch loss: 0.45937061309814453\n",
            "[Epoch 83] Batch loss: 0.45321252942085266\n",
            "[Epoch 83] Batch loss: 0.4548743665218353\n",
            "[Epoch 83] Batch loss: 0.40046417713165283\n",
            "[Epoch 83] Batch loss: 0.3866943418979645\n",
            "[Epoch 83] Batch loss: 0.48552656173706055\n",
            "[Epoch 83] Batch loss: 0.4833718538284302\n",
            "[Epoch 84/100] Val loss: 0.7972\n",
            "Best saved model.\n",
            "[Epoch 84] Batch loss: 0.44648176431655884\n",
            "[Epoch 84] Batch loss: 0.4293314814567566\n",
            "[Epoch 84] Batch loss: 0.5672683119773865\n",
            "[Epoch 84] Batch loss: 0.46087414026260376\n",
            "[Epoch 84] Batch loss: 0.41441312432289124\n",
            "[Epoch 84] Batch loss: 0.4109709560871124\n",
            "[Epoch 84] Batch loss: 0.48474517464637756\n",
            "[Epoch 84] Batch loss: 0.43688157200813293\n",
            "[Epoch 84] Batch loss: 0.5169889330863953\n",
            "[Epoch 84] Batch loss: 0.45589715242385864\n",
            "[Epoch 84] Batch loss: 0.4152678847312927\n",
            "[Epoch 84] Batch loss: 0.48510560393333435\n",
            "[Epoch 84] Batch loss: 0.591962993144989\n",
            "[Epoch 84] Batch loss: 0.41094696521759033\n",
            "[Epoch 84] Batch loss: 0.42156925797462463\n",
            "[Epoch 84] Batch loss: 0.5163203477859497\n",
            "[Epoch 84] Batch loss: 0.42556512355804443\n",
            "[Epoch 84] Batch loss: 0.531074583530426\n",
            "[Epoch 84] Batch loss: 0.5190314650535583\n",
            "[Epoch 84] Batch loss: 0.4569664001464844\n",
            "[Epoch 84] Batch loss: 0.40623509883880615\n",
            "[Epoch 84] Batch loss: 0.4142919182777405\n",
            "[Epoch 84] Batch loss: 0.42108508944511414\n",
            "[Epoch 84] Batch loss: 0.49470067024230957\n",
            "[Epoch 84] Batch loss: 0.5191845297813416\n",
            "[Epoch 84] Batch loss: 0.3802362382411957\n",
            "[Epoch 84] Batch loss: 0.41363832354545593\n",
            "[Epoch 84] Batch loss: 0.501888632774353\n",
            "[Epoch 84] Batch loss: 0.5212408304214478\n",
            "[Epoch 84] Batch loss: 0.4156876504421234\n",
            "[Epoch 84] Batch loss: 0.42667925357818604\n",
            "[Epoch 84] Batch loss: 0.4328269362449646\n",
            "[Epoch 84] Batch loss: 0.47605812549591064\n",
            "[Epoch 84] Batch loss: 0.49831756949424744\n",
            "[Epoch 84] Batch loss: 0.489408403635025\n",
            "[Epoch 84] Batch loss: 0.506680428981781\n",
            "[Epoch 84] Batch loss: 0.44431936740875244\n",
            "[Epoch 84] Batch loss: 0.43710562586784363\n",
            "[Epoch 84] Batch loss: 0.39054644107818604\n",
            "[Epoch 84] Batch loss: 0.4852200746536255\n",
            "[Epoch 84] Batch loss: 0.40966343879699707\n",
            "[Epoch 84] Batch loss: 0.4321529269218445\n",
            "[Epoch 84] Batch loss: 0.4723263680934906\n",
            "[Epoch 84] Batch loss: 0.45985856652259827\n",
            "[Epoch 84] Batch loss: 0.46621280908584595\n",
            "[Epoch 84] Batch loss: 0.5744392275810242\n",
            "[Epoch 84] Batch loss: 0.4309653043746948\n",
            "[Epoch 84] Batch loss: 0.4461962878704071\n",
            "[Epoch 84] Batch loss: 0.43282175064086914\n",
            "[Epoch 84] Batch loss: 0.46673116087913513\n",
            "[Epoch 84] Batch loss: 0.45506688952445984\n",
            "[Epoch 84] Batch loss: 0.38983920216560364\n",
            "[Epoch 84] Batch loss: 0.5498478412628174\n",
            "[Epoch 84] Batch loss: 0.46739810705184937\n",
            "[Epoch 84] Batch loss: 0.3881061375141144\n",
            "[Epoch 84] Batch loss: 0.4998186230659485\n",
            "[Epoch 84] Batch loss: 0.506705105304718\n",
            "[Epoch 84] Batch loss: 0.5137778520584106\n",
            "[Epoch 84] Batch loss: 0.5073170065879822\n",
            "[Epoch 84] Batch loss: 0.4552893340587616\n",
            "[Epoch 84] Batch loss: 0.4636595845222473\n",
            "[Epoch 84] Batch loss: 0.5145881772041321\n",
            "[Epoch 84] Batch loss: 0.44972535967826843\n",
            "[Epoch 84] Batch loss: 0.5098966360092163\n",
            "[Epoch 84] Batch loss: 0.5311567783355713\n",
            "[Epoch 84] Batch loss: 0.46664246916770935\n",
            "[Epoch 84] Batch loss: 0.36887821555137634\n",
            "[Epoch 84] Batch loss: 0.4579804539680481\n",
            "[Epoch 84] Batch loss: 0.4418123960494995\n",
            "[Epoch 84] Batch loss: 0.4302784204483032\n",
            "[Epoch 84] Batch loss: 0.38392165303230286\n",
            "[Epoch 84] Batch loss: 0.4027065932750702\n",
            "[Epoch 84] Batch loss: 0.42522376775741577\n",
            "[Epoch 84] Batch loss: 0.5338220000267029\n",
            "[Epoch 84] Batch loss: 0.4380791187286377\n",
            "[Epoch 84] Batch loss: 0.4118528366088867\n",
            "[Epoch 84] Batch loss: 0.43306711316108704\n",
            "[Epoch 84] Batch loss: 0.48460653424263\n",
            "[Epoch 84] Batch loss: 0.5020309686660767\n",
            "[Epoch 84] Batch loss: 0.5016732215881348\n",
            "[Epoch 84] Batch loss: 0.45148929953575134\n",
            "[Epoch 84] Batch loss: 0.513224720954895\n",
            "[Epoch 84] Batch loss: 0.4603770077228546\n",
            "[Epoch 84] Batch loss: 0.5736014246940613\n",
            "[Epoch 84] Batch loss: 0.4243725538253784\n",
            "[Epoch 84] Batch loss: 0.45596885681152344\n",
            "[Epoch 84] Batch loss: 0.48522135615348816\n",
            "[Epoch 84] Batch loss: 0.4721454679965973\n",
            "[Epoch 84] Batch loss: 0.4987388849258423\n",
            "[Epoch 84] Batch loss: 0.5084151029586792\n",
            "[Epoch 84] Batch loss: 0.4524023234844208\n",
            "[Epoch 84] Batch loss: 0.5257859230041504\n",
            "[Epoch 84] Batch loss: 0.5038138628005981\n",
            "[Epoch 84] Batch loss: 0.4017190933227539\n",
            "[Epoch 84] Batch loss: 0.4031703472137451\n",
            "[Epoch 84] Batch loss: 0.41751739382743835\n",
            "[Epoch 84] Batch loss: 0.47710615396499634\n",
            "[Epoch 84] Batch loss: 0.4484211802482605\n",
            "[Epoch 84] Batch loss: 0.44336965680122375\n",
            "[Epoch 84] Batch loss: 0.4867441654205322\n",
            "[Epoch 84] Batch loss: 0.416580468416214\n",
            "[Epoch 84] Batch loss: 0.4769955277442932\n",
            "[Epoch 84] Batch loss: 0.3887580335140228\n",
            "[Epoch 84] Batch loss: 0.48651280999183655\n",
            "[Epoch 84] Batch loss: 0.43417662382125854\n",
            "[Epoch 84] Batch loss: 0.435472309589386\n",
            "[Epoch 84] Batch loss: 0.4859921932220459\n",
            "[Epoch 84] Batch loss: 0.504647433757782\n",
            "[Epoch 84] Batch loss: 0.4815320670604706\n",
            "[Epoch 84] Batch loss: 0.5241204500198364\n",
            "[Epoch 84] Batch loss: 0.5913403034210205\n",
            "[Epoch 84] Batch loss: 0.4004853367805481\n",
            "[Epoch 84] Batch loss: 0.4737509489059448\n",
            "[Epoch 84] Batch loss: 0.4154258370399475\n",
            "[Epoch 84] Batch loss: 0.4411686360836029\n",
            "[Epoch 84] Batch loss: 0.5915656685829163\n",
            "[Epoch 84] Batch loss: 0.5177832245826721\n",
            "[Epoch 84] Batch loss: 0.4593331515789032\n",
            "[Epoch 84] Batch loss: 0.4423355758190155\n",
            "[Epoch 84] Batch loss: 0.3898561894893646\n",
            "[Epoch 84] Batch loss: 0.5467699766159058\n",
            "[Epoch 84] Batch loss: 0.38620302081108093\n",
            "[Epoch 84] Batch loss: 0.4389079809188843\n",
            "[Epoch 84] Batch loss: 0.5670824646949768\n",
            "[Epoch 84] Batch loss: 0.46314769983291626\n",
            "[Epoch 84] Batch loss: 0.6157581210136414\n",
            "[Epoch 85/100] Val loss: 0.7803\n",
            "Best saved model.\n",
            "[Epoch 85] Batch loss: 0.5197330713272095\n",
            "[Epoch 85] Batch loss: 0.5210404396057129\n",
            "[Epoch 85] Batch loss: 0.4618016183376312\n",
            "[Epoch 85] Batch loss: 0.49810078740119934\n",
            "[Epoch 85] Batch loss: 0.4237050414085388\n",
            "[Epoch 85] Batch loss: 0.46743571758270264\n",
            "[Epoch 85] Batch loss: 0.4857100546360016\n",
            "[Epoch 85] Batch loss: 0.4616270959377289\n",
            "[Epoch 85] Batch loss: 0.41279712319374084\n",
            "[Epoch 85] Batch loss: 0.5239356160163879\n",
            "[Epoch 85] Batch loss: 0.44594067335128784\n",
            "[Epoch 85] Batch loss: 0.4607517719268799\n",
            "[Epoch 85] Batch loss: 0.4114670157432556\n",
            "[Epoch 85] Batch loss: 0.4149775207042694\n",
            "[Epoch 85] Batch loss: 0.42388850450515747\n",
            "[Epoch 85] Batch loss: 0.45681658387184143\n",
            "[Epoch 85] Batch loss: 0.3841959238052368\n",
            "[Epoch 85] Batch loss: 0.4314672648906708\n",
            "[Epoch 85] Batch loss: 0.40631139278411865\n",
            "[Epoch 85] Batch loss: 0.47530290484428406\n",
            "[Epoch 85] Batch loss: 0.5189684629440308\n",
            "[Epoch 85] Batch loss: 0.49834463000297546\n",
            "[Epoch 85] Batch loss: 0.4037175178527832\n",
            "[Epoch 85] Batch loss: 0.5869273543357849\n",
            "[Epoch 85] Batch loss: 0.41168710589408875\n",
            "[Epoch 85] Batch loss: 0.6042056679725647\n",
            "[Epoch 85] Batch loss: 0.41056859493255615\n",
            "[Epoch 85] Batch loss: 0.41384357213974\n",
            "[Epoch 85] Batch loss: 0.39561769366264343\n",
            "[Epoch 85] Batch loss: 0.45329245924949646\n",
            "[Epoch 85] Batch loss: 0.39883047342300415\n",
            "[Epoch 85] Batch loss: 0.4975178837776184\n",
            "[Epoch 85] Batch loss: 0.45721399784088135\n",
            "[Epoch 85] Batch loss: 0.39061835408210754\n",
            "[Epoch 85] Batch loss: 0.3839868903160095\n",
            "[Epoch 85] Batch loss: 0.413292795419693\n",
            "[Epoch 85] Batch loss: 0.4866161048412323\n",
            "[Epoch 85] Batch loss: 0.5500580072402954\n",
            "[Epoch 85] Batch loss: 0.44262561202049255\n",
            "[Epoch 85] Batch loss: 0.4043480157852173\n",
            "[Epoch 85] Batch loss: 0.4618400037288666\n",
            "[Epoch 85] Batch loss: 0.3551527261734009\n",
            "[Epoch 85] Batch loss: 0.41483426094055176\n",
            "[Epoch 85] Batch loss: 0.36222872138023376\n",
            "[Epoch 85] Batch loss: 0.5836318731307983\n",
            "[Epoch 85] Batch loss: 0.4543223977088928\n",
            "[Epoch 85] Batch loss: 0.4603458046913147\n",
            "[Epoch 85] Batch loss: 0.43934378027915955\n",
            "[Epoch 85] Batch loss: 0.5036963820457458\n",
            "[Epoch 85] Batch loss: 0.37214529514312744\n",
            "[Epoch 85] Batch loss: 0.5168658494949341\n",
            "[Epoch 85] Batch loss: 0.377825528383255\n",
            "[Epoch 85] Batch loss: 0.42900440096855164\n",
            "[Epoch 85] Batch loss: 0.44554805755615234\n",
            "[Epoch 85] Batch loss: 0.5072517991065979\n",
            "[Epoch 85] Batch loss: 0.4736933708190918\n",
            "[Epoch 85] Batch loss: 0.4790116548538208\n",
            "[Epoch 85] Batch loss: 0.45169535279273987\n",
            "[Epoch 85] Batch loss: 0.4730091392993927\n",
            "[Epoch 85] Batch loss: 0.5158752202987671\n",
            "[Epoch 85] Batch loss: 0.4972209632396698\n",
            "[Epoch 85] Batch loss: 0.46174466609954834\n",
            "[Epoch 85] Batch loss: 0.47161027789115906\n",
            "[Epoch 85] Batch loss: 0.3936677575111389\n",
            "[Epoch 85] Batch loss: 0.5534905195236206\n",
            "[Epoch 85] Batch loss: 0.48020315170288086\n",
            "[Epoch 85] Batch loss: 0.40699082612991333\n",
            "[Epoch 85] Batch loss: 0.44724467396736145\n",
            "[Epoch 85] Batch loss: 0.5013488531112671\n",
            "[Epoch 85] Batch loss: 0.5414571166038513\n",
            "[Epoch 85] Batch loss: 0.4696636199951172\n",
            "[Epoch 85] Batch loss: 0.4271154999732971\n",
            "[Epoch 85] Batch loss: 0.6301239132881165\n",
            "[Epoch 85] Batch loss: 0.5895872712135315\n",
            "[Epoch 85] Batch loss: 0.4869106113910675\n",
            "[Epoch 85] Batch loss: 0.4311632513999939\n",
            "[Epoch 85] Batch loss: 0.4463758170604706\n",
            "[Epoch 85] Batch loss: 0.4034843444824219\n",
            "[Epoch 85] Batch loss: 0.5465859770774841\n",
            "[Epoch 85] Batch loss: 0.39218202233314514\n",
            "[Epoch 85] Batch loss: 0.4479866325855255\n",
            "[Epoch 85] Batch loss: 0.4200425148010254\n",
            "[Epoch 85] Batch loss: 0.5018370747566223\n",
            "[Epoch 85] Batch loss: 0.47451692819595337\n",
            "[Epoch 85] Batch loss: 0.4381009638309479\n",
            "[Epoch 85] Batch loss: 0.394185334444046\n",
            "[Epoch 85] Batch loss: 0.49338194727897644\n",
            "[Epoch 85] Batch loss: 0.5778810381889343\n",
            "[Epoch 85] Batch loss: 0.4243258535861969\n",
            "[Epoch 85] Batch loss: 0.5410279035568237\n",
            "[Epoch 85] Batch loss: 0.4884583652019501\n",
            "[Epoch 85] Batch loss: 0.43870463967323303\n",
            "[Epoch 85] Batch loss: 0.5299808979034424\n",
            "[Epoch 85] Batch loss: 0.40384066104888916\n",
            "[Epoch 85] Batch loss: 0.5055725574493408\n",
            "[Epoch 85] Batch loss: 0.39272958040237427\n",
            "[Epoch 85] Batch loss: 0.5380331873893738\n",
            "[Epoch 85] Batch loss: 0.49593716859817505\n",
            "[Epoch 85] Batch loss: 0.5353526473045349\n",
            "[Epoch 85] Batch loss: 0.5208419561386108\n",
            "[Epoch 85] Batch loss: 0.39887529611587524\n",
            "[Epoch 85] Batch loss: 0.4100588262081146\n",
            "[Epoch 85] Batch loss: 0.429752379655838\n",
            "[Epoch 85] Batch loss: 0.4039646089076996\n",
            "[Epoch 85] Batch loss: 0.4433569312095642\n",
            "[Epoch 85] Batch loss: 0.48946577310562134\n",
            "[Epoch 85] Batch loss: 0.43923816084861755\n",
            "[Epoch 85] Batch loss: 0.39549732208251953\n",
            "[Epoch 85] Batch loss: 0.4944775104522705\n",
            "[Epoch 85] Batch loss: 0.43063583970069885\n",
            "[Epoch 85] Batch loss: 0.44269824028015137\n",
            "[Epoch 85] Batch loss: 0.46805140376091003\n",
            "[Epoch 85] Batch loss: 0.4232514500617981\n",
            "[Epoch 85] Batch loss: 0.5114243626594543\n",
            "[Epoch 85] Batch loss: 0.49434685707092285\n",
            "[Epoch 85] Batch loss: 0.4189628064632416\n",
            "[Epoch 85] Batch loss: 0.4332713782787323\n",
            "[Epoch 85] Batch loss: 0.6076183319091797\n",
            "[Epoch 85] Batch loss: 0.5292856693267822\n",
            "[Epoch 85] Batch loss: 0.4414702355861664\n",
            "[Epoch 85] Batch loss: 0.455131858587265\n",
            "[Epoch 85] Batch loss: 0.5691295862197876\n",
            "[Epoch 85] Batch loss: 0.46116286516189575\n",
            "[Epoch 85] Batch loss: 0.4331248998641968\n",
            "[Epoch 85] Batch loss: 0.4139629602432251\n",
            "[Epoch 85] Batch loss: 0.3334989547729492\n",
            "[Epoch 86/100] Val loss: 0.7841\n",
            "Epochs without improvement: 1/20\n",
            "[Epoch 86] Batch loss: 0.44104135036468506\n",
            "[Epoch 86] Batch loss: 0.5455117225646973\n",
            "[Epoch 86] Batch loss: 0.44500458240509033\n",
            "[Epoch 86] Batch loss: 0.3769281208515167\n",
            "[Epoch 86] Batch loss: 0.3825279474258423\n",
            "[Epoch 86] Batch loss: 0.5750581622123718\n",
            "[Epoch 86] Batch loss: 0.4753258228302002\n",
            "[Epoch 86] Batch loss: 0.545822262763977\n",
            "[Epoch 86] Batch loss: 0.43495407700538635\n",
            "[Epoch 86] Batch loss: 0.4259481728076935\n",
            "[Epoch 86] Batch loss: 0.3825438618659973\n",
            "[Epoch 86] Batch loss: 0.5396330952644348\n",
            "[Epoch 86] Batch loss: 0.48788443207740784\n",
            "[Epoch 86] Batch loss: 0.43153566122055054\n",
            "[Epoch 86] Batch loss: 0.4669002592563629\n",
            "[Epoch 86] Batch loss: 0.4215281009674072\n",
            "[Epoch 86] Batch loss: 0.4287344515323639\n",
            "[Epoch 86] Batch loss: 0.4486488401889801\n",
            "[Epoch 86] Batch loss: 0.49539703130722046\n",
            "[Epoch 86] Batch loss: 0.6286988258361816\n",
            "[Epoch 86] Batch loss: 0.46130967140197754\n",
            "[Epoch 86] Batch loss: 0.4550476372241974\n",
            "[Epoch 86] Batch loss: 0.39502614736557007\n",
            "[Epoch 86] Batch loss: 0.40077435970306396\n",
            "[Epoch 86] Batch loss: 0.40299493074417114\n",
            "[Epoch 86] Batch loss: 0.570598304271698\n",
            "[Epoch 86] Batch loss: 0.40734800696372986\n",
            "[Epoch 86] Batch loss: 0.5570484399795532\n",
            "[Epoch 86] Batch loss: 0.4842034876346588\n",
            "[Epoch 86] Batch loss: 0.46242278814315796\n",
            "[Epoch 86] Batch loss: 0.3867076635360718\n",
            "[Epoch 86] Batch loss: 0.3931134045124054\n",
            "[Epoch 86] Batch loss: 0.40847110748291016\n",
            "[Epoch 86] Batch loss: 0.5228409171104431\n",
            "[Epoch 86] Batch loss: 0.46942058205604553\n",
            "[Epoch 86] Batch loss: 0.4881846308708191\n",
            "[Epoch 86] Batch loss: 0.512047290802002\n",
            "[Epoch 86] Batch loss: 0.4253641366958618\n",
            "[Epoch 86] Batch loss: 0.43341922760009766\n",
            "[Epoch 86] Batch loss: 0.4387359619140625\n",
            "[Epoch 86] Batch loss: 0.5364543795585632\n",
            "[Epoch 86] Batch loss: 0.48214805126190186\n",
            "[Epoch 86] Batch loss: 0.43254217505455017\n",
            "[Epoch 86] Batch loss: 0.41383522748947144\n",
            "[Epoch 86] Batch loss: 0.38708341121673584\n",
            "[Epoch 86] Batch loss: 0.4500371217727661\n",
            "[Epoch 86] Batch loss: 0.5052262544631958\n",
            "[Epoch 86] Batch loss: 0.3608275055885315\n",
            "[Epoch 86] Batch loss: 0.4255654811859131\n",
            "[Epoch 86] Batch loss: 0.5515869855880737\n",
            "[Epoch 86] Batch loss: 0.5085214972496033\n",
            "[Epoch 86] Batch loss: 0.4272046387195587\n",
            "[Epoch 86] Batch loss: 0.4880139231681824\n",
            "[Epoch 86] Batch loss: 0.509206235408783\n",
            "[Epoch 86] Batch loss: 0.42498090863227844\n",
            "[Epoch 86] Batch loss: 0.4708057940006256\n",
            "[Epoch 86] Batch loss: 0.42228928208351135\n",
            "[Epoch 86] Batch loss: 0.5099707245826721\n",
            "[Epoch 86] Batch loss: 0.4274504482746124\n",
            "[Epoch 86] Batch loss: 0.44882914423942566\n",
            "[Epoch 86] Batch loss: 0.46937188506126404\n",
            "[Epoch 86] Batch loss: 0.460362046957016\n",
            "[Epoch 86] Batch loss: 0.3919578790664673\n",
            "[Epoch 86] Batch loss: 0.48804229497909546\n",
            "[Epoch 86] Batch loss: 0.38230472803115845\n",
            "[Epoch 86] Batch loss: 0.40495947003364563\n",
            "[Epoch 86] Batch loss: 0.37523528933525085\n",
            "[Epoch 86] Batch loss: 0.41513556241989136\n",
            "[Epoch 86] Batch loss: 0.389813095331192\n",
            "[Epoch 86] Batch loss: 0.4125295877456665\n",
            "[Epoch 86] Batch loss: 0.49478745460510254\n",
            "[Epoch 86] Batch loss: 0.4008682370185852\n",
            "[Epoch 86] Batch loss: 0.526978075504303\n",
            "[Epoch 86] Batch loss: 0.46697548031806946\n",
            "[Epoch 86] Batch loss: 0.47709107398986816\n",
            "[Epoch 86] Batch loss: 0.42142826318740845\n",
            "[Epoch 86] Batch loss: 0.42229199409484863\n",
            "[Epoch 86] Batch loss: 0.40237507224082947\n",
            "[Epoch 86] Batch loss: 0.481393426656723\n",
            "[Epoch 86] Batch loss: 0.5133237242698669\n",
            "[Epoch 86] Batch loss: 0.38425081968307495\n",
            "[Epoch 86] Batch loss: 0.527876615524292\n",
            "[Epoch 86] Batch loss: 0.41844868659973145\n",
            "[Epoch 86] Batch loss: 0.34698230028152466\n",
            "[Epoch 86] Batch loss: 0.468111127614975\n",
            "[Epoch 86] Batch loss: 0.4793759286403656\n",
            "[Epoch 86] Batch loss: 0.40421807765960693\n",
            "[Epoch 86] Batch loss: 0.41241317987442017\n",
            "[Epoch 86] Batch loss: 0.4550517797470093\n",
            "[Epoch 86] Batch loss: 0.4392438530921936\n",
            "[Epoch 86] Batch loss: 0.45860081911087036\n",
            "[Epoch 86] Batch loss: 0.4092285931110382\n",
            "[Epoch 86] Batch loss: 0.5015580654144287\n",
            "[Epoch 86] Batch loss: 0.5408833622932434\n",
            "[Epoch 86] Batch loss: 0.5228775143623352\n",
            "[Epoch 86] Batch loss: 0.4471457302570343\n",
            "[Epoch 86] Batch loss: 0.5136929750442505\n",
            "[Epoch 86] Batch loss: 0.520566463470459\n",
            "[Epoch 86] Batch loss: 0.45267853140830994\n",
            "[Epoch 86] Batch loss: 0.37365394830703735\n",
            "[Epoch 86] Batch loss: 0.5509715676307678\n",
            "[Epoch 86] Batch loss: 0.4733109474182129\n",
            "[Epoch 86] Batch loss: 0.4925927519798279\n",
            "[Epoch 86] Batch loss: 0.4218825399875641\n",
            "[Epoch 86] Batch loss: 0.4190400242805481\n",
            "[Epoch 86] Batch loss: 0.4308589994907379\n",
            "[Epoch 86] Batch loss: 0.46352729201316833\n",
            "[Epoch 86] Batch loss: 0.46693846583366394\n",
            "[Epoch 86] Batch loss: 0.4818287491798401\n",
            "[Epoch 86] Batch loss: 0.4305709898471832\n",
            "[Epoch 86] Batch loss: 0.4562293291091919\n",
            "[Epoch 86] Batch loss: 0.44286882877349854\n",
            "[Epoch 86] Batch loss: 0.4253174960613251\n",
            "[Epoch 86] Batch loss: 0.5366830229759216\n",
            "[Epoch 86] Batch loss: 0.43275848031044006\n",
            "[Epoch 86] Batch loss: 0.5199408531188965\n",
            "[Epoch 86] Batch loss: 0.4020640552043915\n",
            "[Epoch 86] Batch loss: 0.47345346212387085\n",
            "[Epoch 86] Batch loss: 0.3929567337036133\n",
            "[Epoch 86] Batch loss: 0.6312456130981445\n",
            "[Epoch 86] Batch loss: 0.3992020785808563\n",
            "[Epoch 86] Batch loss: 0.44379687309265137\n",
            "[Epoch 86] Batch loss: 0.5672381520271301\n",
            "[Epoch 86] Batch loss: 0.4277630150318146\n",
            "[Epoch 86] Batch loss: 0.41326674818992615\n",
            "[Epoch 86] Batch loss: 0.4646662473678589\n",
            "[Epoch 87/100] Val loss: 0.7845\n",
            "Epochs without improvement: 2/20\n",
            "[Epoch 87] Batch loss: 0.42083412408828735\n",
            "[Epoch 87] Batch loss: 0.37180110812187195\n",
            "[Epoch 87] Batch loss: 0.4904256761074066\n",
            "[Epoch 87] Batch loss: 0.39581719040870667\n",
            "[Epoch 87] Batch loss: 0.40167129039764404\n",
            "[Epoch 87] Batch loss: 0.4216688871383667\n",
            "[Epoch 87] Batch loss: 0.5037580728530884\n",
            "[Epoch 87] Batch loss: 0.4096895456314087\n",
            "[Epoch 87] Batch loss: 0.46344277262687683\n",
            "[Epoch 87] Batch loss: 0.4758809804916382\n",
            "[Epoch 87] Batch loss: 0.38131800293922424\n",
            "[Epoch 87] Batch loss: 0.43551361560821533\n",
            "[Epoch 87] Batch loss: 0.5066908001899719\n",
            "[Epoch 87] Batch loss: 0.38994961977005005\n",
            "[Epoch 87] Batch loss: 0.44165340065956116\n",
            "[Epoch 87] Batch loss: 0.505652129650116\n",
            "[Epoch 87] Batch loss: 0.38644886016845703\n",
            "[Epoch 87] Batch loss: 0.4789932072162628\n",
            "[Epoch 87] Batch loss: 0.5174432992935181\n",
            "[Epoch 87] Batch loss: 0.4927154779434204\n",
            "[Epoch 87] Batch loss: 0.4032018780708313\n",
            "[Epoch 87] Batch loss: 0.4917526841163635\n",
            "[Epoch 87] Batch loss: 0.47287100553512573\n",
            "[Epoch 87] Batch loss: 0.4778355062007904\n",
            "[Epoch 87] Batch loss: 0.407686322927475\n",
            "[Epoch 87] Batch loss: 0.3804038166999817\n",
            "[Epoch 87] Batch loss: 0.48765313625335693\n",
            "[Epoch 87] Batch loss: 0.455369234085083\n",
            "[Epoch 87] Batch loss: 0.4149739444255829\n",
            "[Epoch 87] Batch loss: 0.46201780438423157\n",
            "[Epoch 87] Batch loss: 0.4106554090976715\n",
            "[Epoch 87] Batch loss: 0.5230612754821777\n",
            "[Epoch 87] Batch loss: 0.4141918420791626\n",
            "[Epoch 87] Batch loss: 0.4370500445365906\n",
            "[Epoch 87] Batch loss: 0.4261195957660675\n",
            "[Epoch 87] Batch loss: 0.5162250399589539\n",
            "[Epoch 87] Batch loss: 0.5767431259155273\n",
            "[Epoch 87] Batch loss: 0.4242565929889679\n",
            "[Epoch 87] Batch loss: 0.4801419675350189\n",
            "[Epoch 87] Batch loss: 0.5362173318862915\n",
            "[Epoch 87] Batch loss: 0.3881629407405853\n",
            "[Epoch 87] Batch loss: 0.39193812012672424\n",
            "[Epoch 87] Batch loss: 0.4734702408313751\n",
            "[Epoch 87] Batch loss: 0.4210955798625946\n",
            "[Epoch 87] Batch loss: 0.45676469802856445\n",
            "[Epoch 87] Batch loss: 0.4031398296356201\n",
            "[Epoch 87] Batch loss: 0.4525165557861328\n",
            "[Epoch 87] Batch loss: 0.5017873644828796\n",
            "[Epoch 87] Batch loss: 0.45731353759765625\n",
            "[Epoch 87] Batch loss: 0.4255956709384918\n",
            "[Epoch 87] Batch loss: 0.46544143557548523\n",
            "[Epoch 87] Batch loss: 0.41078245639801025\n",
            "[Epoch 87] Batch loss: 0.47412168979644775\n",
            "[Epoch 87] Batch loss: 0.4487663507461548\n",
            "[Epoch 87] Batch loss: 0.5256569981575012\n",
            "[Epoch 87] Batch loss: 0.44420936703681946\n",
            "[Epoch 87] Batch loss: 0.46498483419418335\n",
            "[Epoch 87] Batch loss: 0.4939529597759247\n",
            "[Epoch 87] Batch loss: 0.43747153878211975\n",
            "[Epoch 87] Batch loss: 0.4124780595302582\n",
            "[Epoch 87] Batch loss: 0.40364277362823486\n",
            "[Epoch 87] Batch loss: 0.5191118121147156\n",
            "[Epoch 87] Batch loss: 0.4581781327724457\n",
            "[Epoch 87] Batch loss: 0.3712172210216522\n",
            "[Epoch 87] Batch loss: 0.40717896819114685\n",
            "[Epoch 87] Batch loss: 0.5078644156455994\n",
            "[Epoch 87] Batch loss: 0.4038654565811157\n",
            "[Epoch 87] Batch loss: 0.37915295362472534\n",
            "[Epoch 87] Batch loss: 0.4294356405735016\n",
            "[Epoch 87] Batch loss: 0.4368794858455658\n",
            "[Epoch 87] Batch loss: 0.5046245455741882\n",
            "[Epoch 87] Batch loss: 0.4600016474723816\n",
            "[Epoch 87] Batch loss: 0.39533597230911255\n",
            "[Epoch 87] Batch loss: 0.6146935224533081\n",
            "[Epoch 87] Batch loss: 0.46023303270339966\n",
            "[Epoch 87] Batch loss: 0.4861544370651245\n",
            "[Epoch 87] Batch loss: 0.43846628069877625\n",
            "[Epoch 87] Batch loss: 0.5275773406028748\n",
            "[Epoch 87] Batch loss: 0.46482008695602417\n",
            "[Epoch 87] Batch loss: 0.4535503387451172\n",
            "[Epoch 87] Batch loss: 0.45385003089904785\n",
            "[Epoch 87] Batch loss: 0.46949946880340576\n",
            "[Epoch 87] Batch loss: 0.4359705448150635\n",
            "[Epoch 87] Batch loss: 0.45135998725891113\n",
            "[Epoch 87] Batch loss: 0.4631596505641937\n",
            "[Epoch 87] Batch loss: 0.46774664521217346\n",
            "[Epoch 87] Batch loss: 0.5012815594673157\n",
            "[Epoch 87] Batch loss: 0.5073536038398743\n",
            "[Epoch 87] Batch loss: 0.5449334383010864\n",
            "[Epoch 87] Batch loss: 0.5162969827651978\n",
            "[Epoch 87] Batch loss: 0.4237239956855774\n",
            "[Epoch 87] Batch loss: 0.4579896330833435\n",
            "[Epoch 87] Batch loss: 0.39515677094459534\n",
            "[Epoch 87] Batch loss: 0.4279654622077942\n",
            "[Epoch 87] Batch loss: 0.38616442680358887\n",
            "[Epoch 87] Batch loss: 0.3820114731788635\n",
            "[Epoch 87] Batch loss: 0.5109012722969055\n",
            "[Epoch 87] Batch loss: 0.43087154626846313\n",
            "[Epoch 87] Batch loss: 0.449471652507782\n",
            "[Epoch 87] Batch loss: 0.43542447686195374\n",
            "[Epoch 87] Batch loss: 0.47996771335601807\n",
            "[Epoch 87] Batch loss: 0.5038807392120361\n",
            "[Epoch 87] Batch loss: 0.3942926824092865\n",
            "[Epoch 87] Batch loss: 0.41348177194595337\n",
            "[Epoch 87] Batch loss: 0.4321461617946625\n",
            "[Epoch 87] Batch loss: 0.41361480951309204\n",
            "[Epoch 87] Batch loss: 0.39444267749786377\n",
            "[Epoch 87] Batch loss: 0.3973959684371948\n",
            "[Epoch 87] Batch loss: 0.49098631739616394\n",
            "[Epoch 87] Batch loss: 0.4811171293258667\n",
            "[Epoch 87] Batch loss: 0.3769579827785492\n",
            "[Epoch 87] Batch loss: 0.42534399032592773\n",
            "[Epoch 87] Batch loss: 0.5201126337051392\n",
            "[Epoch 87] Batch loss: 0.47673553228378296\n",
            "[Epoch 87] Batch loss: 0.4912419319152832\n",
            "[Epoch 87] Batch loss: 0.4675663411617279\n",
            "[Epoch 87] Batch loss: 0.4028642475605011\n",
            "[Epoch 87] Batch loss: 0.3582070767879486\n",
            "[Epoch 87] Batch loss: 0.5203163027763367\n",
            "[Epoch 87] Batch loss: 0.3783849775791168\n",
            "[Epoch 87] Batch loss: 0.47569403052330017\n",
            "[Epoch 87] Batch loss: 0.44827112555503845\n",
            "[Epoch 87] Batch loss: 0.45692750811576843\n",
            "[Epoch 87] Batch loss: 0.41512706875801086\n",
            "[Epoch 87] Batch loss: 0.7183977365493774\n",
            "[Epoch 87] Batch loss: 0.4356318712234497\n",
            "[Epoch 88/100] Val loss: 0.7714\n",
            "Best saved model.\n",
            "[Epoch 88] Batch loss: 0.3661312460899353\n",
            "[Epoch 88] Batch loss: 0.47390997409820557\n",
            "[Epoch 88] Batch loss: 0.4371783137321472\n",
            "[Epoch 88] Batch loss: 0.4297441244125366\n",
            "[Epoch 88] Batch loss: 0.5310219526290894\n",
            "[Epoch 88] Batch loss: 0.47811922430992126\n",
            "[Epoch 88] Batch loss: 0.41817378997802734\n",
            "[Epoch 88] Batch loss: 0.41286158561706543\n",
            "[Epoch 88] Batch loss: 0.5316739678382874\n",
            "[Epoch 88] Batch loss: 0.4099752604961395\n",
            "[Epoch 88] Batch loss: 0.3905995786190033\n",
            "[Epoch 88] Batch loss: 0.5579872727394104\n",
            "[Epoch 88] Batch loss: 0.5029945373535156\n",
            "[Epoch 88] Batch loss: 0.416215717792511\n",
            "[Epoch 88] Batch loss: 0.40829819440841675\n",
            "[Epoch 88] Batch loss: 0.4868205785751343\n",
            "[Epoch 88] Batch loss: 0.5038543343544006\n",
            "[Epoch 88] Batch loss: 0.4786330461502075\n",
            "[Epoch 88] Batch loss: 0.39761173725128174\n",
            "[Epoch 88] Batch loss: 0.4750213921070099\n",
            "[Epoch 88] Batch loss: 0.40085697174072266\n",
            "[Epoch 88] Batch loss: 0.4931986927986145\n",
            "[Epoch 88] Batch loss: 0.41853851079940796\n",
            "[Epoch 88] Batch loss: 0.43024522066116333\n",
            "[Epoch 88] Batch loss: 0.45573022961616516\n",
            "[Epoch 88] Batch loss: 0.5703881978988647\n",
            "[Epoch 88] Batch loss: 0.42154014110565186\n",
            "[Epoch 88] Batch loss: 0.34939301013946533\n",
            "[Epoch 88] Batch loss: 0.46058937907218933\n",
            "[Epoch 88] Batch loss: 0.39497464895248413\n",
            "[Epoch 88] Batch loss: 0.5271349549293518\n",
            "[Epoch 88] Batch loss: 0.5064095258712769\n",
            "[Epoch 88] Batch loss: 0.4013844132423401\n",
            "[Epoch 88] Batch loss: 0.5186846852302551\n",
            "[Epoch 88] Batch loss: 0.4590721130371094\n",
            "[Epoch 88] Batch loss: 0.49094003438949585\n",
            "[Epoch 88] Batch loss: 0.46614277362823486\n",
            "[Epoch 88] Batch loss: 0.40848082304000854\n",
            "[Epoch 88] Batch loss: 0.46360355615615845\n",
            "[Epoch 88] Batch loss: 0.515638530254364\n",
            "[Epoch 88] Batch loss: 0.4888898432254791\n",
            "[Epoch 88] Batch loss: 0.4646552801132202\n",
            "[Epoch 88] Batch loss: 0.4255886375904083\n",
            "[Epoch 88] Batch loss: 0.42882031202316284\n",
            "[Epoch 88] Batch loss: 0.462694376707077\n",
            "[Epoch 88] Batch loss: 0.46112769842147827\n",
            "[Epoch 88] Batch loss: 0.49169495701789856\n",
            "[Epoch 88] Batch loss: 0.45895659923553467\n",
            "[Epoch 88] Batch loss: 0.4178362488746643\n",
            "[Epoch 88] Batch loss: 0.5060714483261108\n",
            "[Epoch 88] Batch loss: 0.451072096824646\n",
            "[Epoch 88] Batch loss: 0.4668394923210144\n",
            "[Epoch 88] Batch loss: 0.3820444941520691\n",
            "[Epoch 88] Batch loss: 0.5016219615936279\n",
            "[Epoch 88] Batch loss: 0.5044105648994446\n",
            "[Epoch 88] Batch loss: 0.41480857133865356\n",
            "[Epoch 88] Batch loss: 0.40759485960006714\n",
            "[Epoch 88] Batch loss: 0.46099886298179626\n",
            "[Epoch 88] Batch loss: 0.4399515390396118\n",
            "[Epoch 88] Batch loss: 0.3993934690952301\n",
            "[Epoch 88] Batch loss: 0.400687038898468\n",
            "[Epoch 88] Batch loss: 0.3878922760486603\n",
            "[Epoch 88] Batch loss: 0.39080387353897095\n",
            "[Epoch 88] Batch loss: 0.546342670917511\n",
            "[Epoch 88] Batch loss: 0.6016655564308167\n",
            "[Epoch 88] Batch loss: 0.5015628933906555\n",
            "[Epoch 88] Batch loss: 0.42992013692855835\n",
            "[Epoch 88] Batch loss: 0.6109522581100464\n",
            "[Epoch 88] Batch loss: 0.47021380066871643\n",
            "[Epoch 88] Batch loss: 0.5300914645195007\n",
            "[Epoch 88] Batch loss: 0.43451032042503357\n",
            "[Epoch 88] Batch loss: 0.4421115815639496\n",
            "[Epoch 88] Batch loss: 0.5273270010948181\n",
            "[Epoch 88] Batch loss: 0.42035239934921265\n",
            "[Epoch 88] Batch loss: 0.42032933235168457\n",
            "[Epoch 88] Batch loss: 0.39935386180877686\n",
            "[Epoch 88] Batch loss: 0.4123941659927368\n",
            "[Epoch 88] Batch loss: 0.4062420129776001\n",
            "[Epoch 88] Batch loss: 0.4532471299171448\n",
            "[Epoch 88] Batch loss: 0.37058183550834656\n",
            "[Epoch 88] Batch loss: 0.4379291236400604\n",
            "[Epoch 88] Batch loss: 0.4578988254070282\n",
            "[Epoch 88] Batch loss: 0.4695014953613281\n",
            "[Epoch 88] Batch loss: 0.4875001013278961\n",
            "[Epoch 88] Batch loss: 0.3942815065383911\n",
            "[Epoch 88] Batch loss: 0.4856187105178833\n",
            "[Epoch 88] Batch loss: 0.43989142775535583\n",
            "[Epoch 88] Batch loss: 0.43572673201560974\n",
            "[Epoch 88] Batch loss: 0.40342921018600464\n",
            "[Epoch 88] Batch loss: 0.5158668756484985\n",
            "[Epoch 88] Batch loss: 0.38110148906707764\n",
            "[Epoch 88] Batch loss: 0.5206539630889893\n",
            "[Epoch 88] Batch loss: 0.46762049198150635\n",
            "[Epoch 88] Batch loss: 0.4109481871128082\n",
            "[Epoch 88] Batch loss: 0.4253374934196472\n",
            "[Epoch 88] Batch loss: 0.4343505799770355\n",
            "[Epoch 88] Batch loss: 0.44408345222473145\n",
            "[Epoch 88] Batch loss: 0.44366154074668884\n",
            "[Epoch 88] Batch loss: 0.5125210881233215\n",
            "[Epoch 88] Batch loss: 0.3470540940761566\n",
            "[Epoch 88] Batch loss: 0.5085733532905579\n",
            "[Epoch 88] Batch loss: 0.4124946892261505\n",
            "[Epoch 88] Batch loss: 0.46741628646850586\n",
            "[Epoch 88] Batch loss: 0.40932410955429077\n",
            "[Epoch 88] Batch loss: 0.45755043625831604\n",
            "[Epoch 88] Batch loss: 0.42873892188072205\n",
            "[Epoch 88] Batch loss: 0.3757213056087494\n",
            "[Epoch 88] Batch loss: 0.48518651723861694\n",
            "[Epoch 88] Batch loss: 0.42069604992866516\n",
            "[Epoch 88] Batch loss: 0.4579331576824188\n",
            "[Epoch 88] Batch loss: 0.46318432688713074\n",
            "[Epoch 88] Batch loss: 0.41942310333251953\n",
            "[Epoch 88] Batch loss: 0.40521931648254395\n",
            "[Epoch 88] Batch loss: 0.37969547510147095\n",
            "[Epoch 88] Batch loss: 0.4931347966194153\n",
            "[Epoch 88] Batch loss: 0.43816521763801575\n",
            "[Epoch 88] Batch loss: 0.45361319184303284\n",
            "[Epoch 88] Batch loss: 0.38211822509765625\n",
            "[Epoch 88] Batch loss: 0.47240573167800903\n",
            "[Epoch 88] Batch loss: 0.47544920444488525\n",
            "[Epoch 88] Batch loss: 0.4820463955402374\n",
            "[Epoch 88] Batch loss: 0.467885285615921\n",
            "[Epoch 88] Batch loss: 0.3695969581604004\n",
            "[Epoch 88] Batch loss: 0.41195544600486755\n",
            "[Epoch 88] Batch loss: 0.43887725472450256\n",
            "[Epoch 88] Batch loss: 0.5118178129196167\n",
            "[Epoch 89/100] Val loss: 0.7611\n",
            "Best saved model.\n",
            "[Epoch 89] Batch loss: 0.41613999009132385\n",
            "[Epoch 89] Batch loss: 0.3652181625366211\n",
            "[Epoch 89] Batch loss: 0.5644492506980896\n",
            "[Epoch 89] Batch loss: 0.3877594470977783\n",
            "[Epoch 89] Batch loss: 0.40441203117370605\n",
            "[Epoch 89] Batch loss: 0.4061335325241089\n",
            "[Epoch 89] Batch loss: 0.4434701204299927\n",
            "[Epoch 89] Batch loss: 0.4091876745223999\n",
            "[Epoch 89] Batch loss: 0.40538808703422546\n",
            "[Epoch 89] Batch loss: 0.4702221751213074\n",
            "[Epoch 89] Batch loss: 0.5016850233078003\n",
            "[Epoch 89] Batch loss: 0.45551496744155884\n",
            "[Epoch 89] Batch loss: 0.38020768761634827\n",
            "[Epoch 89] Batch loss: 0.43896421790122986\n",
            "[Epoch 89] Batch loss: 0.43418076634407043\n",
            "[Epoch 89] Batch loss: 0.455265074968338\n",
            "[Epoch 89] Batch loss: 0.5305520296096802\n",
            "[Epoch 89] Batch loss: 0.5134177803993225\n",
            "[Epoch 89] Batch loss: 0.4619954526424408\n",
            "[Epoch 89] Batch loss: 0.4622766077518463\n",
            "[Epoch 89] Batch loss: 0.4731205105781555\n",
            "[Epoch 89] Batch loss: 0.4375574588775635\n",
            "[Epoch 89] Batch loss: 0.464874267578125\n",
            "[Epoch 89] Batch loss: 0.4361840486526489\n",
            "[Epoch 89] Batch loss: 0.43989861011505127\n",
            "[Epoch 89] Batch loss: 0.5112878680229187\n",
            "[Epoch 89] Batch loss: 0.3849906921386719\n",
            "[Epoch 89] Batch loss: 0.5028420686721802\n",
            "[Epoch 89] Batch loss: 0.5359207391738892\n",
            "[Epoch 89] Batch loss: 0.3921395242214203\n",
            "[Epoch 89] Batch loss: 0.3891321122646332\n",
            "[Epoch 89] Batch loss: 0.39503368735313416\n",
            "[Epoch 89] Batch loss: 0.461637020111084\n",
            "[Epoch 89] Batch loss: 0.4545753002166748\n",
            "[Epoch 89] Batch loss: 0.40098777413368225\n",
            "[Epoch 89] Batch loss: 0.5219218134880066\n",
            "[Epoch 89] Batch loss: 0.4293181598186493\n",
            "[Epoch 89] Batch loss: 0.3735887110233307\n",
            "[Epoch 89] Batch loss: 0.3655284345149994\n",
            "[Epoch 89] Batch loss: 0.48836690187454224\n",
            "[Epoch 89] Batch loss: 0.4563235938549042\n",
            "[Epoch 89] Batch loss: 0.4152427017688751\n",
            "[Epoch 89] Batch loss: 0.5087453722953796\n",
            "[Epoch 89] Batch loss: 0.42669743299484253\n",
            "[Epoch 89] Batch loss: 0.5416813492774963\n",
            "[Epoch 89] Batch loss: 0.4146523177623749\n",
            "[Epoch 89] Batch loss: 0.4557339549064636\n",
            "[Epoch 89] Batch loss: 0.3979104459285736\n",
            "[Epoch 89] Batch loss: 0.41675129532814026\n",
            "[Epoch 89] Batch loss: 0.5887191891670227\n",
            "[Epoch 89] Batch loss: 0.40661150217056274\n",
            "[Epoch 89] Batch loss: 0.42996644973754883\n",
            "[Epoch 89] Batch loss: 0.3717127740383148\n",
            "[Epoch 89] Batch loss: 0.4560708999633789\n",
            "[Epoch 89] Batch loss: 0.46097302436828613\n",
            "[Epoch 89] Batch loss: 0.41800656914711\n",
            "[Epoch 89] Batch loss: 0.34349343180656433\n",
            "[Epoch 89] Batch loss: 0.45692965388298035\n",
            "[Epoch 89] Batch loss: 0.5114333629608154\n",
            "[Epoch 89] Batch loss: 0.3922886550426483\n",
            "[Epoch 89] Batch loss: 0.3918451964855194\n",
            "[Epoch 89] Batch loss: 0.41615188121795654\n",
            "[Epoch 89] Batch loss: 0.3926249146461487\n",
            "[Epoch 89] Batch loss: 0.39702609181404114\n",
            "[Epoch 89] Batch loss: 0.4283001720905304\n",
            "[Epoch 89] Batch loss: 0.38435718417167664\n",
            "[Epoch 89] Batch loss: 0.4408234655857086\n",
            "[Epoch 89] Batch loss: 0.43224307894706726\n",
            "[Epoch 89] Batch loss: 0.43188154697418213\n",
            "[Epoch 89] Batch loss: 0.531753420829773\n",
            "[Epoch 89] Batch loss: 0.46900200843811035\n",
            "[Epoch 89] Batch loss: 0.40932655334472656\n",
            "[Epoch 89] Batch loss: 0.42770901322364807\n",
            "[Epoch 89] Batch loss: 0.5044181942939758\n",
            "[Epoch 89] Batch loss: 0.4176792502403259\n",
            "[Epoch 89] Batch loss: 0.3958362638950348\n",
            "[Epoch 89] Batch loss: 0.42473796010017395\n",
            "[Epoch 89] Batch loss: 0.5303422212600708\n",
            "[Epoch 89] Batch loss: 0.3958616256713867\n",
            "[Epoch 89] Batch loss: 0.4254199266433716\n",
            "[Epoch 89] Batch loss: 0.4361521601676941\n",
            "[Epoch 89] Batch loss: 0.5289912819862366\n",
            "[Epoch 89] Batch loss: 0.4158448874950409\n",
            "[Epoch 89] Batch loss: 0.5543414354324341\n",
            "[Epoch 89] Batch loss: 0.44292256236076355\n",
            "[Epoch 89] Batch loss: 0.4010322093963623\n",
            "[Epoch 89] Batch loss: 0.48180434107780457\n",
            "[Epoch 89] Batch loss: 0.4925527274608612\n",
            "[Epoch 89] Batch loss: 0.39973023533821106\n",
            "[Epoch 89] Batch loss: 0.35326188802719116\n",
            "[Epoch 89] Batch loss: 0.5159879922866821\n",
            "[Epoch 89] Batch loss: 0.3979160189628601\n",
            "[Epoch 89] Batch loss: 0.4338914752006531\n",
            "[Epoch 89] Batch loss: 0.5097601413726807\n",
            "[Epoch 89] Batch loss: 0.4993501305580139\n",
            "[Epoch 89] Batch loss: 0.34489136934280396\n",
            "[Epoch 89] Batch loss: 0.40733402967453003\n",
            "[Epoch 89] Batch loss: 0.40443018078804016\n",
            "[Epoch 89] Batch loss: 0.46513229608535767\n",
            "[Epoch 89] Batch loss: 0.40758055448532104\n",
            "[Epoch 89] Batch loss: 0.4289547801017761\n",
            "[Epoch 89] Batch loss: 0.5040277242660522\n",
            "[Epoch 89] Batch loss: 0.47010982036590576\n",
            "[Epoch 89] Batch loss: 0.5423993468284607\n",
            "[Epoch 89] Batch loss: 0.36255863308906555\n",
            "[Epoch 89] Batch loss: 0.48711514472961426\n",
            "[Epoch 89] Batch loss: 0.40236929059028625\n",
            "[Epoch 89] Batch loss: 0.46826401352882385\n",
            "[Epoch 89] Batch loss: 0.4432036280632019\n",
            "[Epoch 89] Batch loss: 0.4717825651168823\n",
            "[Epoch 89] Batch loss: 0.45748648047447205\n",
            "[Epoch 89] Batch loss: 0.5562843084335327\n",
            "[Epoch 89] Batch loss: 0.4743354916572571\n",
            "[Epoch 89] Batch loss: 0.4897843897342682\n",
            "[Epoch 89] Batch loss: 0.5064651966094971\n",
            "[Epoch 89] Batch loss: 0.3616170585155487\n",
            "[Epoch 89] Batch loss: 0.5158851146697998\n",
            "[Epoch 89] Batch loss: 0.3881376385688782\n",
            "[Epoch 89] Batch loss: 0.3715629279613495\n",
            "[Epoch 89] Batch loss: 0.3646964430809021\n",
            "[Epoch 89] Batch loss: 0.4871710538864136\n",
            "[Epoch 89] Batch loss: 0.4027441740036011\n",
            "[Epoch 89] Batch loss: 0.42539095878601074\n",
            "[Epoch 89] Batch loss: 0.4913712739944458\n",
            "[Epoch 89] Batch loss: 0.44891199469566345\n",
            "[Epoch 89] Batch loss: 0.4003998637199402\n",
            "[Epoch 90/100] Val loss: 0.7569\n",
            "Best saved model.\n",
            "[Epoch 90] Batch loss: 0.47368812561035156\n",
            "[Epoch 90] Batch loss: 0.4987892806529999\n",
            "[Epoch 90] Batch loss: 0.5265954732894897\n",
            "[Epoch 90] Batch loss: 0.48854297399520874\n",
            "[Epoch 90] Batch loss: 0.4468972086906433\n",
            "[Epoch 90] Batch loss: 0.3905026912689209\n",
            "[Epoch 90] Batch loss: 0.43079352378845215\n",
            "[Epoch 90] Batch loss: 0.44738876819610596\n",
            "[Epoch 90] Batch loss: 0.4091148376464844\n",
            "[Epoch 90] Batch loss: 0.5478464365005493\n",
            "[Epoch 90] Batch loss: 0.4377268850803375\n",
            "[Epoch 90] Batch loss: 0.44337573647499084\n",
            "[Epoch 90] Batch loss: 0.3762604892253876\n",
            "[Epoch 90] Batch loss: 0.37013763189315796\n",
            "[Epoch 90] Batch loss: 0.5337044596672058\n",
            "[Epoch 90] Batch loss: 0.3992091417312622\n",
            "[Epoch 90] Batch loss: 0.45715993642807007\n",
            "[Epoch 90] Batch loss: 0.3582417666912079\n",
            "[Epoch 90] Batch loss: 0.3615075945854187\n",
            "[Epoch 90] Batch loss: 0.476876825094223\n",
            "[Epoch 90] Batch loss: 0.37949371337890625\n",
            "[Epoch 90] Batch loss: 0.49077722430229187\n",
            "[Epoch 90] Batch loss: 0.3939616084098816\n",
            "[Epoch 90] Batch loss: 0.5384874939918518\n",
            "[Epoch 90] Batch loss: 0.47146350145339966\n",
            "[Epoch 90] Batch loss: 0.4778854548931122\n",
            "[Epoch 90] Batch loss: 0.5535100102424622\n",
            "[Epoch 90] Batch loss: 0.4545639157295227\n",
            "[Epoch 90] Batch loss: 0.4325987696647644\n",
            "[Epoch 90] Batch loss: 0.4023156464099884\n",
            "[Epoch 90] Batch loss: 0.39747554063796997\n",
            "[Epoch 90] Batch loss: 0.4473631680011749\n",
            "[Epoch 90] Batch loss: 0.48023518919944763\n",
            "[Epoch 90] Batch loss: 0.47079169750213623\n",
            "[Epoch 90] Batch loss: 0.40942317247390747\n",
            "[Epoch 90] Batch loss: 0.4673392176628113\n",
            "[Epoch 90] Batch loss: 0.49182024598121643\n",
            "[Epoch 90] Batch loss: 0.4927268624305725\n",
            "[Epoch 90] Batch loss: 0.4377410113811493\n",
            "[Epoch 90] Batch loss: 0.3932209312915802\n",
            "[Epoch 90] Batch loss: 0.3658800721168518\n",
            "[Epoch 90] Batch loss: 0.37515944242477417\n",
            "[Epoch 90] Batch loss: 0.494445264339447\n",
            "[Epoch 90] Batch loss: 0.39551281929016113\n",
            "[Epoch 90] Batch loss: 0.46629267930984497\n",
            "[Epoch 90] Batch loss: 0.4789523482322693\n",
            "[Epoch 90] Batch loss: 0.413726806640625\n",
            "[Epoch 90] Batch loss: 0.4291398227214813\n",
            "[Epoch 90] Batch loss: 0.4118024408817291\n",
            "[Epoch 90] Batch loss: 0.46770578622817993\n",
            "[Epoch 90] Batch loss: 0.39476943016052246\n",
            "[Epoch 90] Batch loss: 0.47398948669433594\n",
            "[Epoch 90] Batch loss: 0.4503735899925232\n",
            "[Epoch 90] Batch loss: 0.4522227942943573\n",
            "[Epoch 90] Batch loss: 0.4594738185405731\n",
            "[Epoch 90] Batch loss: 0.40531763434410095\n",
            "[Epoch 90] Batch loss: 0.4604186415672302\n",
            "[Epoch 90] Batch loss: 0.4524480700492859\n",
            "[Epoch 90] Batch loss: 0.42771580815315247\n",
            "[Epoch 90] Batch loss: 0.3790847361087799\n",
            "[Epoch 90] Batch loss: 0.42156413197517395\n",
            "[Epoch 90] Batch loss: 0.437241792678833\n",
            "[Epoch 90] Batch loss: 0.48360592126846313\n",
            "[Epoch 90] Batch loss: 0.44442224502563477\n",
            "[Epoch 90] Batch loss: 0.37334728240966797\n",
            "[Epoch 90] Batch loss: 0.4002933204174042\n",
            "[Epoch 90] Batch loss: 0.4832187294960022\n",
            "[Epoch 90] Batch loss: 0.3991318643093109\n",
            "[Epoch 90] Batch loss: 0.4444541335105896\n",
            "[Epoch 90] Batch loss: 0.40814992785453796\n",
            "[Epoch 90] Batch loss: 0.47459059953689575\n",
            "[Epoch 90] Batch loss: 0.4228541851043701\n",
            "[Epoch 90] Batch loss: 0.44597741961479187\n",
            "[Epoch 90] Batch loss: 0.46544507145881653\n",
            "[Epoch 90] Batch loss: 0.4320390522480011\n",
            "[Epoch 90] Batch loss: 0.4256538152694702\n",
            "[Epoch 90] Batch loss: 0.43285465240478516\n",
            "[Epoch 90] Batch loss: 0.5075748562812805\n",
            "[Epoch 90] Batch loss: 0.3719479441642761\n",
            "[Epoch 90] Batch loss: 0.5050041079521179\n",
            "[Epoch 90] Batch loss: 0.46664416790008545\n",
            "[Epoch 90] Batch loss: 0.44700953364372253\n",
            "[Epoch 90] Batch loss: 0.43955299258232117\n",
            "[Epoch 90] Batch loss: 0.4768374264240265\n",
            "[Epoch 90] Batch loss: 0.35417333245277405\n",
            "[Epoch 90] Batch loss: 0.5193315148353577\n",
            "[Epoch 90] Batch loss: 0.45015692710876465\n",
            "[Epoch 90] Batch loss: 0.4878019094467163\n",
            "[Epoch 90] Batch loss: 0.4437341094017029\n",
            "[Epoch 90] Batch loss: 0.5486140847206116\n",
            "[Epoch 90] Batch loss: 0.4000599980354309\n",
            "[Epoch 90] Batch loss: 0.34982097148895264\n",
            "[Epoch 90] Batch loss: 0.471280038356781\n",
            "[Epoch 90] Batch loss: 0.42113909125328064\n",
            "[Epoch 90] Batch loss: 0.3603699505329132\n",
            "[Epoch 90] Batch loss: 0.43414658308029175\n",
            "[Epoch 90] Batch loss: 0.36513444781303406\n",
            "[Epoch 90] Batch loss: 0.41816186904907227\n",
            "[Epoch 90] Batch loss: 0.436854749917984\n",
            "[Epoch 90] Batch loss: 0.43696486949920654\n",
            "[Epoch 90] Batch loss: 0.3851868808269501\n",
            "[Epoch 90] Batch loss: 0.4936458468437195\n",
            "[Epoch 90] Batch loss: 0.3625941276550293\n",
            "[Epoch 90] Batch loss: 0.47712740302085876\n",
            "[Epoch 90] Batch loss: 0.42892104387283325\n",
            "[Epoch 90] Batch loss: 0.45347151160240173\n",
            "[Epoch 90] Batch loss: 0.4431207776069641\n",
            "[Epoch 90] Batch loss: 0.36855873465538025\n",
            "[Epoch 90] Batch loss: 0.41042497754096985\n",
            "[Epoch 90] Batch loss: 0.39863085746765137\n",
            "[Epoch 90] Batch loss: 0.4756300449371338\n",
            "[Epoch 90] Batch loss: 0.3848167955875397\n",
            "[Epoch 90] Batch loss: 0.4852132797241211\n",
            "[Epoch 90] Batch loss: 0.4569746255874634\n",
            "[Epoch 90] Batch loss: 0.417344331741333\n",
            "[Epoch 90] Batch loss: 0.3747391700744629\n",
            "[Epoch 90] Batch loss: 0.4437568187713623\n",
            "[Epoch 90] Batch loss: 0.5444713830947876\n",
            "[Epoch 90] Batch loss: 0.4754839539527893\n",
            "[Epoch 90] Batch loss: 0.5180171132087708\n",
            "[Epoch 90] Batch loss: 0.46870872378349304\n",
            "[Epoch 90] Batch loss: 0.45243316888809204\n",
            "[Epoch 90] Batch loss: 0.4388326406478882\n",
            "[Epoch 90] Batch loss: 0.4506329298019409\n",
            "[Epoch 90] Batch loss: 0.43105414509773254\n",
            "[Epoch 90] Batch loss: 0.5981304049491882\n",
            "[Epoch 91/100] Val loss: 0.7646\n",
            "Epochs without improvement: 1/20\n",
            "[Epoch 91] Batch loss: 0.4187659025192261\n",
            "[Epoch 91] Batch loss: 0.40973782539367676\n",
            "[Epoch 91] Batch loss: 0.42369580268859863\n",
            "[Epoch 91] Batch loss: 0.4472656846046448\n",
            "[Epoch 91] Batch loss: 0.43518900871276855\n",
            "[Epoch 91] Batch loss: 0.5365235805511475\n",
            "[Epoch 91] Batch loss: 0.44774502515792847\n",
            "[Epoch 91] Batch loss: 0.417309433221817\n",
            "[Epoch 91] Batch loss: 0.4062885046005249\n",
            "[Epoch 91] Batch loss: 0.4997981786727905\n",
            "[Epoch 91] Batch loss: 0.4043617248535156\n",
            "[Epoch 91] Batch loss: 0.41804245114326477\n",
            "[Epoch 91] Batch loss: 0.5184661149978638\n",
            "[Epoch 91] Batch loss: 0.4521491229534149\n",
            "[Epoch 91] Batch loss: 0.46526700258255005\n",
            "[Epoch 91] Batch loss: 0.47705259919166565\n",
            "[Epoch 91] Batch loss: 0.3866567313671112\n",
            "[Epoch 91] Batch loss: 0.3877306580543518\n",
            "[Epoch 91] Batch loss: 0.43965569138526917\n",
            "[Epoch 91] Batch loss: 0.403403639793396\n",
            "[Epoch 91] Batch loss: 0.3841388523578644\n",
            "[Epoch 91] Batch loss: 0.5307743549346924\n",
            "[Epoch 91] Batch loss: 0.3689979612827301\n",
            "[Epoch 91] Batch loss: 0.3977266252040863\n",
            "[Epoch 91] Batch loss: 0.4608137905597687\n",
            "[Epoch 91] Batch loss: 0.5141909122467041\n",
            "[Epoch 91] Batch loss: 0.3962799310684204\n",
            "[Epoch 91] Batch loss: 0.3985348641872406\n",
            "[Epoch 91] Batch loss: 0.37270987033843994\n",
            "[Epoch 91] Batch loss: 0.41040942072868347\n",
            "[Epoch 91] Batch loss: 0.5063255429267883\n",
            "[Epoch 91] Batch loss: 0.4050329327583313\n",
            "[Epoch 91] Batch loss: 0.4920814037322998\n",
            "[Epoch 91] Batch loss: 0.4427074193954468\n",
            "[Epoch 91] Batch loss: 0.38751423358917236\n",
            "[Epoch 91] Batch loss: 0.4620193541049957\n",
            "[Epoch 91] Batch loss: 0.464775949716568\n",
            "[Epoch 91] Batch loss: 0.40147292613983154\n",
            "[Epoch 91] Batch loss: 0.41929036378860474\n",
            "[Epoch 91] Batch loss: 0.4321586787700653\n",
            "[Epoch 91] Batch loss: 0.42585980892181396\n",
            "[Epoch 91] Batch loss: 0.43442288041114807\n",
            "[Epoch 91] Batch loss: 0.43054237961769104\n",
            "[Epoch 91] Batch loss: 0.45764845609664917\n",
            "[Epoch 91] Batch loss: 0.4141997694969177\n",
            "[Epoch 91] Batch loss: 0.40028080344200134\n",
            "[Epoch 91] Batch loss: 0.446207195520401\n",
            "[Epoch 91] Batch loss: 0.4477243423461914\n",
            "[Epoch 91] Batch loss: 0.4204181432723999\n",
            "[Epoch 91] Batch loss: 0.3955543339252472\n",
            "[Epoch 91] Batch loss: 0.35637640953063965\n",
            "[Epoch 91] Batch loss: 0.45660534501075745\n",
            "[Epoch 91] Batch loss: 0.3727969825267792\n",
            "[Epoch 91] Batch loss: 0.41766610741615295\n",
            "[Epoch 91] Batch loss: 0.3970339000225067\n",
            "[Epoch 91] Batch loss: 0.4883055090904236\n",
            "[Epoch 91] Batch loss: 0.4428591728210449\n",
            "[Epoch 91] Batch loss: 0.45260751247406006\n",
            "[Epoch 91] Batch loss: 0.441020667552948\n",
            "[Epoch 91] Batch loss: 0.37729495763778687\n",
            "[Epoch 91] Batch loss: 0.4141453504562378\n",
            "[Epoch 91] Batch loss: 0.4592670798301697\n",
            "[Epoch 91] Batch loss: 0.46429795026779175\n",
            "[Epoch 91] Batch loss: 0.3830026388168335\n",
            "[Epoch 91] Batch loss: 0.37378549575805664\n",
            "[Epoch 91] Batch loss: 0.3870902359485626\n",
            "[Epoch 91] Batch loss: 0.4575057327747345\n",
            "[Epoch 91] Batch loss: 0.40218761563301086\n",
            "[Epoch 91] Batch loss: 0.4360734224319458\n",
            "[Epoch 91] Batch loss: 0.49310991168022156\n",
            "[Epoch 91] Batch loss: 0.44892194867134094\n",
            "[Epoch 91] Batch loss: 0.418692022562027\n",
            "[Epoch 91] Batch loss: 0.4365568459033966\n",
            "[Epoch 91] Batch loss: 0.44070664048194885\n",
            "[Epoch 91] Batch loss: 0.39669787883758545\n",
            "[Epoch 91] Batch loss: 0.3726375699043274\n",
            "[Epoch 91] Batch loss: 0.4182882308959961\n",
            "[Epoch 91] Batch loss: 0.431750625371933\n",
            "[Epoch 91] Batch loss: 0.5504550337791443\n",
            "[Epoch 91] Batch loss: 0.45484277606010437\n",
            "[Epoch 91] Batch loss: 0.4450196623802185\n",
            "[Epoch 91] Batch loss: 0.4874194860458374\n",
            "[Epoch 91] Batch loss: 0.5596634149551392\n",
            "[Epoch 91] Batch loss: 0.46333369612693787\n",
            "[Epoch 91] Batch loss: 0.4591160714626312\n",
            "[Epoch 91] Batch loss: 0.3941008448600769\n",
            "[Epoch 91] Batch loss: 0.38914862275123596\n",
            "[Epoch 91] Batch loss: 0.42719000577926636\n",
            "[Epoch 91] Batch loss: 0.36718273162841797\n",
            "[Epoch 91] Batch loss: 0.3823637366294861\n",
            "[Epoch 91] Batch loss: 0.45805004239082336\n",
            "[Epoch 91] Batch loss: 0.4113485813140869\n",
            "[Epoch 91] Batch loss: 0.40086328983306885\n",
            "[Epoch 91] Batch loss: 0.47022005915641785\n",
            "[Epoch 91] Batch loss: 0.45996442437171936\n",
            "[Epoch 91] Batch loss: 0.42267876863479614\n",
            "[Epoch 91] Batch loss: 0.4217269718647003\n",
            "[Epoch 91] Batch loss: 0.39857977628707886\n",
            "[Epoch 91] Batch loss: 0.4507073760032654\n",
            "[Epoch 91] Batch loss: 0.4378589391708374\n",
            "[Epoch 91] Batch loss: 0.43586984276771545\n",
            "[Epoch 91] Batch loss: 0.46194717288017273\n",
            "[Epoch 91] Batch loss: 0.4044535756111145\n",
            "[Epoch 91] Batch loss: 0.4643808901309967\n",
            "[Epoch 91] Batch loss: 0.5445971488952637\n",
            "[Epoch 91] Batch loss: 0.37831059098243713\n",
            "[Epoch 91] Batch loss: 0.4380071461200714\n",
            "[Epoch 91] Batch loss: 0.48295754194259644\n",
            "[Epoch 91] Batch loss: 0.36340221762657166\n",
            "[Epoch 91] Batch loss: 0.4192192852497101\n",
            "[Epoch 91] Batch loss: 0.48749682307243347\n",
            "[Epoch 91] Batch loss: 0.49567753076553345\n",
            "[Epoch 91] Batch loss: 0.4168252646923065\n",
            "[Epoch 91] Batch loss: 0.4013071656227112\n",
            "[Epoch 91] Batch loss: 0.47989049553871155\n",
            "[Epoch 91] Batch loss: 0.4187200367450714\n",
            "[Epoch 91] Batch loss: 0.4028792679309845\n",
            "[Epoch 91] Batch loss: 0.40155965089797974\n",
            "[Epoch 91] Batch loss: 0.41477030515670776\n",
            "[Epoch 91] Batch loss: 0.38223007321357727\n",
            "[Epoch 91] Batch loss: 0.4234488606452942\n",
            "[Epoch 91] Batch loss: 0.41919562220573425\n",
            "[Epoch 91] Batch loss: 0.4997175931930542\n",
            "[Epoch 91] Batch loss: 0.40490883588790894\n",
            "[Epoch 91] Batch loss: 0.4885960817337036\n",
            "[Epoch 91] Batch loss: 0.515608012676239\n",
            "[Epoch 92/100] Val loss: 0.7500\n",
            "Best saved model.\n",
            "[Epoch 92] Batch loss: 0.4039974510669708\n",
            "[Epoch 92] Batch loss: 0.3564087152481079\n",
            "[Epoch 92] Batch loss: 0.3974519968032837\n",
            "[Epoch 92] Batch loss: 0.4495026171207428\n",
            "[Epoch 92] Batch loss: 0.4305296838283539\n",
            "[Epoch 92] Batch loss: 0.46845629811286926\n",
            "[Epoch 92] Batch loss: 0.40954768657684326\n",
            "[Epoch 92] Batch loss: 0.3696817457675934\n",
            "[Epoch 92] Batch loss: 0.42366376519203186\n",
            "[Epoch 92] Batch loss: 0.42067545652389526\n",
            "[Epoch 92] Batch loss: 0.5149965286254883\n",
            "[Epoch 92] Batch loss: 0.47956931591033936\n",
            "[Epoch 92] Batch loss: 0.569040834903717\n",
            "[Epoch 92] Batch loss: 0.44567856192588806\n",
            "[Epoch 92] Batch loss: 0.31444913148880005\n",
            "[Epoch 92] Batch loss: 0.35859215259552\n",
            "[Epoch 92] Batch loss: 0.42364633083343506\n",
            "[Epoch 92] Batch loss: 0.4557940363883972\n",
            "[Epoch 92] Batch loss: 0.35296621918678284\n",
            "[Epoch 92] Batch loss: 0.47078317403793335\n",
            "[Epoch 92] Batch loss: 0.48527035117149353\n",
            "[Epoch 92] Batch loss: 0.5575529336929321\n",
            "[Epoch 92] Batch loss: 0.4053441286087036\n",
            "[Epoch 92] Batch loss: 0.4868405759334564\n",
            "[Epoch 92] Batch loss: 0.47205355763435364\n",
            "[Epoch 92] Batch loss: 0.4400752782821655\n",
            "[Epoch 92] Batch loss: 0.488566130399704\n",
            "[Epoch 92] Batch loss: 0.48273372650146484\n",
            "[Epoch 92] Batch loss: 0.44862380623817444\n",
            "[Epoch 92] Batch loss: 0.4251379370689392\n",
            "[Epoch 92] Batch loss: 0.4665643274784088\n",
            "[Epoch 92] Batch loss: 0.3155396282672882\n",
            "[Epoch 92] Batch loss: 0.3997829854488373\n",
            "[Epoch 92] Batch loss: 0.46618255972862244\n",
            "[Epoch 92] Batch loss: 0.4340308904647827\n",
            "[Epoch 92] Batch loss: 0.37008389830589294\n",
            "[Epoch 92] Batch loss: 0.46824315190315247\n",
            "[Epoch 92] Batch loss: 0.4340840280056\n",
            "[Epoch 92] Batch loss: 0.48741933703422546\n",
            "[Epoch 92] Batch loss: 0.41466081142425537\n",
            "[Epoch 92] Batch loss: 0.42581525444984436\n",
            "[Epoch 92] Batch loss: 0.38072893023490906\n",
            "[Epoch 92] Batch loss: 0.38771942257881165\n",
            "[Epoch 92] Batch loss: 0.5578058362007141\n",
            "[Epoch 92] Batch loss: 0.4466099143028259\n",
            "[Epoch 92] Batch loss: 0.42596325278282166\n",
            "[Epoch 92] Batch loss: 0.40803295373916626\n",
            "[Epoch 92] Batch loss: 0.4146966338157654\n",
            "[Epoch 92] Batch loss: 0.44406718015670776\n",
            "[Epoch 92] Batch loss: 0.45574450492858887\n",
            "[Epoch 92] Batch loss: 0.398881733417511\n",
            "[Epoch 92] Batch loss: 0.4858964681625366\n",
            "[Epoch 92] Batch loss: 0.3943087160587311\n",
            "[Epoch 92] Batch loss: 0.3990109860897064\n",
            "[Epoch 92] Batch loss: 0.40240418910980225\n",
            "[Epoch 92] Batch loss: 0.517499566078186\n",
            "[Epoch 92] Batch loss: 0.4453698694705963\n",
            "[Epoch 92] Batch loss: 0.370834618806839\n",
            "[Epoch 92] Batch loss: 0.4400539696216583\n",
            "[Epoch 92] Batch loss: 0.553104043006897\n",
            "[Epoch 92] Batch loss: 0.5457908511161804\n",
            "[Epoch 92] Batch loss: 0.379660427570343\n",
            "[Epoch 92] Batch loss: 0.3983326852321625\n",
            "[Epoch 92] Batch loss: 0.4331454634666443\n",
            "[Epoch 92] Batch loss: 0.4285019338130951\n",
            "[Epoch 92] Batch loss: 0.416975736618042\n",
            "[Epoch 92] Batch loss: 0.46274465322494507\n",
            "[Epoch 92] Batch loss: 0.3818606734275818\n",
            "[Epoch 92] Batch loss: 0.3741573393344879\n",
            "[Epoch 92] Batch loss: 0.40606898069381714\n",
            "[Epoch 92] Batch loss: 0.3776569664478302\n",
            "[Epoch 92] Batch loss: 0.40765729546546936\n",
            "[Epoch 92] Batch loss: 0.3938332796096802\n",
            "[Epoch 92] Batch loss: 0.4032842814922333\n",
            "[Epoch 92] Batch loss: 0.4901048243045807\n",
            "[Epoch 92] Batch loss: 0.4519038498401642\n",
            "[Epoch 92] Batch loss: 0.3917209208011627\n",
            "[Epoch 92] Batch loss: 0.42209893465042114\n",
            "[Epoch 92] Batch loss: 0.3872440755367279\n",
            "[Epoch 92] Batch loss: 0.4586257040500641\n",
            "[Epoch 92] Batch loss: 0.432052880525589\n",
            "[Epoch 92] Batch loss: 0.5239156484603882\n",
            "[Epoch 92] Batch loss: 0.4097978174686432\n",
            "[Epoch 92] Batch loss: 0.4484345316886902\n",
            "[Epoch 92] Batch loss: 0.38712695240974426\n",
            "[Epoch 92] Batch loss: 0.4576469957828522\n",
            "[Epoch 92] Batch loss: 0.4686509668827057\n",
            "[Epoch 92] Batch loss: 0.3328342139720917\n",
            "[Epoch 92] Batch loss: 0.36346060037612915\n",
            "[Epoch 92] Batch loss: 0.45478788018226624\n",
            "[Epoch 92] Batch loss: 0.4025905132293701\n",
            "[Epoch 92] Batch loss: 0.5010063052177429\n",
            "[Epoch 92] Batch loss: 0.40992629528045654\n",
            "[Epoch 92] Batch loss: 0.4749145805835724\n",
            "[Epoch 92] Batch loss: 0.48445966839790344\n",
            "[Epoch 92] Batch loss: 0.372951477766037\n",
            "[Epoch 92] Batch loss: 0.40081995725631714\n",
            "[Epoch 92] Batch loss: 0.3505114018917084\n",
            "[Epoch 92] Batch loss: 0.5723245739936829\n",
            "[Epoch 92] Batch loss: 0.43550020456314087\n",
            "[Epoch 92] Batch loss: 0.4816778004169464\n",
            "[Epoch 92] Batch loss: 0.2901255488395691\n",
            "[Epoch 92] Batch loss: 0.44534027576446533\n",
            "[Epoch 92] Batch loss: 0.41240596771240234\n",
            "[Epoch 92] Batch loss: 0.397012859582901\n",
            "[Epoch 92] Batch loss: 0.4439620077610016\n",
            "[Epoch 92] Batch loss: 0.4921939969062805\n",
            "[Epoch 92] Batch loss: 0.46451467275619507\n",
            "[Epoch 92] Batch loss: 0.40528857707977295\n",
            "[Epoch 92] Batch loss: 0.35038331151008606\n",
            "[Epoch 92] Batch loss: 0.5552123188972473\n",
            "[Epoch 92] Batch loss: 0.3850981891155243\n",
            "[Epoch 92] Batch loss: 0.40515080094337463\n",
            "[Epoch 92] Batch loss: 0.37909820675849915\n",
            "[Epoch 92] Batch loss: 0.39951789379119873\n",
            "[Epoch 92] Batch loss: 0.4067244231700897\n",
            "[Epoch 92] Batch loss: 0.3984445631504059\n",
            "[Epoch 92] Batch loss: 0.412901908159256\n",
            "[Epoch 92] Batch loss: 0.42978042364120483\n",
            "[Epoch 92] Batch loss: 0.5144242644309998\n",
            "[Epoch 92] Batch loss: 0.5564632415771484\n",
            "[Epoch 92] Batch loss: 0.40061119198799133\n",
            "[Epoch 92] Batch loss: 0.46447864174842834\n",
            "[Epoch 92] Batch loss: 0.43091660737991333\n",
            "[Epoch 92] Batch loss: 0.4419631361961365\n",
            "[Epoch 92] Batch loss: 0.3121334910392761\n",
            "[Epoch 93/100] Val loss: 0.7492\n",
            "Best saved model.\n",
            "[Epoch 93] Batch loss: 0.41996636986732483\n",
            "[Epoch 93] Batch loss: 0.41925063729286194\n",
            "[Epoch 93] Batch loss: 0.3937273919582367\n",
            "[Epoch 93] Batch loss: 0.4954127073287964\n",
            "[Epoch 93] Batch loss: 0.3871035873889923\n",
            "[Epoch 93] Batch loss: 0.44317105412483215\n",
            "[Epoch 93] Batch loss: 0.40340355038642883\n",
            "[Epoch 93] Batch loss: 0.431203156709671\n",
            "[Epoch 93] Batch loss: 0.37367120385169983\n",
            "[Epoch 93] Batch loss: 0.41210371255874634\n",
            "[Epoch 93] Batch loss: 0.39455562829971313\n",
            "[Epoch 93] Batch loss: 0.4094889461994171\n",
            "[Epoch 93] Batch loss: 0.517501175403595\n",
            "[Epoch 93] Batch loss: 0.41009819507598877\n",
            "[Epoch 93] Batch loss: 0.42040613293647766\n",
            "[Epoch 93] Batch loss: 0.4257034659385681\n",
            "[Epoch 93] Batch loss: 0.42018651962280273\n",
            "[Epoch 93] Batch loss: 0.42642319202423096\n",
            "[Epoch 93] Batch loss: 0.464789479970932\n",
            "[Epoch 93] Batch loss: 0.49229177832603455\n",
            "[Epoch 93] Batch loss: 0.39403265714645386\n",
            "[Epoch 93] Batch loss: 0.4330384135246277\n",
            "[Epoch 93] Batch loss: 0.48736757040023804\n",
            "[Epoch 93] Batch loss: 0.48898816108703613\n",
            "[Epoch 93] Batch loss: 0.4809094965457916\n",
            "[Epoch 93] Batch loss: 0.39886823296546936\n",
            "[Epoch 93] Batch loss: 0.40037161111831665\n",
            "[Epoch 93] Batch loss: 0.4325791299343109\n",
            "[Epoch 93] Batch loss: 0.45414304733276367\n",
            "[Epoch 93] Batch loss: 0.41240760684013367\n",
            "[Epoch 93] Batch loss: 0.3809220492839813\n",
            "[Epoch 93] Batch loss: 0.44592446088790894\n",
            "[Epoch 93] Batch loss: 0.46118175983428955\n",
            "[Epoch 93] Batch loss: 0.3687688410282135\n",
            "[Epoch 93] Batch loss: 0.4267135560512543\n",
            "[Epoch 93] Batch loss: 0.37713930010795593\n",
            "[Epoch 93] Batch loss: 0.3908945620059967\n",
            "[Epoch 93] Batch loss: 0.4778287410736084\n",
            "[Epoch 93] Batch loss: 0.486675500869751\n",
            "[Epoch 93] Batch loss: 0.44728586077690125\n",
            "[Epoch 93] Batch loss: 0.4720524251461029\n",
            "[Epoch 93] Batch loss: 0.4877687692642212\n",
            "[Epoch 93] Batch loss: 0.4359055757522583\n",
            "[Epoch 93] Batch loss: 0.37626928091049194\n",
            "[Epoch 93] Batch loss: 0.4856656491756439\n",
            "[Epoch 93] Batch loss: 0.3759996294975281\n",
            "[Epoch 93] Batch loss: 0.436005175113678\n",
            "[Epoch 93] Batch loss: 0.47972372174263\n",
            "[Epoch 93] Batch loss: 0.3940906524658203\n",
            "[Epoch 93] Batch loss: 0.5636666417121887\n",
            "[Epoch 93] Batch loss: 0.4564960300922394\n",
            "[Epoch 93] Batch loss: 0.46513012051582336\n",
            "[Epoch 93] Batch loss: 0.48959338665008545\n",
            "[Epoch 93] Batch loss: 0.4217059910297394\n",
            "[Epoch 93] Batch loss: 0.3872358202934265\n",
            "[Epoch 93] Batch loss: 0.4389447867870331\n",
            "[Epoch 93] Batch loss: 0.4778349995613098\n",
            "[Epoch 93] Batch loss: 0.3898487985134125\n",
            "[Epoch 93] Batch loss: 0.41127288341522217\n",
            "[Epoch 93] Batch loss: 0.42738983035087585\n",
            "[Epoch 93] Batch loss: 0.484430193901062\n",
            "[Epoch 93] Batch loss: 0.4418961703777313\n",
            "[Epoch 93] Batch loss: 0.4252955913543701\n",
            "[Epoch 93] Batch loss: 0.44474470615386963\n",
            "[Epoch 93] Batch loss: 0.3753857910633087\n",
            "[Epoch 93] Batch loss: 0.43602439761161804\n",
            "[Epoch 93] Batch loss: 0.4607775807380676\n",
            "[Epoch 93] Batch loss: 0.39497247338294983\n",
            "[Epoch 93] Batch loss: 0.38455504179000854\n",
            "[Epoch 93] Batch loss: 0.4085444509983063\n",
            "[Epoch 93] Batch loss: 0.39082810282707214\n",
            "[Epoch 93] Batch loss: 0.4834616184234619\n",
            "[Epoch 93] Batch loss: 0.40357542037963867\n",
            "[Epoch 93] Batch loss: 0.42442551255226135\n",
            "[Epoch 93] Batch loss: 0.4190625846385956\n",
            "[Epoch 93] Batch loss: 0.4727991819381714\n",
            "[Epoch 93] Batch loss: 0.45950186252593994\n",
            "[Epoch 93] Batch loss: 0.36114639043807983\n",
            "[Epoch 93] Batch loss: 0.4478242099285126\n",
            "[Epoch 93] Batch loss: 0.4168481230735779\n",
            "[Epoch 93] Batch loss: 0.4865121841430664\n",
            "[Epoch 93] Batch loss: 0.461344838142395\n",
            "[Epoch 93] Batch loss: 0.42140117287635803\n",
            "[Epoch 93] Batch loss: 0.42971426248550415\n",
            "[Epoch 93] Batch loss: 0.3804653286933899\n",
            "[Epoch 93] Batch loss: 0.4181126356124878\n",
            "[Epoch 93] Batch loss: 0.4236598312854767\n",
            "[Epoch 93] Batch loss: 0.36169296503067017\n",
            "[Epoch 93] Batch loss: 0.46388787031173706\n",
            "[Epoch 93] Batch loss: 0.4172475337982178\n",
            "[Epoch 93] Batch loss: 0.4003269076347351\n",
            "[Epoch 93] Batch loss: 0.4380395710468292\n",
            "[Epoch 93] Batch loss: 0.4140225946903229\n",
            "[Epoch 93] Batch loss: 0.43550029397010803\n",
            "[Epoch 93] Batch loss: 0.4179949164390564\n",
            "[Epoch 93] Batch loss: 0.4048329293727875\n",
            "[Epoch 93] Batch loss: 0.43243134021759033\n",
            "[Epoch 93] Batch loss: 0.37974685430526733\n",
            "[Epoch 93] Batch loss: 0.5165708661079407\n",
            "[Epoch 93] Batch loss: 0.3809598386287689\n",
            "[Epoch 93] Batch loss: 0.4236789643764496\n",
            "[Epoch 93] Batch loss: 0.4039815068244934\n",
            "[Epoch 93] Batch loss: 0.4420756995677948\n",
            "[Epoch 93] Batch loss: 0.46633049845695496\n",
            "[Epoch 93] Batch loss: 0.4168489873409271\n",
            "[Epoch 93] Batch loss: 0.3947240710258484\n",
            "[Epoch 93] Batch loss: 0.3568924069404602\n",
            "[Epoch 93] Batch loss: 0.4446640610694885\n",
            "[Epoch 93] Batch loss: 0.3333541750907898\n",
            "[Epoch 93] Batch loss: 0.4101592004299164\n",
            "[Epoch 93] Batch loss: 0.43045833706855774\n",
            "[Epoch 93] Batch loss: 0.39646750688552856\n",
            "[Epoch 93] Batch loss: 0.42920398712158203\n",
            "[Epoch 93] Batch loss: 0.4932468831539154\n",
            "[Epoch 93] Batch loss: 0.46735116839408875\n",
            "[Epoch 93] Batch loss: 0.3654279112815857\n",
            "[Epoch 93] Batch loss: 0.4014858603477478\n",
            "[Epoch 93] Batch loss: 0.3862033784389496\n",
            "[Epoch 93] Batch loss: 0.4465550482273102\n",
            "[Epoch 93] Batch loss: 0.36834338307380676\n",
            "[Epoch 93] Batch loss: 0.38970866799354553\n",
            "[Epoch 93] Batch loss: 0.4733975827693939\n",
            "[Epoch 93] Batch loss: 0.40461817383766174\n",
            "[Epoch 93] Batch loss: 0.34076428413391113\n",
            "[Epoch 93] Batch loss: 0.4458772540092468\n",
            "[Epoch 93] Batch loss: 0.8965910077095032\n",
            "[Epoch 94/100] Val loss: 0.7595\n",
            "Epochs without improvement: 1/20\n",
            "[Epoch 94] Batch loss: 0.44433313608169556\n",
            "[Epoch 94] Batch loss: 0.3570929169654846\n",
            "[Epoch 94] Batch loss: 0.4016343653202057\n",
            "[Epoch 94] Batch loss: 0.411127507686615\n",
            "[Epoch 94] Batch loss: 0.3852498233318329\n",
            "[Epoch 94] Batch loss: 0.42941713333129883\n",
            "[Epoch 94] Batch loss: 0.3524092137813568\n",
            "[Epoch 94] Batch loss: 0.4636540114879608\n",
            "[Epoch 94] Batch loss: 0.425492525100708\n",
            "[Epoch 94] Batch loss: 0.36881065368652344\n",
            "[Epoch 94] Batch loss: 0.3690687119960785\n",
            "[Epoch 94] Batch loss: 0.43779683113098145\n",
            "[Epoch 94] Batch loss: 0.39531612396240234\n",
            "[Epoch 94] Batch loss: 0.4247629642486572\n",
            "[Epoch 94] Batch loss: 0.4102938175201416\n",
            "[Epoch 94] Batch loss: 0.44185370206832886\n",
            "[Epoch 94] Batch loss: 0.41331544518470764\n",
            "[Epoch 94] Batch loss: 0.4317625164985657\n",
            "[Epoch 94] Batch loss: 0.4888869822025299\n",
            "[Epoch 94] Batch loss: 0.4134117662906647\n",
            "[Epoch 94] Batch loss: 0.36043068766593933\n",
            "[Epoch 94] Batch loss: 0.45815688371658325\n",
            "[Epoch 94] Batch loss: 0.3531777560710907\n",
            "[Epoch 94] Batch loss: 0.39547035098075867\n",
            "[Epoch 94] Batch loss: 0.3989483118057251\n",
            "[Epoch 94] Batch loss: 0.43597427010536194\n",
            "[Epoch 94] Batch loss: 0.4457375109195709\n",
            "[Epoch 94] Batch loss: 0.543624997138977\n",
            "[Epoch 94] Batch loss: 0.4953806400299072\n",
            "[Epoch 94] Batch loss: 0.48145413398742676\n",
            "[Epoch 94] Batch loss: 0.41055962443351746\n",
            "[Epoch 94] Batch loss: 0.5175356268882751\n",
            "[Epoch 94] Batch loss: 0.5388541221618652\n",
            "[Epoch 94] Batch loss: 0.471530944108963\n",
            "[Epoch 94] Batch loss: 0.43566060066223145\n",
            "[Epoch 94] Batch loss: 0.4617237150669098\n",
            "[Epoch 94] Batch loss: 0.4696795344352722\n",
            "[Epoch 94] Batch loss: 0.45930758118629456\n",
            "[Epoch 94] Batch loss: 0.3961537480354309\n",
            "[Epoch 94] Batch loss: 0.4265376925468445\n",
            "[Epoch 94] Batch loss: 0.3646174371242523\n",
            "[Epoch 94] Batch loss: 0.3605842590332031\n",
            "[Epoch 94] Batch loss: 0.45100730657577515\n",
            "[Epoch 94] Batch loss: 0.32133960723876953\n",
            "[Epoch 94] Batch loss: 0.38941314816474915\n",
            "[Epoch 94] Batch loss: 0.41636669635772705\n",
            "[Epoch 94] Batch loss: 0.3614360988140106\n",
            "[Epoch 94] Batch loss: 0.42888790369033813\n",
            "[Epoch 94] Batch loss: 0.35897284746170044\n",
            "[Epoch 94] Batch loss: 0.4802403151988983\n",
            "[Epoch 94] Batch loss: 0.36561354994773865\n",
            "[Epoch 94] Batch loss: 0.38464778661727905\n",
            "[Epoch 94] Batch loss: 0.403817355632782\n",
            "[Epoch 94] Batch loss: 0.3590671420097351\n",
            "[Epoch 94] Batch loss: 0.4214697480201721\n",
            "[Epoch 94] Batch loss: 0.39988601207733154\n",
            "[Epoch 94] Batch loss: 0.47335878014564514\n",
            "[Epoch 94] Batch loss: 0.366113543510437\n",
            "[Epoch 94] Batch loss: 0.5221741199493408\n",
            "[Epoch 94] Batch loss: 0.3710227608680725\n",
            "[Epoch 94] Batch loss: 0.35672736167907715\n",
            "[Epoch 94] Batch loss: 0.3976125717163086\n",
            "[Epoch 94] Batch loss: 0.3886699080467224\n",
            "[Epoch 94] Batch loss: 0.5460894703865051\n",
            "[Epoch 94] Batch loss: 0.3887587785720825\n",
            "[Epoch 94] Batch loss: 0.4178176522254944\n",
            "[Epoch 94] Batch loss: 0.388886958360672\n",
            "[Epoch 94] Batch loss: 0.46086639165878296\n",
            "[Epoch 94] Batch loss: 0.342418909072876\n",
            "[Epoch 94] Batch loss: 0.40100333094596863\n",
            "[Epoch 94] Batch loss: 0.43841618299484253\n",
            "[Epoch 94] Batch loss: 0.4170578122138977\n",
            "[Epoch 94] Batch loss: 0.4395064115524292\n",
            "[Epoch 94] Batch loss: 0.4421401917934418\n",
            "[Epoch 94] Batch loss: 0.4192461669445038\n",
            "[Epoch 94] Batch loss: 0.49238115549087524\n",
            "[Epoch 94] Batch loss: 0.43422675132751465\n",
            "[Epoch 94] Batch loss: 0.3689514696598053\n",
            "[Epoch 94] Batch loss: 0.4802131950855255\n",
            "[Epoch 94] Batch loss: 0.4081569314002991\n",
            "[Epoch 94] Batch loss: 0.4628177881240845\n",
            "[Epoch 94] Batch loss: 0.5277707576751709\n",
            "[Epoch 94] Batch loss: 0.4446697533130646\n",
            "[Epoch 94] Batch loss: 0.5083154439926147\n",
            "[Epoch 94] Batch loss: 0.4431687295436859\n",
            "[Epoch 94] Batch loss: 0.4880005419254303\n",
            "[Epoch 94] Batch loss: 0.4145306348800659\n",
            "[Epoch 94] Batch loss: 0.4042060375213623\n",
            "[Epoch 94] Batch loss: 0.42977994680404663\n",
            "[Epoch 94] Batch loss: 0.33580124378204346\n",
            "[Epoch 94] Batch loss: 0.414119154214859\n",
            "[Epoch 94] Batch loss: 0.38794374465942383\n",
            "[Epoch 94] Batch loss: 0.42332130670547485\n",
            "[Epoch 94] Batch loss: 0.48288026452064514\n",
            "[Epoch 94] Batch loss: 0.4101858139038086\n",
            "[Epoch 94] Batch loss: 0.3865140676498413\n",
            "[Epoch 94] Batch loss: 0.4412789046764374\n",
            "[Epoch 94] Batch loss: 0.5405606627464294\n",
            "[Epoch 94] Batch loss: 0.409713476896286\n",
            "[Epoch 94] Batch loss: 0.4284854531288147\n",
            "[Epoch 94] Batch loss: 0.35138368606567383\n",
            "[Epoch 94] Batch loss: 0.5052908062934875\n",
            "[Epoch 94] Batch loss: 0.42619088292121887\n",
            "[Epoch 94] Batch loss: 0.4989003539085388\n",
            "[Epoch 94] Batch loss: 0.43205204606056213\n",
            "[Epoch 94] Batch loss: 0.36625319719314575\n",
            "[Epoch 94] Batch loss: 0.455858439207077\n",
            "[Epoch 94] Batch loss: 0.4955258369445801\n",
            "[Epoch 94] Batch loss: 0.5658679008483887\n",
            "[Epoch 94] Batch loss: 0.41150206327438354\n",
            "[Epoch 94] Batch loss: 0.37772825360298157\n",
            "[Epoch 94] Batch loss: 0.4044360816478729\n",
            "[Epoch 94] Batch loss: 0.41657331585884094\n",
            "[Epoch 94] Batch loss: 0.3672539293766022\n",
            "[Epoch 94] Batch loss: 0.42234599590301514\n",
            "[Epoch 94] Batch loss: 0.408795565366745\n",
            "[Epoch 94] Batch loss: 0.4210568368434906\n",
            "[Epoch 94] Batch loss: 0.4851512908935547\n",
            "[Epoch 94] Batch loss: 0.43255186080932617\n",
            "[Epoch 94] Batch loss: 0.46499428153038025\n",
            "[Epoch 94] Batch loss: 0.4747107923030853\n",
            "[Epoch 94] Batch loss: 0.5063031315803528\n",
            "[Epoch 94] Batch loss: 0.44619280099868774\n",
            "[Epoch 94] Batch loss: 0.6688438057899475\n",
            "[Epoch 94] Batch loss: 0.4305991530418396\n",
            "[Epoch 94] Batch loss: 0.5384861826896667\n",
            "[Epoch 95/100] Val loss: 0.7408\n",
            "Best saved model.\n",
            "[Epoch 95] Batch loss: 0.4221785366535187\n",
            "[Epoch 95] Batch loss: 0.4056611657142639\n",
            "[Epoch 95] Batch loss: 0.5150195956230164\n",
            "[Epoch 95] Batch loss: 0.3508993983268738\n",
            "[Epoch 95] Batch loss: 0.44320520758628845\n",
            "[Epoch 95] Batch loss: 0.40195637941360474\n",
            "[Epoch 95] Batch loss: 0.41591504216194153\n",
            "[Epoch 95] Batch loss: 0.4064334034919739\n",
            "[Epoch 95] Batch loss: 0.44398969411849976\n",
            "[Epoch 95] Batch loss: 0.3796471059322357\n",
            "[Epoch 95] Batch loss: 0.38583090901374817\n",
            "[Epoch 95] Batch loss: 0.3834044933319092\n",
            "[Epoch 95] Batch loss: 0.3652692437171936\n",
            "[Epoch 95] Batch loss: 0.4439265727996826\n",
            "[Epoch 95] Batch loss: 0.4758843183517456\n",
            "[Epoch 95] Batch loss: 0.48967990279197693\n",
            "[Epoch 95] Batch loss: 0.35442614555358887\n",
            "[Epoch 95] Batch loss: 0.3734534978866577\n",
            "[Epoch 95] Batch loss: 0.3593096137046814\n",
            "[Epoch 95] Batch loss: 0.4120479226112366\n",
            "[Epoch 95] Batch loss: 0.4212478995323181\n",
            "[Epoch 95] Batch loss: 0.4110921621322632\n",
            "[Epoch 95] Batch loss: 0.42965060472488403\n",
            "[Epoch 95] Batch loss: 0.44818878173828125\n",
            "[Epoch 95] Batch loss: 0.5867926478385925\n",
            "[Epoch 95] Batch loss: 0.4244556725025177\n",
            "[Epoch 95] Batch loss: 0.379049688577652\n",
            "[Epoch 95] Batch loss: 0.377763569355011\n",
            "[Epoch 95] Batch loss: 0.35102778673171997\n",
            "[Epoch 95] Batch loss: 0.4551635682582855\n",
            "[Epoch 95] Batch loss: 0.38604190945625305\n",
            "[Epoch 95] Batch loss: 0.5056143403053284\n",
            "[Epoch 95] Batch loss: 0.4028068482875824\n",
            "[Epoch 95] Batch loss: 0.3746242821216583\n",
            "[Epoch 95] Batch loss: 0.4208003580570221\n",
            "[Epoch 95] Batch loss: 0.43998682498931885\n",
            "[Epoch 95] Batch loss: 0.3771430552005768\n",
            "[Epoch 95] Batch loss: 0.4853358566761017\n",
            "[Epoch 95] Batch loss: 0.4870661199092865\n",
            "[Epoch 95] Batch loss: 0.4015144109725952\n",
            "[Epoch 95] Batch loss: 0.4375182092189789\n",
            "[Epoch 95] Batch loss: 0.4452471137046814\n",
            "[Epoch 95] Batch loss: 0.515093982219696\n",
            "[Epoch 95] Batch loss: 0.36913132667541504\n",
            "[Epoch 95] Batch loss: 0.43210676312446594\n",
            "[Epoch 95] Batch loss: 0.4220394194126129\n",
            "[Epoch 95] Batch loss: 0.35942116379737854\n",
            "[Epoch 95] Batch loss: 0.3794666826725006\n",
            "[Epoch 95] Batch loss: 0.48641860485076904\n",
            "[Epoch 95] Batch loss: 0.37595608830451965\n",
            "[Epoch 95] Batch loss: 0.40252748131752014\n",
            "[Epoch 95] Batch loss: 0.3986314535140991\n",
            "[Epoch 95] Batch loss: 0.44349849224090576\n",
            "[Epoch 95] Batch loss: 0.44176632165908813\n",
            "[Epoch 95] Batch loss: 0.39860257506370544\n",
            "[Epoch 95] Batch loss: 0.4133833944797516\n",
            "[Epoch 95] Batch loss: 0.45048582553863525\n",
            "[Epoch 95] Batch loss: 0.47257405519485474\n",
            "[Epoch 95] Batch loss: 0.405824214220047\n",
            "[Epoch 95] Batch loss: 0.3990817368030548\n",
            "[Epoch 95] Batch loss: 0.4824806749820709\n",
            "[Epoch 95] Batch loss: 0.39193448424339294\n",
            "[Epoch 95] Batch loss: 0.43793025612831116\n",
            "[Epoch 95] Batch loss: 0.469769150018692\n",
            "[Epoch 95] Batch loss: 0.4354120194911957\n",
            "[Epoch 95] Batch loss: 0.3556661009788513\n",
            "[Epoch 95] Batch loss: 0.43888574838638306\n",
            "[Epoch 95] Batch loss: 0.4191894233226776\n",
            "[Epoch 95] Batch loss: 0.3891405165195465\n",
            "[Epoch 95] Batch loss: 0.3963329493999481\n",
            "[Epoch 95] Batch loss: 0.4286225140094757\n",
            "[Epoch 95] Batch loss: 0.4607393145561218\n",
            "[Epoch 95] Batch loss: 0.37319618463516235\n",
            "[Epoch 95] Batch loss: 0.43842971324920654\n",
            "[Epoch 95] Batch loss: 0.4667831063270569\n",
            "[Epoch 95] Batch loss: 0.34159836173057556\n",
            "[Epoch 95] Batch loss: 0.35877394676208496\n",
            "[Epoch 95] Batch loss: 0.375735342502594\n",
            "[Epoch 95] Batch loss: 0.43436914682388306\n",
            "[Epoch 95] Batch loss: 0.4121500849723816\n",
            "[Epoch 95] Batch loss: 0.4453437030315399\n",
            "[Epoch 95] Batch loss: 0.380064457654953\n",
            "[Epoch 95] Batch loss: 0.44439762830734253\n",
            "[Epoch 95] Batch loss: 0.5082612633705139\n",
            "[Epoch 95] Batch loss: 0.4618113338947296\n",
            "[Epoch 95] Batch loss: 0.4600059688091278\n",
            "[Epoch 95] Batch loss: 0.4102751612663269\n",
            "[Epoch 95] Batch loss: 0.34583780169487\n",
            "[Epoch 95] Batch loss: 0.4257713556289673\n",
            "[Epoch 95] Batch loss: 0.47853484749794006\n",
            "[Epoch 95] Batch loss: 0.4709395170211792\n",
            "[Epoch 95] Batch loss: 0.4202815294265747\n",
            "[Epoch 95] Batch loss: 0.4222526550292969\n",
            "[Epoch 95] Batch loss: 0.37138280272483826\n",
            "[Epoch 95] Batch loss: 0.4442237317562103\n",
            "[Epoch 95] Batch loss: 0.47894978523254395\n",
            "[Epoch 95] Batch loss: 0.41356605291366577\n",
            "[Epoch 95] Batch loss: 0.3610585033893585\n",
            "[Epoch 95] Batch loss: 0.36322587728500366\n",
            "[Epoch 95] Batch loss: 0.41350364685058594\n",
            "[Epoch 95] Batch loss: 0.4138263463973999\n",
            "[Epoch 95] Batch loss: 0.4328618049621582\n",
            "[Epoch 95] Batch loss: 0.39119765162467957\n",
            "[Epoch 95] Batch loss: 0.4779154360294342\n",
            "[Epoch 95] Batch loss: 0.3945373296737671\n",
            "[Epoch 95] Batch loss: 0.44187068939208984\n",
            "[Epoch 95] Batch loss: 0.4501207172870636\n",
            "[Epoch 95] Batch loss: 0.4034898579120636\n",
            "[Epoch 95] Batch loss: 0.4612295627593994\n",
            "[Epoch 95] Batch loss: 0.4035530090332031\n",
            "[Epoch 95] Batch loss: 0.47455713152885437\n",
            "[Epoch 95] Batch loss: 0.3994383215904236\n",
            "[Epoch 95] Batch loss: 0.43123510479927063\n",
            "[Epoch 95] Batch loss: 0.36154109239578247\n",
            "[Epoch 95] Batch loss: 0.5116454362869263\n",
            "[Epoch 95] Batch loss: 0.4019278287887573\n",
            "[Epoch 95] Batch loss: 0.3529411256313324\n",
            "[Epoch 95] Batch loss: 0.4334249794483185\n",
            "[Epoch 95] Batch loss: 0.4191703796386719\n",
            "[Epoch 95] Batch loss: 0.353191614151001\n",
            "[Epoch 95] Batch loss: 0.37515661120414734\n",
            "[Epoch 95] Batch loss: 0.347225159406662\n",
            "[Epoch 95] Batch loss: 0.45158642530441284\n",
            "[Epoch 95] Batch loss: 0.41912463307380676\n",
            "[Epoch 95] Batch loss: 0.43018925189971924\n",
            "[Epoch 95] Batch loss: 0.3892304599285126\n",
            "[Epoch 96/100] Val loss: 0.7370\n",
            "Best saved model.\n",
            "[Epoch 96] Batch loss: 0.3993975520133972\n",
            "[Epoch 96] Batch loss: 0.441964328289032\n",
            "[Epoch 96] Batch loss: 0.44327205419540405\n",
            "[Epoch 96] Batch loss: 0.3948923647403717\n",
            "[Epoch 96] Batch loss: 0.34410712122917175\n",
            "[Epoch 96] Batch loss: 0.33795619010925293\n",
            "[Epoch 96] Batch loss: 0.4120665192604065\n",
            "[Epoch 96] Batch loss: 0.417601078748703\n",
            "[Epoch 96] Batch loss: 0.45132628083229065\n",
            "[Epoch 96] Batch loss: 0.38857999444007874\n",
            "[Epoch 96] Batch loss: 0.40951007604599\n",
            "[Epoch 96] Batch loss: 0.4005453884601593\n",
            "[Epoch 96] Batch loss: 0.43277275562286377\n",
            "[Epoch 96] Batch loss: 0.4140530526638031\n",
            "[Epoch 96] Batch loss: 0.38211923837661743\n",
            "[Epoch 96] Batch loss: 0.4296407997608185\n",
            "[Epoch 96] Batch loss: 0.3811470568180084\n",
            "[Epoch 96] Batch loss: 0.49901923537254333\n",
            "[Epoch 96] Batch loss: 0.5280999541282654\n",
            "[Epoch 96] Batch loss: 0.46296820044517517\n",
            "[Epoch 96] Batch loss: 0.41026046872138977\n",
            "[Epoch 96] Batch loss: 0.3771905303001404\n",
            "[Epoch 96] Batch loss: 0.40898397564888\n",
            "[Epoch 96] Batch loss: 0.3089509606361389\n",
            "[Epoch 96] Batch loss: 0.4552244544029236\n",
            "[Epoch 96] Batch loss: 0.4045320451259613\n",
            "[Epoch 96] Batch loss: 0.4761611819267273\n",
            "[Epoch 96] Batch loss: 0.33610859513282776\n",
            "[Epoch 96] Batch loss: 0.564467191696167\n",
            "[Epoch 96] Batch loss: 0.4474905729293823\n",
            "[Epoch 96] Batch loss: 0.44691014289855957\n",
            "[Epoch 96] Batch loss: 0.3130078911781311\n",
            "[Epoch 96] Batch loss: 0.4086330831050873\n",
            "[Epoch 96] Batch loss: 0.3198952376842499\n",
            "[Epoch 96] Batch loss: 0.47103095054626465\n",
            "[Epoch 96] Batch loss: 0.3865453898906708\n",
            "[Epoch 96] Batch loss: 0.4269108176231384\n",
            "[Epoch 96] Batch loss: 0.5348556041717529\n",
            "[Epoch 96] Batch loss: 0.5252742171287537\n",
            "[Epoch 96] Batch loss: 0.457588791847229\n",
            "[Epoch 96] Batch loss: 0.3517724275588989\n",
            "[Epoch 96] Batch loss: 0.4010576605796814\n",
            "[Epoch 96] Batch loss: 0.4566882848739624\n",
            "[Epoch 96] Batch loss: 0.4491470158100128\n",
            "[Epoch 96] Batch loss: 0.42119425535202026\n",
            "[Epoch 96] Batch loss: 0.41825392842292786\n",
            "[Epoch 96] Batch loss: 0.45174476504325867\n",
            "[Epoch 96] Batch loss: 0.3366643488407135\n",
            "[Epoch 96] Batch loss: 0.3866977393627167\n",
            "[Epoch 96] Batch loss: 0.4141819477081299\n",
            "[Epoch 96] Batch loss: 0.43586108088493347\n",
            "[Epoch 96] Batch loss: 0.3791871964931488\n",
            "[Epoch 96] Batch loss: 0.41949737071990967\n",
            "[Epoch 96] Batch loss: 0.42426666617393494\n",
            "[Epoch 96] Batch loss: 0.4350389540195465\n",
            "[Epoch 96] Batch loss: 0.5086851119995117\n",
            "[Epoch 96] Batch loss: 0.4144030213356018\n",
            "[Epoch 96] Batch loss: 0.4180279076099396\n",
            "[Epoch 96] Batch loss: 0.3813270926475525\n",
            "[Epoch 96] Batch loss: 0.40043115615844727\n",
            "[Epoch 96] Batch loss: 0.46237659454345703\n",
            "[Epoch 96] Batch loss: 0.42125365138053894\n",
            "[Epoch 96] Batch loss: 0.376074880361557\n",
            "[Epoch 96] Batch loss: 0.34299972653388977\n",
            "[Epoch 96] Batch loss: 0.4040601849555969\n",
            "[Epoch 96] Batch loss: 0.4360286295413971\n",
            "[Epoch 96] Batch loss: 0.3994985818862915\n",
            "[Epoch 96] Batch loss: 0.35488471388816833\n",
            "[Epoch 96] Batch loss: 0.4479832351207733\n",
            "[Epoch 96] Batch loss: 0.4196875989437103\n",
            "[Epoch 96] Batch loss: 0.44929248094558716\n",
            "[Epoch 96] Batch loss: 0.4171923100948334\n",
            "[Epoch 96] Batch loss: 0.4882999658584595\n",
            "[Epoch 96] Batch loss: 0.3553802967071533\n",
            "[Epoch 96] Batch loss: 0.43086525797843933\n",
            "[Epoch 96] Batch loss: 0.4403541684150696\n",
            "[Epoch 96] Batch loss: 0.39510032534599304\n",
            "[Epoch 96] Batch loss: 0.3670724332332611\n",
            "[Epoch 96] Batch loss: 0.4228658974170685\n",
            "[Epoch 96] Batch loss: 0.507662832736969\n",
            "[Epoch 96] Batch loss: 0.3626863658428192\n",
            "[Epoch 96] Batch loss: 0.4080853760242462\n",
            "[Epoch 96] Batch loss: 0.4157331883907318\n",
            "[Epoch 96] Batch loss: 0.4248129427433014\n",
            "[Epoch 96] Batch loss: 0.3241749405860901\n",
            "[Epoch 96] Batch loss: 0.4255940616130829\n",
            "[Epoch 96] Batch loss: 0.38369685411453247\n",
            "[Epoch 96] Batch loss: 0.4070795774459839\n",
            "[Epoch 96] Batch loss: 0.42096424102783203\n",
            "[Epoch 96] Batch loss: 0.4484693706035614\n",
            "[Epoch 96] Batch loss: 0.365340918302536\n",
            "[Epoch 96] Batch loss: 0.48058128356933594\n",
            "[Epoch 96] Batch loss: 0.481301486492157\n",
            "[Epoch 96] Batch loss: 0.353268027305603\n",
            "[Epoch 96] Batch loss: 0.4028118848800659\n",
            "[Epoch 96] Batch loss: 0.42052677273750305\n",
            "[Epoch 96] Batch loss: 0.36166587471961975\n",
            "[Epoch 96] Batch loss: 0.40833622217178345\n",
            "[Epoch 96] Batch loss: 0.40500015020370483\n",
            "[Epoch 96] Batch loss: 0.41806188225746155\n",
            "[Epoch 96] Batch loss: 0.3427466154098511\n",
            "[Epoch 96] Batch loss: 0.3997753858566284\n",
            "[Epoch 96] Batch loss: 0.3940163552761078\n",
            "[Epoch 96] Batch loss: 0.4514749050140381\n",
            "[Epoch 96] Batch loss: 0.4479081928730011\n",
            "[Epoch 96] Batch loss: 0.3567388653755188\n",
            "[Epoch 96] Batch loss: 0.3849638104438782\n",
            "[Epoch 96] Batch loss: 0.48451000452041626\n",
            "[Epoch 96] Batch loss: 0.3610430657863617\n",
            "[Epoch 96] Batch loss: 0.3810674548149109\n",
            "[Epoch 96] Batch loss: 0.399753600358963\n",
            "[Epoch 96] Batch loss: 0.4758293628692627\n",
            "[Epoch 96] Batch loss: 0.4337364733219147\n",
            "[Epoch 96] Batch loss: 0.40209755301475525\n",
            "[Epoch 96] Batch loss: 0.3999154567718506\n",
            "[Epoch 96] Batch loss: 0.44209253787994385\n",
            "[Epoch 96] Batch loss: 0.38973715901374817\n",
            "[Epoch 96] Batch loss: 0.4969920217990875\n",
            "[Epoch 96] Batch loss: 0.38708654046058655\n",
            "[Epoch 96] Batch loss: 0.45097753405570984\n",
            "[Epoch 96] Batch loss: 0.41542354226112366\n",
            "[Epoch 96] Batch loss: 0.45245689153671265\n",
            "[Epoch 96] Batch loss: 0.3634445369243622\n",
            "[Epoch 96] Batch loss: 0.3843797743320465\n",
            "[Epoch 96] Batch loss: 0.3804623782634735\n",
            "[Epoch 96] Batch loss: 0.4345461130142212\n",
            "[Epoch 97/100] Val loss: 0.7379\n",
            "Epochs without improvement: 1/20\n",
            "[Epoch 97] Batch loss: 0.42417994141578674\n",
            "[Epoch 97] Batch loss: 0.3894822597503662\n",
            "[Epoch 97] Batch loss: 0.34042343497276306\n",
            "[Epoch 97] Batch loss: 0.4352899193763733\n",
            "[Epoch 97] Batch loss: 0.40164995193481445\n",
            "[Epoch 97] Batch loss: 0.4187210202217102\n",
            "[Epoch 97] Batch loss: 0.4566451609134674\n",
            "[Epoch 97] Batch loss: 0.49979299306869507\n",
            "[Epoch 97] Batch loss: 0.413505494594574\n",
            "[Epoch 97] Batch loss: 0.4152124524116516\n",
            "[Epoch 97] Batch loss: 0.4288322925567627\n",
            "[Epoch 97] Batch loss: 0.3916831612586975\n",
            "[Epoch 97] Batch loss: 0.5049164891242981\n",
            "[Epoch 97] Batch loss: 0.4402829110622406\n",
            "[Epoch 97] Batch loss: 0.39621466398239136\n",
            "[Epoch 97] Batch loss: 0.380630224943161\n",
            "[Epoch 97] Batch loss: 0.42372894287109375\n",
            "[Epoch 97] Batch loss: 0.3876708149909973\n",
            "[Epoch 97] Batch loss: 0.3793150782585144\n",
            "[Epoch 97] Batch loss: 0.39448627829551697\n",
            "[Epoch 97] Batch loss: 0.4077658951282501\n",
            "[Epoch 97] Batch loss: 0.37962809205055237\n",
            "[Epoch 97] Batch loss: 0.42775774002075195\n",
            "[Epoch 97] Batch loss: 0.4249221682548523\n",
            "[Epoch 97] Batch loss: 0.37351885437965393\n",
            "[Epoch 97] Batch loss: 0.38097742199897766\n",
            "[Epoch 97] Batch loss: 0.538028359413147\n",
            "[Epoch 97] Batch loss: 0.39962708950042725\n",
            "[Epoch 97] Batch loss: 0.40043631196022034\n",
            "[Epoch 97] Batch loss: 0.4673307538032532\n",
            "[Epoch 97] Batch loss: 0.34784752130508423\n",
            "[Epoch 97] Batch loss: 0.4064734876155853\n",
            "[Epoch 97] Batch loss: 0.3681952655315399\n",
            "[Epoch 97] Batch loss: 0.38535577058792114\n",
            "[Epoch 97] Batch loss: 0.4112889766693115\n",
            "[Epoch 97] Batch loss: 0.45470014214515686\n",
            "[Epoch 97] Batch loss: 0.40477100014686584\n",
            "[Epoch 97] Batch loss: 0.3488365113735199\n",
            "[Epoch 97] Batch loss: 0.34045061469078064\n",
            "[Epoch 97] Batch loss: 0.49275076389312744\n",
            "[Epoch 97] Batch loss: 0.34875890612602234\n",
            "[Epoch 97] Batch loss: 0.4014166593551636\n",
            "[Epoch 97] Batch loss: 0.4880988895893097\n",
            "[Epoch 97] Batch loss: 0.353263258934021\n",
            "[Epoch 97] Batch loss: 0.4554170072078705\n",
            "[Epoch 97] Batch loss: 0.47021692991256714\n",
            "[Epoch 97] Batch loss: 0.42910563945770264\n",
            "[Epoch 97] Batch loss: 0.44089043140411377\n",
            "[Epoch 97] Batch loss: 0.3976917564868927\n",
            "[Epoch 97] Batch loss: 0.39363113045692444\n",
            "[Epoch 97] Batch loss: 0.42827022075653076\n",
            "[Epoch 97] Batch loss: 0.3538073003292084\n",
            "[Epoch 97] Batch loss: 0.41527625918388367\n",
            "[Epoch 97] Batch loss: 0.4056580662727356\n",
            "[Epoch 97] Batch loss: 0.3920394778251648\n",
            "[Epoch 97] Batch loss: 0.4626883864402771\n",
            "[Epoch 97] Batch loss: 0.4568280875682831\n",
            "[Epoch 97] Batch loss: 0.4128720164299011\n",
            "[Epoch 97] Batch loss: 0.4864186942577362\n",
            "[Epoch 97] Batch loss: 0.4385288655757904\n",
            "[Epoch 97] Batch loss: 0.4007866084575653\n",
            "[Epoch 97] Batch loss: 0.42866194248199463\n",
            "[Epoch 97] Batch loss: 0.3758929669857025\n",
            "[Epoch 97] Batch loss: 0.3880615234375\n",
            "[Epoch 97] Batch loss: 0.39483070373535156\n",
            "[Epoch 97] Batch loss: 0.40885770320892334\n",
            "[Epoch 97] Batch loss: 0.3852725028991699\n",
            "[Epoch 97] Batch loss: 0.2915331721305847\n",
            "[Epoch 97] Batch loss: 0.4516676664352417\n",
            "[Epoch 97] Batch loss: 0.4588337540626526\n",
            "[Epoch 97] Batch loss: 0.4133385717868805\n",
            "[Epoch 97] Batch loss: 0.43442535400390625\n",
            "[Epoch 97] Batch loss: 0.4322602152824402\n",
            "[Epoch 97] Batch loss: 0.37621667981147766\n",
            "[Epoch 97] Batch loss: 0.41665711998939514\n",
            "[Epoch 97] Batch loss: 0.4228854477405548\n",
            "[Epoch 97] Batch loss: 0.48212894797325134\n",
            "[Epoch 97] Batch loss: 0.4138343632221222\n",
            "[Epoch 97] Batch loss: 0.4562755227088928\n",
            "[Epoch 97] Batch loss: 0.39251503348350525\n",
            "[Epoch 97] Batch loss: 0.4198358356952667\n",
            "[Epoch 97] Batch loss: 0.4064073860645294\n",
            "[Epoch 97] Batch loss: 0.385837584733963\n",
            "[Epoch 97] Batch loss: 0.40061140060424805\n",
            "[Epoch 97] Batch loss: 0.38449278473854065\n",
            "[Epoch 97] Batch loss: 0.4489748477935791\n",
            "[Epoch 97] Batch loss: 0.3969595432281494\n",
            "[Epoch 97] Batch loss: 0.47278717160224915\n",
            "[Epoch 97] Batch loss: 0.45900580286979675\n",
            "[Epoch 97] Batch loss: 0.39706528186798096\n",
            "[Epoch 97] Batch loss: 0.37645283341407776\n",
            "[Epoch 97] Batch loss: 0.4767896234989166\n",
            "[Epoch 97] Batch loss: 0.4097369313240051\n",
            "[Epoch 97] Batch loss: 0.4176865220069885\n",
            "[Epoch 97] Batch loss: 0.3407353460788727\n",
            "[Epoch 97] Batch loss: 0.4811294674873352\n",
            "[Epoch 97] Batch loss: 0.3959505259990692\n",
            "[Epoch 97] Batch loss: 0.39326080679893494\n",
            "[Epoch 97] Batch loss: 0.3660709857940674\n",
            "[Epoch 97] Batch loss: 0.504305362701416\n",
            "[Epoch 97] Batch loss: 0.38858261704444885\n",
            "[Epoch 97] Batch loss: 0.4644296169281006\n",
            "[Epoch 97] Batch loss: 0.43231260776519775\n",
            "[Epoch 97] Batch loss: 0.38432201743125916\n",
            "[Epoch 97] Batch loss: 0.38454481959342957\n",
            "[Epoch 97] Batch loss: 0.42564165592193604\n",
            "[Epoch 97] Batch loss: 0.49274080991744995\n",
            "[Epoch 97] Batch loss: 0.4693584144115448\n",
            "[Epoch 97] Batch loss: 0.41458770632743835\n",
            "[Epoch 97] Batch loss: 0.4213317930698395\n",
            "[Epoch 97] Batch loss: 0.4253402054309845\n",
            "[Epoch 97] Batch loss: 0.40446585416793823\n",
            "[Epoch 97] Batch loss: 0.382432758808136\n",
            "[Epoch 97] Batch loss: 0.45298150181770325\n",
            "[Epoch 97] Batch loss: 0.4420606195926666\n",
            "[Epoch 97] Batch loss: 0.4926837980747223\n",
            "[Epoch 97] Batch loss: 0.45797067880630493\n",
            "[Epoch 97] Batch loss: 0.48024120926856995\n",
            "[Epoch 97] Batch loss: 0.36446627974510193\n",
            "[Epoch 97] Batch loss: 0.44555458426475525\n",
            "[Epoch 97] Batch loss: 0.4661358892917633\n",
            "[Epoch 97] Batch loss: 0.36652371287345886\n",
            "[Epoch 97] Batch loss: 0.3643508553504944\n",
            "[Epoch 97] Batch loss: 0.3117564022541046\n",
            "[Epoch 97] Batch loss: 0.42431148886680603\n",
            "[Epoch 97] Batch loss: 0.44802069664001465\n",
            "[Epoch 98/100] Val loss: 0.7311\n",
            "Best saved model.\n",
            "[Epoch 98] Batch loss: 0.3855525553226471\n",
            "[Epoch 98] Batch loss: 0.4828546643257141\n",
            "[Epoch 98] Batch loss: 0.4388565123081207\n",
            "[Epoch 98] Batch loss: 0.48897650837898254\n",
            "[Epoch 98] Batch loss: 0.4031843841075897\n",
            "[Epoch 98] Batch loss: 0.43720293045043945\n",
            "[Epoch 98] Batch loss: 0.3385789692401886\n",
            "[Epoch 98] Batch loss: 0.4050902724266052\n",
            "[Epoch 98] Batch loss: 0.45321357250213623\n",
            "[Epoch 98] Batch loss: 0.41066452860832214\n",
            "[Epoch 98] Batch loss: 0.3737471401691437\n",
            "[Epoch 98] Batch loss: 0.39333203434944153\n",
            "[Epoch 98] Batch loss: 0.4044901430606842\n",
            "[Epoch 98] Batch loss: 0.41439032554626465\n",
            "[Epoch 98] Batch loss: 0.4027797281742096\n",
            "[Epoch 98] Batch loss: 0.3529318869113922\n",
            "[Epoch 98] Batch loss: 0.369460791349411\n",
            "[Epoch 98] Batch loss: 0.4907107949256897\n",
            "[Epoch 98] Batch loss: 0.4267479181289673\n",
            "[Epoch 98] Batch loss: 0.4738317131996155\n",
            "[Epoch 98] Batch loss: 0.4193325340747833\n",
            "[Epoch 98] Batch loss: 0.4281150996685028\n",
            "[Epoch 98] Batch loss: 0.36599260568618774\n",
            "[Epoch 98] Batch loss: 0.40869882702827454\n",
            "[Epoch 98] Batch loss: 0.4923218786716461\n",
            "[Epoch 98] Batch loss: 0.43272557854652405\n",
            "[Epoch 98] Batch loss: 0.4293234050273895\n",
            "[Epoch 98] Batch loss: 0.4288228154182434\n",
            "[Epoch 98] Batch loss: 0.4016644060611725\n",
            "[Epoch 98] Batch loss: 0.40922895073890686\n",
            "[Epoch 98] Batch loss: 0.4582585096359253\n",
            "[Epoch 98] Batch loss: 0.465204656124115\n",
            "[Epoch 98] Batch loss: 0.4463233947753906\n",
            "[Epoch 98] Batch loss: 0.4123648405075073\n",
            "[Epoch 98] Batch loss: 0.29817354679107666\n",
            "[Epoch 98] Batch loss: 0.5285206437110901\n",
            "[Epoch 98] Batch loss: 0.32971811294555664\n",
            "[Epoch 98] Batch loss: 0.3303177058696747\n",
            "[Epoch 98] Batch loss: 0.4577794373035431\n",
            "[Epoch 98] Batch loss: 0.4483509361743927\n",
            "[Epoch 98] Batch loss: 0.38947662711143494\n",
            "[Epoch 98] Batch loss: 0.37257257103919983\n",
            "[Epoch 98] Batch loss: 0.3825235366821289\n",
            "[Epoch 98] Batch loss: 0.451665997505188\n",
            "[Epoch 98] Batch loss: 0.4607568681240082\n",
            "[Epoch 98] Batch loss: 0.39325663447380066\n",
            "[Epoch 98] Batch loss: 0.37270134687423706\n",
            "[Epoch 98] Batch loss: 0.3942559063434601\n",
            "[Epoch 98] Batch loss: 0.3553459346294403\n",
            "[Epoch 98] Batch loss: 0.4449104964733124\n",
            "[Epoch 98] Batch loss: 0.40473324060440063\n",
            "[Epoch 98] Batch loss: 0.4847883880138397\n",
            "[Epoch 98] Batch loss: 0.3939056694507599\n",
            "[Epoch 98] Batch loss: 0.4093117117881775\n",
            "[Epoch 98] Batch loss: 0.45655953884124756\n",
            "[Epoch 98] Batch loss: 0.41848430037498474\n",
            "[Epoch 98] Batch loss: 0.37455737590789795\n",
            "[Epoch 98] Batch loss: 0.39773598313331604\n",
            "[Epoch 98] Batch loss: 0.40318483114242554\n",
            "[Epoch 98] Batch loss: 0.44869738817214966\n",
            "[Epoch 98] Batch loss: 0.43803179264068604\n",
            "[Epoch 98] Batch loss: 0.40805336833000183\n",
            "[Epoch 98] Batch loss: 0.3872634172439575\n",
            "[Epoch 98] Batch loss: 0.35202956199645996\n",
            "[Epoch 98] Batch loss: 0.36415478587150574\n",
            "[Epoch 98] Batch loss: 0.3466821312904358\n",
            "[Epoch 98] Batch loss: 0.49224504828453064\n",
            "[Epoch 98] Batch loss: 0.381846159696579\n",
            "[Epoch 98] Batch loss: 0.39480316638946533\n",
            "[Epoch 98] Batch loss: 0.44255635142326355\n",
            "[Epoch 98] Batch loss: 0.4335559904575348\n",
            "[Epoch 98] Batch loss: 0.38989314436912537\n",
            "[Epoch 98] Batch loss: 0.3970029056072235\n",
            "[Epoch 98] Batch loss: 0.46672555804252625\n",
            "[Epoch 98] Batch loss: 0.42139825224876404\n",
            "[Epoch 98] Batch loss: 0.38792264461517334\n",
            "[Epoch 98] Batch loss: 0.3851701319217682\n",
            "[Epoch 98] Batch loss: 0.38901519775390625\n",
            "[Epoch 98] Batch loss: 0.4064823389053345\n",
            "[Epoch 98] Batch loss: 0.37064921855926514\n",
            "[Epoch 98] Batch loss: 0.43166860938072205\n",
            "[Epoch 98] Batch loss: 0.3313462734222412\n",
            "[Epoch 98] Batch loss: 0.3718886077404022\n",
            "[Epoch 98] Batch loss: 0.47497668862342834\n",
            "[Epoch 98] Batch loss: 0.48324012756347656\n",
            "[Epoch 98] Batch loss: 0.38929319381713867\n",
            "[Epoch 98] Batch loss: 0.35205793380737305\n",
            "[Epoch 98] Batch loss: 0.4193073809146881\n",
            "[Epoch 98] Batch loss: 0.3755997121334076\n",
            "[Epoch 98] Batch loss: 0.605298638343811\n",
            "[Epoch 98] Batch loss: 0.4433630406856537\n",
            "[Epoch 98] Batch loss: 0.43889662623405457\n",
            "[Epoch 98] Batch loss: 0.476489394903183\n",
            "[Epoch 98] Batch loss: 0.4286960959434509\n",
            "[Epoch 98] Batch loss: 0.3670463562011719\n",
            "[Epoch 98] Batch loss: 0.4200708568096161\n",
            "[Epoch 98] Batch loss: 0.38149014115333557\n",
            "[Epoch 98] Batch loss: 0.39022383093833923\n",
            "[Epoch 98] Batch loss: 0.40875929594039917\n",
            "[Epoch 98] Batch loss: 0.4204842150211334\n",
            "[Epoch 98] Batch loss: 0.3974951505661011\n",
            "[Epoch 98] Batch loss: 0.33157816529273987\n",
            "[Epoch 98] Batch loss: 0.3801482021808624\n",
            "[Epoch 98] Batch loss: 0.41573119163513184\n",
            "[Epoch 98] Batch loss: 0.44620516896247864\n",
            "[Epoch 98] Batch loss: 0.44609612226486206\n",
            "[Epoch 98] Batch loss: 0.36066025495529175\n",
            "[Epoch 98] Batch loss: 0.42962342500686646\n",
            "[Epoch 98] Batch loss: 0.45924925804138184\n",
            "[Epoch 98] Batch loss: 0.4124646782875061\n",
            "[Epoch 98] Batch loss: 0.41888681054115295\n",
            "[Epoch 98] Batch loss: 0.43854987621307373\n",
            "[Epoch 98] Batch loss: 0.3468363583087921\n",
            "[Epoch 98] Batch loss: 0.4178425073623657\n",
            "[Epoch 98] Batch loss: 0.5161340832710266\n",
            "[Epoch 98] Batch loss: 0.4911038875579834\n",
            "[Epoch 98] Batch loss: 0.4638931453227997\n",
            "[Epoch 98] Batch loss: 0.34369879961013794\n",
            "[Epoch 98] Batch loss: 0.38688334822654724\n",
            "[Epoch 98] Batch loss: 0.34072792530059814\n",
            "[Epoch 98] Batch loss: 0.45958152413368225\n",
            "[Epoch 98] Batch loss: 0.39629071950912476\n",
            "[Epoch 98] Batch loss: 0.3572934865951538\n",
            "[Epoch 98] Batch loss: 0.3693917989730835\n",
            "[Epoch 98] Batch loss: 0.47414183616638184\n",
            "[Epoch 98] Batch loss: 0.4326635003089905\n",
            "[Epoch 99/100] Val loss: 0.7302\n",
            "Best saved model.\n",
            "[Epoch 99] Batch loss: 0.389553040266037\n",
            "[Epoch 99] Batch loss: 0.4648166298866272\n",
            "[Epoch 99] Batch loss: 0.3557991087436676\n",
            "[Epoch 99] Batch loss: 0.3470240831375122\n",
            "[Epoch 99] Batch loss: 0.446147084236145\n",
            "[Epoch 99] Batch loss: 0.40581440925598145\n",
            "[Epoch 99] Batch loss: 0.40417298674583435\n",
            "[Epoch 99] Batch loss: 0.4747566878795624\n",
            "[Epoch 99] Batch loss: 0.4439806044101715\n",
            "[Epoch 99] Batch loss: 0.3883168697357178\n",
            "[Epoch 99] Batch loss: 0.3521192967891693\n",
            "[Epoch 99] Batch loss: 0.45804259181022644\n",
            "[Epoch 99] Batch loss: 0.3950592577457428\n",
            "[Epoch 99] Batch loss: 0.4667384624481201\n",
            "[Epoch 99] Batch loss: 0.3554689586162567\n",
            "[Epoch 99] Batch loss: 0.3634130656719208\n",
            "[Epoch 99] Batch loss: 0.4134337604045868\n",
            "[Epoch 99] Batch loss: 0.38934990763664246\n",
            "[Epoch 99] Batch loss: 0.38157880306243896\n",
            "[Epoch 99] Batch loss: 0.40605759620666504\n",
            "[Epoch 99] Batch loss: 0.37306714057922363\n",
            "[Epoch 99] Batch loss: 0.48827895522117615\n",
            "[Epoch 99] Batch loss: 0.37061548233032227\n",
            "[Epoch 99] Batch loss: 0.4248462915420532\n",
            "[Epoch 99] Batch loss: 0.3400401771068573\n",
            "[Epoch 99] Batch loss: 0.3642681837081909\n",
            "[Epoch 99] Batch loss: 0.45713484287261963\n",
            "[Epoch 99] Batch loss: 0.4795517921447754\n",
            "[Epoch 99] Batch loss: 0.39632871747016907\n",
            "[Epoch 99] Batch loss: 0.4585714340209961\n",
            "[Epoch 99] Batch loss: 0.42158493399620056\n",
            "[Epoch 99] Batch loss: 0.4615817368030548\n",
            "[Epoch 99] Batch loss: 0.3841174244880676\n",
            "[Epoch 99] Batch loss: 0.42691266536712646\n",
            "[Epoch 99] Batch loss: 0.3711693584918976\n",
            "[Epoch 99] Batch loss: 0.3993608057498932\n",
            "[Epoch 99] Batch loss: 0.37310656905174255\n",
            "[Epoch 99] Batch loss: 0.44560399651527405\n",
            "[Epoch 99] Batch loss: 0.37129148840904236\n",
            "[Epoch 99] Batch loss: 0.47891688346862793\n",
            "[Epoch 99] Batch loss: 0.45559370517730713\n",
            "[Epoch 99] Batch loss: 0.38033542037010193\n",
            "[Epoch 99] Batch loss: 0.3839564025402069\n",
            "[Epoch 99] Batch loss: 0.41934680938720703\n",
            "[Epoch 99] Batch loss: 0.461882084608078\n",
            "[Epoch 99] Batch loss: 0.3885984718799591\n",
            "[Epoch 99] Batch loss: 0.34903860092163086\n",
            "[Epoch 99] Batch loss: 0.4113187789916992\n",
            "[Epoch 99] Batch loss: 0.37564322352409363\n",
            "[Epoch 99] Batch loss: 0.39506790041923523\n",
            "[Epoch 99] Batch loss: 0.3960700035095215\n",
            "[Epoch 99] Batch loss: 0.42501741647720337\n",
            "[Epoch 99] Batch loss: 0.4607136845588684\n",
            "[Epoch 99] Batch loss: 0.4940066933631897\n",
            "[Epoch 99] Batch loss: 0.3503991365432739\n",
            "[Epoch 99] Batch loss: 0.45125675201416016\n",
            "[Epoch 99] Batch loss: 0.44798094034194946\n",
            "[Epoch 99] Batch loss: 0.38141143321990967\n",
            "[Epoch 99] Batch loss: 0.5188032388687134\n",
            "[Epoch 99] Batch loss: 0.42540764808654785\n",
            "[Epoch 99] Batch loss: 0.3892538547515869\n",
            "[Epoch 99] Batch loss: 0.3790966868400574\n",
            "[Epoch 99] Batch loss: 0.38997432589530945\n",
            "[Epoch 99] Batch loss: 0.4017448127269745\n",
            "[Epoch 99] Batch loss: 0.406504362821579\n",
            "[Epoch 99] Batch loss: 0.4742721915245056\n",
            "[Epoch 99] Batch loss: 0.3820478320121765\n",
            "[Epoch 99] Batch loss: 0.35493025183677673\n",
            "[Epoch 99] Batch loss: 0.3570161759853363\n",
            "[Epoch 99] Batch loss: 0.27989038825035095\n",
            "[Epoch 99] Batch loss: 0.3810967803001404\n",
            "[Epoch 99] Batch loss: 0.44517114758491516\n",
            "[Epoch 99] Batch loss: 0.47412776947021484\n",
            "[Epoch 99] Batch loss: 0.5428139567375183\n",
            "[Epoch 99] Batch loss: 0.4424591362476349\n",
            "[Epoch 99] Batch loss: 0.5250951051712036\n",
            "[Epoch 99] Batch loss: 0.40172722935676575\n",
            "[Epoch 99] Batch loss: 0.38361385464668274\n",
            "[Epoch 99] Batch loss: 0.45960745215415955\n",
            "[Epoch 99] Batch loss: 0.47541138529777527\n",
            "[Epoch 99] Batch loss: 0.41179585456848145\n",
            "[Epoch 99] Batch loss: 0.39694133400917053\n",
            "[Epoch 99] Batch loss: 0.40149980783462524\n",
            "[Epoch 99] Batch loss: 0.3818887174129486\n",
            "[Epoch 99] Batch loss: 0.3918819725513458\n",
            "[Epoch 99] Batch loss: 0.4089164435863495\n",
            "[Epoch 99] Batch loss: 0.3815593421459198\n",
            "[Epoch 99] Batch loss: 0.43022674322128296\n",
            "[Epoch 99] Batch loss: 0.336526095867157\n",
            "[Epoch 99] Batch loss: 0.40185052156448364\n",
            "[Epoch 99] Batch loss: 0.4627586603164673\n",
            "[Epoch 99] Batch loss: 0.4454423785209656\n",
            "[Epoch 99] Batch loss: 0.3825685679912567\n",
            "[Epoch 99] Batch loss: 0.39827457070350647\n",
            "[Epoch 99] Batch loss: 0.3772187829017639\n",
            "[Epoch 99] Batch loss: 0.4750220775604248\n",
            "[Epoch 99] Batch loss: 0.34651538729667664\n",
            "[Epoch 99] Batch loss: 0.471086710691452\n",
            "[Epoch 99] Batch loss: 0.44474515318870544\n",
            "[Epoch 99] Batch loss: 0.43606308102607727\n",
            "[Epoch 99] Batch loss: 0.35718899965286255\n",
            "[Epoch 99] Batch loss: 0.43187209963798523\n",
            "[Epoch 99] Batch loss: 0.4236129820346832\n",
            "[Epoch 99] Batch loss: 0.4201802909374237\n",
            "[Epoch 99] Batch loss: 0.37253624200820923\n",
            "[Epoch 99] Batch loss: 0.32358118891716003\n",
            "[Epoch 99] Batch loss: 0.33949899673461914\n",
            "[Epoch 99] Batch loss: 0.390671044588089\n",
            "[Epoch 99] Batch loss: 0.4470137059688568\n",
            "[Epoch 99] Batch loss: 0.3595101237297058\n",
            "[Epoch 99] Batch loss: 0.35759755969047546\n",
            "[Epoch 99] Batch loss: 0.362726628780365\n",
            "[Epoch 99] Batch loss: 0.4473472833633423\n",
            "[Epoch 99] Batch loss: 0.4021494388580322\n",
            "[Epoch 99] Batch loss: 0.40392109751701355\n",
            "[Epoch 99] Batch loss: 0.4027773439884186\n",
            "[Epoch 99] Batch loss: 0.3979000747203827\n",
            "[Epoch 99] Batch loss: 0.3980931043624878\n",
            "[Epoch 99] Batch loss: 0.3213256001472473\n",
            "[Epoch 99] Batch loss: 0.4249582290649414\n",
            "[Epoch 99] Batch loss: 0.45105379819869995\n",
            "[Epoch 99] Batch loss: 0.41513943672180176\n",
            "[Epoch 99] Batch loss: 0.5320952534675598\n",
            "[Epoch 99] Batch loss: 0.4569053649902344\n",
            "[Epoch 99] Batch loss: 0.3629477620124817\n",
            "[Epoch 99] Batch loss: 0.4344313442707062\n",
            "[Epoch 100/100] Val loss: 0.7167\n",
            "Best saved model.\n",
            "End of training: reached maximum number of epochs.\n"
          ]
        }
      ],
      "source": [
        "# Numero di epoche dopo cui “scongelare” il backbone\n",
        "freeze_epochs = 10\n",
        "\n",
        "# Early stopping setup:\n",
        "epoch = 0\n",
        "epochs_no_improve = 0  # Count how many epochs passed without validation improvement\n",
        "best_loss = float('inf')  # Best (minimal) validation loss so far\n",
        "best_model_path = f'effdet_{VARIANT}_best.pth'  # File to save best weights\n",
        "\n",
        "# Training loop\n",
        "while epoch < EPOCHS and epochs_no_improve < PATIENCE:\n",
        "    if epoch == freeze_epochs:\n",
        "        for param in net.backbone.parameters():\n",
        "            param.requires_grad = True   # ← unfreeze backbone layers\n",
        "        # ricrea l’optimizer per includere ora tutti i parametri\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "        print(f\"[Info] Backbone scongelato all'epoca {epoch}\")\n",
        "\n",
        "    model.train()  # Training mode\n",
        "    total_loss = 0.0  # Sum of training losses this epoch\n",
        "\n",
        "    for images, targets, _ in train_loader: # con _ scarto 'silenziosamente' il terzo elemento quando non serve\n",
        "        images = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for t in targets:\n",
        "            b = []\n",
        "            l = []\n",
        "            for obj in t:\n",
        "                x, y, w, h = obj['bbox']\n",
        "                if w > 1 and h > 1:  # Filter invalid bboxes\n",
        "                    x1, y1 = x, y\n",
        "                    x2, y2 = x + w, y + h\n",
        "                    b.append([x1, y1, x2, y2])\n",
        "                    l.append(0)  # Class index (0 for single class)\n",
        "\n",
        "            b = torch.tensor(b, dtype=torch.float32)\n",
        "            l = torch.tensor(l, dtype=torch.int64)\n",
        "            boxes.append(b.to(DEVICE))\n",
        "            labels.append(l.to(DEVICE))\n",
        "\n",
        "        # img_sizes = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(images), device=DEVICE)\n",
        "        # img_sizes = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "        # img_scales = torch.ones(len(images), device=DEVICE)\n",
        "        img_sizes  = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "        img_scales = torch.ones(len(images), device=DEVICE)\n",
        "\n",
        "        loss_dict = model(images, {\n",
        "            'bbox': boxes,\n",
        "            'cls':  labels,\n",
        "            'img_scale': img_scales,\n",
        "            'img_size': img_sizes\n",
        "        })\n",
        "\n",
        "        # Forward pass and loss calculation\n",
        "        #loss_dict = model(images, {\n",
        "          #   'bbox': boxes,\n",
        "          #  'cls': labels,\n",
        "            # 'img_scale': img_scales,\n",
        "            #'img_size': img_sizes\n",
        "        #})\n",
        "        loss = loss_dict['loss']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Epoch {epoch}] Batch loss: {loss.item()}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, _ in val_loader:\n",
        "            images = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "\n",
        "            for t in targets:\n",
        "                b = []\n",
        "                l = []\n",
        "                for obj in t:\n",
        "                    x, y, w, h = obj['bbox']\n",
        "                    if w > 1 and h > 1:\n",
        "                        x1, y1 = x, y\n",
        "                        x2, y2 = x + w, y + h\n",
        "                        b.append([x1, y1, x2, y2])\n",
        "                        l.append(0)\n",
        "                b = torch.tensor(b, dtype=torch.float32)\n",
        "                l = torch.tensor(l, dtype=torch.int64)\n",
        "                boxes.append(b.to(DEVICE))\n",
        "                labels.append(l.to(DEVICE))\n",
        "\n",
        "            # img_sizes = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(images), device=DEVICE)\n",
        "            # img_sizes = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "            # img_scales = torch.ones(len(images), device=DEVICE)\n",
        "\n",
        "            img_sizes  = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "            img_scales = torch.ones(len(images), device=DEVICE)\n",
        "\n",
        "            loss_dict = model(images, {\n",
        "                'bbox': boxes,\n",
        "                'cls':  labels,\n",
        "                'img_scale': img_scales,\n",
        "                'img_size': img_sizes\n",
        "            })\n",
        "            #loss_dict = model(images, {\n",
        "             #   'bbox': boxes,\n",
        "              #  'cls': labels,\n",
        "               # 'img_scale': img_scales,\n",
        "                #'img_size': img_sizes\n",
        "            #})\n",
        "            loss = loss_dict['loss']\n",
        "\n",
        "            if not torch.isnan(loss):\n",
        "                val_loss += loss.item()\n",
        "            else:\n",
        "                print(\"NaN detected in loss, batch skipped.\")\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"[Epoch {epoch+1}/{EPOCHS}] Val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        epochs_no_improve = 0\n",
        "        print(\"Best saved model.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Epochs without improvement: {epochs_no_improve}/{PATIENCE}\")\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "# Training end reason\n",
        "if epochs_no_improve >= PATIENCE:\n",
        "    print(\"Early stopping enabled.\")\n",
        "else:\n",
        "    print(\"End of training: reached maximum number of epochs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE29z2ysuc2r"
      },
      "source": [
        "Copy the weights of the best saved model (.pth) from your local file system to your Google Drive folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "km31RC121m88"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifichiamo che il file esista:"
      ],
      "metadata": {
        "id": "FL7z8THZOQ6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U77Zs6je2ZHH",
        "outputId": "669e6383-4688-4a6d-c25f-547499dfb060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientDet weights copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "if not os.path.isfile(best_model_path):\n",
        "  raise FileNotFoundError(f\"Il file dei migliori pesi non è stato trovato: {best_model_path}\")\n",
        "\n",
        "weights_dir = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/weights'\n",
        "os.makedirs(weights_dir, exist_ok=True)\n",
        "shutil.copy(best_model_path, weights_dir)\n",
        "print(\"EfficientDet weights copied to Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDGvR6aWur-R"
      },
      "source": [
        "Load the model in evaluation mode (inference):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq_MTCo7ijCr",
        "outputId": "64b95a36-17ee-41fd-f707-3813a9e04edd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DetBenchPredict(\n",
              "  (model): EfficientDet(\n",
              "    (backbone): EfficientNetFeatures(\n",
              "      (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn1): BatchNormAct2d(\n",
              "        32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "        (drop): Identity()\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): Sequential(\n",
              "          (0): DepthwiseSeparableConv(\n",
              "            (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.013)\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.025)\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.038)\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.050)\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.062)\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.075)\n",
              "          )\n",
              "          (2): InvertedResidual(\n",
              "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.088)\n",
              "          )\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.100)\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.113)\n",
              "          )\n",
              "          (2): InvertedResidual(\n",
              "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.125)\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.138)\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.150)\n",
              "          )\n",
              "          (2): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.163)\n",
              "          )\n",
              "          (3): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.175)\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.188)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): BiFpn(\n",
              "      (resample): ModuleDict(\n",
              "        (3): ResampleFeatureMap(\n",
              "          (conv): ConvBnAct2d(\n",
              "            (conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          )\n",
              "          (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "        )\n",
              "        (4): ResampleFeatureMap(\n",
              "          (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "        )\n",
              "      )\n",
              "      (cell): SequentialList(\n",
              "        (0): BiFpnLayer(\n",
              "          (fnode): ModuleList(\n",
              "            (0): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (3): ResampleFeatureMap()\n",
              "                  (4): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (2): ResampleFeatureMap(\n",
              "                    (conv): ConvBnAct2d(\n",
              "                      (conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (5): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (1): ResampleFeatureMap(\n",
              "                    (conv): ConvBnAct2d(\n",
              "                      (conv): Conv2d(112, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (6): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (0): ResampleFeatureMap(\n",
              "                    (conv): ConvBnAct2d(\n",
              "                      (conv): Conv2d(40, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (7): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (4): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (1): ResampleFeatureMap(\n",
              "                    (conv): ConvBnAct2d(\n",
              "                      (conv): Conv2d(112, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (7): ResampleFeatureMap()\n",
              "                  (8): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (5): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (2): ResampleFeatureMap(\n",
              "                    (conv): ConvBnAct2d(\n",
              "                      (conv): Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                    )\n",
              "                  )\n",
              "                  (6): ResampleFeatureMap()\n",
              "                  (9): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (6): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (3): ResampleFeatureMap()\n",
              "                  (5): ResampleFeatureMap()\n",
              "                  (10): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (7): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (4): ResampleFeatureMap()\n",
              "                  (11): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1): BiFpnLayer(\n",
              "          (fnode): ModuleList(\n",
              "            (0): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (3): ResampleFeatureMap()\n",
              "                  (4): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (2): ResampleFeatureMap()\n",
              "                  (5): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (1): ResampleFeatureMap()\n",
              "                  (6): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (0): ResampleFeatureMap()\n",
              "                  (7): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (4): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (1): ResampleFeatureMap()\n",
              "                  (7): ResampleFeatureMap()\n",
              "                  (8): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (5): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (2): ResampleFeatureMap()\n",
              "                  (6): ResampleFeatureMap()\n",
              "                  (9): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (6): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (3): ResampleFeatureMap()\n",
              "                  (5): ResampleFeatureMap()\n",
              "                  (10): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (7): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (4): ResampleFeatureMap()\n",
              "                  (11): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): BiFpnLayer(\n",
              "          (fnode): ModuleList(\n",
              "            (0): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (3): ResampleFeatureMap()\n",
              "                  (4): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (1): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (2): ResampleFeatureMap()\n",
              "                  (5): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (2): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (1): ResampleFeatureMap()\n",
              "                  (6): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (3): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (0): ResampleFeatureMap()\n",
              "                  (7): ResampleFeatureMap(\n",
              "                    (upsample): Interpolate2d()\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (4): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (1): ResampleFeatureMap()\n",
              "                  (7): ResampleFeatureMap()\n",
              "                  (8): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (5): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (2): ResampleFeatureMap()\n",
              "                  (6): ResampleFeatureMap()\n",
              "                  (9): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (6): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (3): ResampleFeatureMap()\n",
              "                  (5): ResampleFeatureMap()\n",
              "                  (10): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (7): Fnode(\n",
              "              (combine): FpnCombine(\n",
              "                (resample): ModuleDict(\n",
              "                  (4): ResampleFeatureMap()\n",
              "                  (11): ResampleFeatureMap(\n",
              "                    (downsample): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
              "                  )\n",
              "                )\n",
              "              )\n",
              "              (after_combine): Sequential(\n",
              "                (act): SiLU(inplace=True)\n",
              "                (conv): SeparableConv2d(\n",
              "                  (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "                  (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (class_net): HeadNet(\n",
              "      (conv_rep): ModuleList(\n",
              "        (0-2): 3 x SeparableConv2d(\n",
              "          (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (bn_rep): ModuleList(\n",
              "        (0-2): 3 x ModuleList(\n",
              "          (0-4): 5 x Sequential(\n",
              "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (act): SiLU(inplace=True)\n",
              "      (predict): SeparableConv2d(\n",
              "        (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "        (conv_pw): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (box_net): HeadNet(\n",
              "      (conv_rep): ModuleList(\n",
              "        (0-2): 3 x SeparableConv2d(\n",
              "          (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "          (conv_pw): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (bn_rep): ModuleList(\n",
              "        (0-2): 3 x ModuleList(\n",
              "          (0-4): 5 x Sequential(\n",
              "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (act): SiLU(inplace=True)\n",
              "      (predict): SeparableConv2d(\n",
              "        (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
              "        (conv_pw): Conv2d(64, 36, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchors): Anchors()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "config = get_efficientdet_config(VARIANT)\n",
        "config.num_classes = NUM_CLASSES\n",
        "config.image_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "net = EfficientDet(config, pretrained_backbone=False)\n",
        "net.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "\n",
        "model = DetBenchPredict(net).to(DEVICE)\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on complete test set\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets, img_ids in test_loader:\n",
        "        for img, target, img_id in zip(imgs, targets, img_ids):\n",
        "            img_tensor = img.unsqueeze(0).to(DEVICE)\n",
        "            outputs = model(img_tensor)\n",
        "            output = outputs[0]\n",
        "            if output.ndim != 2 or output.shape[1] != 6:\n",
        "                continue\n",
        "\n",
        "            boxes  = output[:, :4].cpu().numpy()\n",
        "            scores = output[:, 4].cpu().numpy()\n",
        "            labels = output[:, 5].cpu().numpy().astype(int)\n",
        "\n",
        "            # dimensioni originali\n",
        "            orig_w = test_loader.dataset.coco.imgs[img_id]['width']\n",
        "            orig_h = test_loader.dataset.coco.imgs[img_id]['height']\n",
        "            scale_x = orig_w / IMAGE_SIZE\n",
        "            scale_y = orig_h / IMAGE_SIZE\n",
        "\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = box\n",
        "\n",
        "                # 1) ripristino dimensioni e clamp\n",
        "                x1 = max(0,     x1 * scale_x)\n",
        "                y1 = max(0,     y1 * scale_y)\n",
        "                x2 = min(orig_w, x2 * scale_x)\n",
        "                y2 = min(orig_h, y2 * scale_y)\n",
        "\n",
        "                # 2) calcolo larghezza/altezza minime\n",
        "                w = max(1, x2 - x1)\n",
        "                h = max(1, y2 - y1)\n",
        "\n",
        "                # 3) appendo predizione “pulita”\n",
        "                predictions.append({\n",
        "                    \"image_id\":    img_id,\n",
        "                    \"category_id\": label + 1,\n",
        "                    \"bbox\":        [x1, y1, w, h],\n",
        "                    \"score\":       float(score)\n",
        "                })\n",
        "\n",
        "print(f\"\\nInference completed. Total predictions collected: {len(predictions)}\")"
      ],
      "metadata": {
        "id": "oixx8AVXRTKA",
        "outputId": "0437d0b0-1e35-45cc-c451-8d0c4efac1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference completed. Total predictions collected: 42300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqxu-ee3w99n"
      },
      "source": [
        "We save all predictions in a .json file in the format required by COCOeval:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all fields to native Python types\n",
        "for pred in predictions:\n",
        "    pred[\"category_id\"] = int(pred[\"category_id\"])\n",
        "    pred[\"image_id\"] = int(pred[\"image_id\"])\n",
        "    pred[\"score\"] = float(pred[\"score\"])\n",
        "    pred[\"bbox\"] = [float(x) for x in pred[\"bbox\"]]\n",
        "\n",
        "# Save to COCO-style JSON\n",
        "with open(\"results_coco.json\", \"w\") as f:\n",
        "    json.dump(predictions, f)"
      ],
      "metadata": {
        "id": "u35m9VZjQ8_d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtuM4uZztzWi"
      },
      "source": [
        "Final evaluation with pycocotools:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ground truth annotations\n",
        "coco_gt = COCO(f\"{BASE_DIR}/annotations_test.json\")\n",
        "\n",
        "# Assicuriamoci che esista il campo 'info'\n",
        "if 'info' not in coco_gt.dataset:\n",
        "    coco_gt.dataset['info'] = {}\n",
        "\n",
        "# Load predictions from file …\n",
        "with open(\"results_coco.json\", \"r\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# … poi prosegui con loadRes e COCOeval\n",
        "coco_dt = coco_gt.loadRes(results)\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()"
      ],
      "metadata": {
        "id": "xkam-jciWcfH",
        "outputId": "b4e5e53e-4ec1-4ebd-ba88-50c985e20c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Loading and preparing results...\n",
            "DONE (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.06s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.02s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v67XFEUjgr7p"
      },
      "source": [
        "We note that values of AP and AR practically at zero indicate that the model is not making correct or meaningful predictions. In essence, it is not recognizing objects well. \\\n",
        "Let's try a small test on a few images and annotations to see if the model recognizes anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZLsCwtmj7Pm"
      },
      "source": [
        "At this point, it may be worth considering that EfficientDet-D0 (or your current EfficientDet configuration) might not be the ideal detector for your specific dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to changing the model, we could also:\n",
        "- Review preprocessing and augmentations to make the training data more representative of the test set.\n",
        "- Experiment with learning rate schedulers (e.g., ReduceLROnPlateau or CosineAnnealingLR) to improve convergence.\n",
        "- Adjust inference filtering thresholds (score and minimum box size) so as not to discard potentially valid predictions."
      ],
      "metadata": {
        "id": "bj2WOYK7ZwTF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxyiRUrk39Xb"
      },
      "source": [
        "### MobileNetV2 + SSDLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdGwZ4a94D2h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LSJWaT04Duk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTI6MMkr4DaS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ja-nuC4EKa"
      },
      "source": [
        "### MobileNetV3 + SSDLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwER9ikS4GJZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmH6fIfp4GB-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmWlK_wR4Fyp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22a2b2db93404820b27c34969f8e740b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_270f3733dd40486085def7bdea1f96ab",
              "IPY_MODEL_897dfc3ada0945ab98366c5f0e444bfb",
              "IPY_MODEL_1e2bc6b00cca4bb6812c08915254cfc0"
            ],
            "layout": "IPY_MODEL_53a7443a0e034e279b7cb2c8dd228df8"
          }
        },
        "270f3733dd40486085def7bdea1f96ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ebfe749d424a4eb60a25c206c784c6",
            "placeholder": "​",
            "style": "IPY_MODEL_0fe99e253405490d815a18395913243e",
            "value": "model.safetensors: 100%"
          }
        },
        "897dfc3ada0945ab98366c5f0e444bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2335ad0847bc4cb588218de1c12c7ab6",
            "max": 21355344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_446727a9c8964dc7a028a1803e6be8eb",
            "value": 21355344
          }
        },
        "1e2bc6b00cca4bb6812c08915254cfc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8e1b69121b44ff6a392b1cb0eaf3b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_2eb1180bbe8147c386238be6ec872106",
            "value": " 21.4M/21.4M [00:00&lt;00:00, 99.0MB/s]"
          }
        },
        "53a7443a0e034e279b7cb2c8dd228df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48ebfe749d424a4eb60a25c206c784c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fe99e253405490d815a18395913243e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2335ad0847bc4cb588218de1c12c7ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446727a9c8964dc7a028a1803e6be8eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8e1b69121b44ff6a392b1cb0eaf3b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb1180bbe8147c386238be6ec872106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}