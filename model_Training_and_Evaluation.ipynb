{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOh9CuVt8rFf00PQqAv6kAk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/model_Training_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "IZxG26MjG3e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Information:** We need to activate the GPU on Colab (_Runtime --> Change runtime type_). \\\n",
        "Every time you start a new session (or reopen the notebook after a few hours) check that the GPU is still active. If we are not using the GPU it can take up to tens of hours to train the models. \\\n",
        "_GPU T4 is the best choice._"
      ],
      "metadata": {
        "id": "Wn4hsdQa0oyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafrlWt-G9_y",
        "outputId": "32e211b5-e6a1-436a-cd00-08a1d136abc1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics # Installation of Ultralytics for YOLO models"
      ],
      "metadata": {
        "id": "cZRM0tKnG-Oj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a8692f-fbfc-405b-a3b5-6e54456af821"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.167)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO # Import of Ultralytics for YOLO models\n",
        "import shutil\n",
        "import os"
      ],
      "metadata": {
        "id": "B7lwWkpiu0JY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Explanation"
      ],
      "metadata": {
        "id": "DlwU4zYYYxnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone"
      ],
      "metadata": {
        "id": "yOtSOJRWpnEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Computer Vision, a _Backbone_ is the part of a convolutional neural network responsible for extracting the main features from an image. \\\n",
        "It serves as the shared base upon which subsequent modules are built (such as heads for classification, object detection, segmentation, etc.).\n",
        "\n",
        "\\\n",
        "Each backbone has been pre-trained on specific datasets (e.g., ImageNet) using particular preprocessing steps, input dimensions, normalization, and augmentation techniques, which should ideally be replicated during fine-tuning to maintain compatibility and achieve optimal performance."
      ],
      "metadata": {
        "id": "uNetjej6sKWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "cWlCNAmDfJPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a deep learning model, it is essential to properly handle data loading and preparation. This is the task of the _Data Loader_, a component responsible for:\n",
        "- Loading images and their corresponding annotations (e.g., .txt or .json) from the dataset.\n",
        "- Applying preprocessing operations, such as resizing, normalization, data augmentation, etc.\n",
        "- Organizing data into batches to feed the model during training.\n",
        "\n",
        "\\\n",
        "Considering the limited resources of my development environment, at first I decided to perform the image and annotation resizing in a separate phase (prior to training), in order to:\n",
        "- Reduce the workload of the data loader during fine-tuning;\n",
        "- Increase data loading and training speed;\n",
        "- Ensure consistency between images and annotations.\n",
        "\n",
        "But due to the different type of scaling technique, I want to try to fine-tuning the model without any pre-scaling."
      ],
      "metadata": {
        "id": "R0R1TXAeOOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other transformations, instead, are handled by the model-specific data loader, since each model uses different preprocessing and normalization techniques. \\\n",
        "Moreover, some models require specific transformations to achieve optimal performance, and the libraries that provide the models (e.g., Ultralytics for YOLO, torchvision for EfficientDet/SSD) already implement loaders that are properly configured and optimized."
      ],
      "metadata": {
        "id": "2RCnQiNkOdt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizzazione"
      ],
      "metadata": {
        "id": "eSgaMN1SZHeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image normalization consists in scaling pixel values from the range [0, 255] to a more suitable interval (e.g., [0, 1] or [-1, 1]), often based on the mean and standard deviation of the pre-training dataset, with the goal of:\n",
        "- Avoiding overly large values in the tensors;\n",
        "- Making the model more stable during training;\n",
        "- Speeding up convergence.\n",
        "\n",
        "\\\n",
        "Normalization helps maintain a consistent pixel range and distribution, which is essential for pre-trained models."
      ],
      "metadata": {
        "id": "QxCOR7b1PV1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentations"
      ],
      "metadata": {
        "id": "2tw6aba8f75J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation consists of random transformations (e.g., rotations, flips, crops, brightness changes, etc.) applied during training. Their purpose is to:\n",
        "- Simulate new visual conditions;\n",
        "- Increase dataset variety;\n",
        "- Reduce overfitting by improving the model’s ability to generalize.\n",
        "\n",
        "\\\n",
        "In practice, the semantic content of the image doesn't change (e.g., a person remains a person), but its visual appearance is altered to help the model \"learn better.\""
      ],
      "metadata": {
        "id": "GPcoqopoPxS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "My goal is to evaluate the real-world performance of each model in its ideal scenario, in order to select the most suitable one for deployment on the Jetson Nano. \\\n",
        "For this reason, each model is trained using its native augmentations, meaning the ones that were designed and optimized as part of its original architecture. \\\n",
        "It wouldn’t make sense to disable them or enforce a uniform setup across models, because what we want to observe is the maximum potential of each model, working in the way it was designed to perform best."
      ],
      "metadata": {
        "id": "-2QIAGYbgIhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Model"
      ],
      "metadata": {
        "id": "AX7HKNc7udFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv8n"
      ],
      "metadata": {
        "id": "SdC8RvUw1F_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to save the dataset locally:\n"
      ],
      "metadata": {
        "id": "Z51qZOzlxvca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n'\n",
        "dst = '/content/AERALIS_YOLOv8n_local'  # is now on the local VM, NOT on drive\n",
        "\n",
        "# If the destination folder already exists, I delete it\n",
        "if os.path.exists(dst):\n",
        "  shutil.rmtree(dst)\n",
        "\n",
        "# Recursive copy of ENTIRE folder (and subfolders)\n",
        "shutil.copytree(src, dst)\n",
        "print(\"Copy completed:\", os.path.exists(dst))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfB0GzrFxu7o",
        "outputId": "f83523ce-fe44-4f7d-9e32-7990f9968f66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copy completed: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the total free space:"
      ],
      "metadata": {
        "id": "8rf3uEUe0j7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h / # It shows the total, used and free space on the root (/) of the Colab VM.\n",
        "\n",
        "# Avail column: space still available for your files."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Wf22_i0Rok",
        "outputId": "adee89d5-1b2d-46af-af17-a1b768d00916"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         108G   55G   53G  52% /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show space used by your local folder\n",
        "!du -sh /content/AERALIS_YOLOv8n_local"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTQQX_sw0fQb",
        "outputId": "4ffc817d-3bdf-4fdf-93a0-eed3c82be2e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.6G\t/content/AERALIS_YOLOv8n_local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show space occupied by various folders in /content/.\n",
        "!du -h --max-depth=1 /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj3PMKWJ1Z3f",
        "outputId": "f990e22d-59ba-4fab-9c3d-6801a6b52046"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140K\t/content/.config\n",
            "du: cannot access '/content/drive/.Encrypted/.shortcut-targets-by-id/1LQbD7p_iS5KLqGNdfrYEvsAx0i_bgB0h/projectUPV': No such file or directory\n",
            "67G\t/content/drive\n",
            "6.6G\t/content/AERALIS_YOLOv8n_local\n",
            "55M\t/content/sample_data\n",
            "74G\t/content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create the data.yaml file, which YOLO uses to know:\n",
        "- the path to the training, validation, and test images\n",
        "- the number of classes (nc)\n",
        "- the names of the classes (names)\n",
        "\n",
        "\\\n",
        "This file is used by YOLO to locate the images and their annotations."
      ],
      "metadata": {
        "id": "wtwQcmy01Odt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YAML dataset (edit routes)\n",
        "data_yaml = \"\"\"\n",
        "train: /content/AERALIS_YOLOv8n_local/train/images\n",
        "val:   /content/AERALIS_YOLOv8n_local/val/images\n",
        "test:  /content/AERALIS_YOLOv8n_local/test/images\n",
        "\n",
        "nc: 1\n",
        "names: ['person']\n",
        "\"\"\"\n",
        "open('data.yaml', 'w').write(data_yaml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6VlyhGe1N10",
        "outputId": "176c5a02-0ee8-4812-8a89-6bf724aa7fdc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect, we have correctly written the data.yaml file for the AERALIS_YOLOv8n dataset. Let's continue with the loading of the model:"
      ],
      "metadata": {
        "id": "QmvOfl4IslZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the pre-trained model we want to use as a starting point\n",
        "\n",
        "model_YOLOv8n = YOLO('yolov8n.pt') # it is the model that will be fine-tuned on the custom dataset"
      ],
      "metadata": {
        "id": "Z6PJD4WHubXR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now downloaded the pre-trained model from the official Ultralytics repository."
      ],
      "metadata": {
        "id": "67c-lmU6sdXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Batch size** is the number of images processed simultaneously in each training step. With 4-8GB of RAM, a batch size of 8 (or even less) is recommended, so we'll start with that value and reduce it if necessary.\n",
        "\n",
        "**Early Stopping** is a technique that automatically stops the training process if the model stops improving after a certain number of epochs. This helps prevent overfitting and saves time.\n",
        "\n",
        "**Workers** are the parallel processes used to load and preprocess data while the model is training. However, due to our limited resources, we’ll start with 2 workers, and if data loading errors occur, we'll reduce this number to 1 or even 0.\n"
      ],
      "metadata": {
        "id": "f0gsz0CG6PPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the available GPU\n",
        "import torch\n",
        "print(torch.cuda.is_available()) # True = you have GPU --> if False then use device='cpu'\n",
        "print(torch.cuda.device_count()) # Name of GPU\n",
        "\n",
        "# If True and at least 1, you can use device=0.\n",
        "# If you don't have GPU: use device='cpu' (much slower).\n",
        "# Locally (not Colab): check with nvidia-smi from terminal."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYQFf9Vf8Jwf",
        "outputId": "21a85733-9c4b-4e05-9ee6-161612d32244"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have confirmation that the GPU is active we can train the model:"
      ],
      "metadata": {
        "id": "PmQfxSxns7I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine‑tuning\n",
        "results_YOLOv8n = model_YOLOv8n.train(\n",
        "  data='data.yaml', # use the newly created yaml file\n",
        "  epochs=100, # Maximum number of training epochs\n",
        "  imgsz=640, # Image input size (recommended for YOLO).\n",
        "  batch=16,  # Batch size\n",
        "  patience=20, # Early stopping if the metrics do not improve for 20 epochs\n",
        "  workers=2, # Number of workers for the dataloader\n",
        "  # device=0, # Use GPU 0 (or put 'cpu' if you don't have GPU)\n",
        "  device='cpu',\n",
        "  project='runs_finetune', # Folder where it will save the results of the experiments (the folder will be created automatically)\n",
        "  name='person_yolov8n' # Subfolder/name specific to our experiment\n",
        ")\n",
        "\n",
        "# results -->  will contain metrics, logs, and the path of the best weights found during the training"
      ],
      "metadata": {
        "id": "YMKnsnJl3Ywr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0e8911b-5006-4991-e1ce-5ec310704fdf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.167 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=person_yolov8n, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs_finetune, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs_finetune/person_yolov8n, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 17.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 107.3±29.3 MB/s, size: 2026.4 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/AERALIS_YOLOv8n_local/train/labels... 2395 images, 388 backgrounds, 0 corrupt: 100%|██████████| 2395/2395 [00:07<00:00, 299.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/AERALIS_YOLOv8n_local/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1965.8±616.5 MB/s, size: 2606.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/AERALIS_YOLOv8n_local/val/labels... 515 images, 75 backgrounds, 0 corrupt: 100%|██████████| 515/515 [00:00<00:00, 1940.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/AERALIS_YOLOv8n_local/val/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs_finetune/person_yolov8n/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns_finetune/person_yolov8n\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      1/100         0G      2.235      3.045      1.092         43        640: 100%|██████████| 150/150 [35:33<00:00, 14.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:19<00:00,  8.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.631      0.465      0.483      0.177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      2/100         0G      2.166      1.839      1.079         49        640: 100%|██████████| 150/150 [35:23<00:00, 14.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:11<00:00,  7.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.653       0.47      0.494      0.199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      3/100         0G      2.148      1.555      1.083         77        640: 100%|██████████| 150/150 [34:56<00:00, 13.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:10<00:00,  7.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.741      0.494      0.541      0.217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      4/100         0G      2.086       1.38      1.066         61        640: 100%|██████████| 150/150 [34:24<00:00, 13.76s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:18<00:00,  8.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.733      0.489      0.554      0.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      5/100         0G      2.045      1.292      1.056         29        640: 100%|██████████| 150/150 [34:32<00:00, 13.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:09<00:00,  7.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.699      0.521      0.566      0.221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      6/100         0G      2.013      1.234      1.041         41        640: 100%|██████████| 150/150 [34:51<00:00, 13.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:12<00:00,  7.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.829      0.513      0.605      0.256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      7/100         0G       1.98      1.181      1.021         46        640: 100%|██████████| 150/150 [34:44<00:00, 13.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:17<00:00,  8.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.753      0.566      0.611      0.259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      8/100         0G      1.941      1.168      1.018         67        640: 100%|██████████| 150/150 [34:45<00:00, 13.91s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:12<00:00,  7.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.787      0.547      0.616      0.269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      9/100         0G      1.897      1.104     0.9971         45        640: 100%|██████████| 150/150 [34:40<00:00, 13.87s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:14<00:00,  7.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.765      0.592      0.654      0.296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     10/100         0G      1.868      1.079      1.008         38        640: 100%|██████████| 150/150 [34:51<00:00, 13.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:15<00:00,  7.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.769      0.567      0.629      0.294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     11/100         0G      1.853       1.09     0.9938         40        640: 100%|██████████| 150/150 [34:58<00:00, 13.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [02:13<00:00,  7.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        515       1258      0.808      0.599      0.668      0.296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "     12/100         0G      1.843      1.061     0.9903         45        640:  57%|█████▋    | 86/150 [20:29<15:15, 14.30s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3915660382.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fine‑tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m results_YOLOv8n = model_YOLOv8n.train(\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# use the newly created yaml file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Maximum number of training epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Image input size (recommended for YOLO).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0;31m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to evaluate the trained model using the Test set defined in data.yaml. \\\n",
        "YOLO does not compute standard accuracy, because in object detection True Negatives (TN) are not counted. Therefore, traditional accuracy is not applicable or useful.\n"
      ],
      "metadata": {
        "id": "2fyPjFuZeghX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we will compute:\n",
        "- **Precision**: how correct your detected positives are\n",
        "- **Recall**: how many of the real objects you detected\n",
        "- **mAP50**: mean Average Precision with IoU ≥ 0.5 (how accurate the predictions are)\n",
        "- **mAP50-95**: average over various IoU thresholds, a more strict metric\n",
        "- **F1_score**: combination of precision and recall (you can compute it as: 2 * (P * R) / (P + R))"
      ],
      "metadata": {
        "id": "aYLO2bxBfHoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates the trained model using the TEST SET defined in data.yaml\n",
        "\n",
        "metrics_YOLOv8n = model_YOLOv8n.val(data='data.yaml', split='test') # returns accuracy metrics (e.g., mAP, precision, recall, etc.) on the test set\n",
        "print(\"Test metrics:\", metrics_YOLOv8n)"
      ],
      "metadata": {
        "id": "aWuTdRMH3Yt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence (confidence) is the probability estimated by the model that a detected object is actually real (i.e., not a false positive)."
      ],
      "metadata": {
        "id": "wmdyUMPKd_V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Inference (practical use of the fine-tuned model)\n",
        "\n",
        "# I load the best weights found\n",
        "best_YOLOv8n = results_YOLOv8n.best or f'runs_finetune/person_yolov8n/weights/best.pt' # or '...': is a manual backup in case results.best does not exist\n",
        "\n",
        "model_inf_YOLOv8n = YOLO(best_YOLOv8n) # Creates a new model instance by loading the best weights\n",
        "\n",
        "# Performs inference on one or more images, or on a video, by specifying the path in the source parameter.\n",
        "# conf=0.25 → Confidence threshold for considering a detection valid.\n",
        "preds_YOLOv8n = model_inf_YOLOv8n.predict(\n",
        "  # source='/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/images',\n",
        "  source='/content/AERALIS_YOLOv8n_local/test/images',\n",
        "  conf=0.25\n",
        ")\n",
        "preds_YOLOv8n.show() # displays the predictions (eventually you can also save them)"
      ],
      "metadata": {
        "id": "y2s3bR6w3Yjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use it on the Jetson Nano we now want to download the model, so:"
      ],
      "metadata": {
        "id": "uJS_ZLwznJS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Per salvarlo su google drive:\n",
        "!cp /content/runs_finetune/person_yolov8n/weights/best.pt /content/drive/MyDrive/\n",
        "\n",
        "# Così lo trovo in Drive (MyDrive/best.pt) e posso scaricarlo da lì in qualsiasi momento."
      ],
      "metadata": {
        "id": "_nz1miD2nOO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per scaricarlo direttamente sul mio computer:\n",
        "from google.colab import files\n",
        "files.download('/content/runs_finetune/person_yolov8n/weights/best.pt')"
      ],
      "metadata": {
        "id": "h-qaF-hFnTnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have the .pt file on the PC, we could:\n",
        "- Upload it to the Jetson Nano,\n",
        "- Convert it to ONNX/TensorRT if you need it for optimization,\n",
        "- Use it with the PyTorch/Ultralytics version on any computer."
      ],
      "metadata": {
        "id": "veHRWTjpnk34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv11n"
      ],
      "metadata": {
        "id": "7n4EVx_z57Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora eseguiamo lo stesso procedimento per YOLOv11n:"
      ],
      "metadata": {
        "id": "CfcXa428v5ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n'\n",
        "dst = '/content/AERALIS_YOLOv11n_local'  # is now on the local VM, NOT on drive\n",
        "\n",
        "# If the destination folder already exists, I delete it\n",
        "if os.path.exists(dst):\n",
        "  shutil.rmtree(dst)\n",
        "\n",
        "# Recursive copy of ENTIRE folder (and subfolders)\n",
        "shutil.copytree(src, dst)\n",
        "print(\"Copy completed:\", os.path.exists(dst))"
      ],
      "metadata": {
        "id": "mBAol2_a3F4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h / # It shows the total, used and free space on the root (/) of the Colab VM.\n",
        "\n",
        "# Avail column: space still available for your files."
      ],
      "metadata": {
        "id": "lO8YB8G43Kkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show space used by your local folder\n",
        "!du -sh /content/AERALIS_YOLOv11n_local"
      ],
      "metadata": {
        "id": "nbpl3LPY3NJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show space occupied by various folders in /content/.\n",
        "!du -h --max-depth=1 /content/"
      ],
      "metadata": {
        "id": "TmqFX4xo3P5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YAML dataset (edit routes)\n",
        "data_yaml = \"\"\"\n",
        "train: /content/AERALIS_YOLOv11n_local/train/images\n",
        "val:   /content/AERALIS_YOLOv11n_local/val/images\n",
        "test:  /content/AERALIS_YOLOv11n_local/test/images\n",
        "\n",
        "nc: 1\n",
        "names: ['person']\n",
        "\"\"\"\n",
        "open('data.yaml', 'w').write(data_yaml)"
      ],
      "metadata": {
        "id": "S7CtXM7O3Uu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello pre-addestrato che vogliamo usare come punto di partenza\n",
        "\n",
        "model_YOLOv11n = YOLO('yolov11n.pt') # è il modello che verrà fine-tunato sul dataset custom"
      ],
      "metadata": {
        "id": "SsT26pMWYhGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine‑tuning\n",
        "results_YOLOv11n = model_YOLOv11n.train(\n",
        "  data='data.yaml',\n",
        "  epochs=100,\n",
        "  imgsz=640,\n",
        "  batch=16,\n",
        "  patience=20,\n",
        "  workers=2,\n",
        "  device=0,\n",
        "  project='runs_finetune',\n",
        "  name='person_yolov11n'\n",
        ")"
      ],
      "metadata": {
        "id": "vBBPkeikYhDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates the trained model using the TEST SET defined in data.yaml\n",
        "\n",
        "metrics_YOLOv11n = model_YOLOv11n.val(data='data.yaml', split='test') # returns accuracy metrics (e.g., mAP, precision, recall, etc.) on the test set\n",
        "print(\"Test metrics:\", metrics_YOLOv11n)"
      ],
      "metadata": {
        "id": "eg0cTtIOYg_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Inference (practical use of the fine-tuned model)\n",
        "\n",
        "# I load the best weights found\n",
        "best_YOLOv11n = results_YOLOv11n.best or f'runs_finetune/person_yolov11n/weights/best.pt' # or '...': is a manual backup in case results.best does not exist\n",
        "\n",
        "model_inf_YOLOv11n = YOLO(best_YOLOv11n) # Creates a new model instance by loading the best weights\n",
        "\n",
        "# Performs inference on one or more images, or on a video, by specifying the path in the source parameter.\n",
        "# conf=0.25 → Confidence threshold for considering a detection valid.\n",
        "preds_YOLOv11n = model_inf_YOLOv11n.predict(\n",
        "  # source='/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/test/images',\n",
        "  source='/content/AERALIS_YOLOv11n_local/test/images',\n",
        "  conf=0.25\n",
        ")\n",
        "preds_YOLOv11n.show() # displays the predictions (eventually you can also save them)"
      ],
      "metadata": {
        "id": "meN-A5gNxGLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientDet D0"
      ],
      "metadata": {
        "id": "G38RcMsn3wbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniziamo ora ad allenare il modello EfficientDet. Utilizzeremo la libreria _effdet_ di _Ross Wightman_, che semplifica tutto il processo:"
      ],
      "metadata": {
        "id": "GGks42T3qV5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install effdet\n",
        "!pip install pycocotools\n"
      ],
      "metadata": {
        "id": "IdEySjIEr3pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
        "from effdet.efficientdet import HeadNet\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from effdet.data.datasets import CocoDataset\n",
        "from effdet.data.loader import create_loader\n",
        "import os\n",
        "\n",
        "# MODEL CONFIG\n",
        "model_name = 'tf_efficientdet_d0'  # Cambia in 'd1', 'd2' se vuoi\n",
        "num_classes = 2  # 1 classe + background\n",
        "image_size = 512\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "lr = 1e-4\n",
        "\n",
        "# EfficientDet config\n",
        "config = get_efficientdet_config(model_name)\n",
        "config.image_size = image_size\n",
        "config.num_classes = num_classes\n",
        "config.norm_kwargs = dict(eps=1e-3, momentum=0.01)\n",
        "\n",
        "# Model\n",
        "net = EfficientDet(config, pretrained_backbone=True)\n",
        "net.class_net = HeadNet(config, num_outputs=num_classes)\n",
        "model = DetBenchTrain(net, config)\n",
        "model = model.cuda()\n",
        "\n",
        "# Dataset (COCO format)\n",
        "train_dataset = CocoDataset(\n",
        "    '/content/dataset/train/images',\n",
        "    '/content/dataset/annotations/instances_train.json',\n",
        "    transform=T.ToTensor()\n",
        ")\n",
        "\n",
        "val_dataset = CocoDataset(\n",
        "    '/content/dataset/val/images',\n",
        "    '/content/dataset/annotations/instances_val.json',\n",
        "    transform=T.ToTensor()\n",
        ")\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in train_loader:\n",
        "        images = torch.stack(images).cuda()\n",
        "        targets = [{k: v.cuda() for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss, _ = model(images, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}/{epochs}] Training Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), f'efficientdet_d0_custom.pt')  # Cambia nome in base alla versione\n"
      ],
      "metadata": {
        "id": "h0Rcp0aar5PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install effdet timm"
      ],
      "metadata": {
        "id": "r-kWcHBY343U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
        "\n",
        "# 1️⃣ Trasformazioni (ToTensor + ImageNet-normalizzazione)\n",
        "_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 2️⃣ Dataset COCO train/val\n",
        "train_ds = CocoDetection(root=\"/content/datasets/train/images\",\n",
        "                         annFile=\"/content/datasets/annotations_train.json\",\n",
        "                         transform=_transform)\n",
        "val_ds = CocoDetection(root=\"/content/datasets/val/images\",\n",
        "                       annFile=\"/content/datasets/annotations_val.json\",\n",
        "                       transform=_transform)\n",
        "\n",
        "def train_efficientdet(model_name, image_size, epochs=20, bs=8, lr=1e-4):\n",
        "    # 3️⃣ Configura e istanzia modello\n",
        "    config = get_efficientdet_config(f'tf_efficientdet_{model_name}')\n",
        "    config.num_classes = 1\n",
        "    config.image_size = image_size\n",
        "    model = EfficientDet(config, pretrained_backbone=True)\n",
        "    model = DetBenchTrain(model, config).to(device)\n",
        "\n",
        "    loader_train = DataLoader(train_ds, batch_size=bs, shuffle=True,\n",
        "                              collate_fn=lambda x: tuple(zip(*x)))\n",
        "    loader_val = DataLoader(val_ds, batch_size=bs, shuffle=False,\n",
        "                            collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    # 4️⃣ Ciclo training + validation\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for imgs, targets in loader_train:\n",
        "            imgs = torch.stack(imgs).to(device)\n",
        "            loss_dict = model(imgs, targets)\n",
        "            loss = sum(loss_dict.values())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss/len(loader_train)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = sum(sum(model(torch.stack(imgs).to(device), tg).values()).item()\n",
        "                        for imgs,tg in loader_val) / len(loader_val)\n",
        "\n",
        "        print(f\"[{model_name} | epoch {epoch}/{epochs}] train_loss={avg_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "    return model, config\n",
        "\n",
        "# Imposta device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ✅ Fine-tuning per D0, D1, D2\n",
        "models = {}\n",
        "for name, size in [('d0',512), ('d1',640), ('d2',768)]:\n",
        "    models[name] = train_efficientdet(name, image_size=size)\n",
        "\n"
      ],
      "metadata": {
        "id": "bzEz6Royw2Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(model, config, img_path, conf_thr=0.25):\n",
        "    model_eval = DetBenchPredict(model.model, config).to(device).eval()\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    inp = _transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model_eval(inp)[0]\n",
        "    # Filtra in base alla soglia\n",
        "    keep = out['scores'] > conf_thr\n",
        "    boxes = out['boxes'][keep].cpu().numpy()\n",
        "    scores = out['scores'][keep].cpu().numpy()\n",
        "    return boxes, scores\n",
        "\n",
        "# Esempio\n",
        "boxes, scores = run_inference(models['d0'][0], models['d0'][1], \"/content/datasets/val/images/sample1.jpg\")\n",
        "print(\"Boxes:\", boxes.shape, \"Scores:\", scores)\n"
      ],
      "metadata": {
        "id": "bG49_Zqlw6LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installa effdet (Ross Wightman)\n",
        "!pip install effdet\n",
        "\n",
        "# Per dataset COCO in PyTorch\n",
        "!pip install pycocotools\n"
      ],
      "metadata": {
        "id": "R-r1PNNi34vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\n",
        "from effdet.efficientdet import HeadNet\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms\n",
        "\n",
        "# Parametri\n",
        "DATASET_DIR = '/content/AERALIS_EfficientDet_D0_local'\n",
        "NUM_CLASSES = 1  # solo 'person' (non mettere background!)\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 50\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- Funzione per creare i DataLoader COCO ---\n",
        "def get_coco_loader(img_dir, ann_path, batch_size, shuffle):\n",
        "    dataset = CocoDetection(\n",
        "        img_dir, ann_path,\n",
        "        transforms=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# --- Scegli la variante EfficientDet (D0, D1, D2) ---\n",
        "# Sostituisci 'tf_efficientdet_d0' con 'tf_efficientdet_d1' o 'tf_efficientdet_d2' se vuoi\n",
        "variant = 'tf_efficientdet_d0'\n",
        "config = get_efficientdet_config(variant)\n",
        "config.num_classes = NUM_CLASSES\n",
        "config.image_size = 512  # D0=512, D1=640, D2=768\n",
        "\n",
        "net = EfficientDet(config, pretrained_backbone=True)\n",
        "net.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "model = DetBenchTrain(net, config).to(DEVICE)\n",
        "\n",
        "# --- Dataloader ---\n",
        "train_loader = get_coco_loader(\n",
        "    os.path.join(DATASET_DIR, 'train/images'),\n",
        "    os.path.join(DATASET_DIR, 'annotations_train.json'),\n",
        "    BATCH_SIZE, shuffle=True\n",
        ")\n",
        "val_loader = get_coco_loader(\n",
        "    os.path.join(DATASET_DIR, 'val/images'),\n",
        "    os.path.join(DATASET_DIR, 'annotations_val.json'),\n",
        "    BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Ottimizzatore ---\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        boxes = [torch.tensor([obj['bbox'] for obj in t], dtype=torch.float32).to(DEVICE) for t in targets]\n",
        "        labels = [torch.tensor([obj['category_id'] for obj in t], dtype=torch.int64).to(DEVICE) for t in targets]\n",
        "        target_dicts = [{'bbox': b, 'cls': l} for b, l in zip(boxes, labels)]\n",
        "        loss_dict = model(images, target_dicts)\n",
        "        loss = loss_dict['loss']\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Salva i pesi migliori\n",
        "    torch.save(model.state_dict(), f'effdet_{variant}_epoch{epoch+1}.pth')\n",
        "\n",
        "# Salva i pesi finali\n",
        "torch.save(model.state_dict(), f'effdet_{variant}_final.pth')\n",
        "print(\"Fine-tuning completato.\")\n"
      ],
      "metadata": {
        "id": "PnsN-CUa34Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from effdet import DetBenchEval\n",
        "\n",
        "# Carica il modello in modalità evaluation\n",
        "net = EfficientDet(config, pretrained_backbone=False)\n",
        "net.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "model = DetBenchEval(net, config).to(DEVICE)\n",
        "model.load_state_dict(torch.load(f'effdet_{variant}_final.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Esempio: predizione su un batch di immagini validation\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_loader:\n",
        "        images = [img.to(DEVICE) for img in images]\n",
        "        preds = model(images)\n",
        "        print(\"Predictions for a batch:\", preds)\n",
        "        break  # rimuovi per valutare su tutto il validation set\n",
        "\n",
        "# Per valutazione COCO (mAP, precision, recall), puoi usare pycocotools e funzioni custom,\n",
        "# oppure vedere la sezione \"evaluation\" della repo (alcuni script di esempio sono inclusi).\n"
      ],
      "metadata": {
        "id": "wSZk14AAsrnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientDet D1"
      ],
      "metadata": {
        "id": "wPr1V4o635QM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GGxLpHR36kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqmxd6nU36dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5zCpVle36LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientDet D2"
      ],
      "metadata": {
        "id": "5wD5PmKK37EK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e6q8Y1qD38v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YA71G5FT38p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XXBMZDBm38Wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MobileNetV2 + SSDLite"
      ],
      "metadata": {
        "id": "YxyiRUrk39Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdGwZ4a94D2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LSJWaT04Duk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTI6MMkr4DaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MobileNetV3 + SSDLite"
      ],
      "metadata": {
        "id": "Y8ja-nuC4EKa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JwER9ikS4GJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmH6fIfp4GB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmWlK_wR4Fyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization and Data Augmentation"
      ],
      "metadata": {
        "id": "7Ao_yVlokodf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per i modelli leggeri ottimizzati come quelli per Jetson Nano, la normalizzazione delle immagini è quasi sempre richiesta prima di passarle al modello."
      ],
      "metadata": {
        "id": "9C7z114snij9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I modelli in PyTorch lavorano SOLO con tensori, NON con immagini PIL o array NumPy.\n",
        "- ToTensor() converte un’immagine (PIL o NumPy) in un tensore PyTorch di tipo float32, formato [C, H, W] (canale, altezza, larghezza).\n",
        "\n",
        "- Inoltre, scala i valori dei pixel da [0,255] a [0,1] automaticamente."
      ],
      "metadata": {
        "id": "1eGZSmm-vJu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La normalizzazione (Normalize) funziona SOLO su tensori.\n",
        "La funzione Normalize(mean, std) richiede input già in formato tensore (float) e applica lo shift/scala canale per canale.\n",
        "\n",
        "Se provi a normalizzare un’immagine PIL o un NumPy array direttamente, ottieni errore o comportamenti inattesi."
      ],
      "metadata": {
        "id": "SQHO73rAvSjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quindi: la sequenza è SEMPRE\n",
        "(Opzionale) Resize\n",
        "\n",
        "ToTensor()   →  Converte e scala [0,255] in [0,1]\n",
        "\n",
        "Normalize()  →  Normalizza ogni canale secondo mean/std richiesto dal modello\n",
        "\n",
        "\n",
        "\\\n",
        " In sintesi:\n",
        "ToTensor è indispensabile, non è solo per PyTorch, ma anche perché la normalizzazione funziona SOLO su tensori, non su immagini raw!\n",
        "\n",
        "La normalizzazione NON sostituisce ToTensor: lavora sopra i dati già convertiti."
      ],
      "metadata": {
        "id": "MSu5W7y6vYNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La sequenza ToTensor() + normalizzazione è quasi sempre necessaria, ma i dettagli della normalizzazione (mean, std, range pixel) possono cambiare in base al modello.\n",
        "Vediamo la situazione per i tuoi modelli:\n",
        "-\n"
      ],
      "metadata": {
        "id": "vmgApfAOv1WA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgXMByxOkmur"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bj7d7CD6l4ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPyrMYl5l5rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0wGCZRNl5pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ByXrXUIl5mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1rpgCMYl5hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aihrQ2E5l5dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7H-43kkl5Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-4Imwmofl6No"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21x3EQDjl7bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Models (Alexia)"
      ],
      "metadata": {
        "id": "fdevgpOAl8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAinZBNZiKdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkU0tydFiK4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynr0wqnUiK03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizzo il codice di Alexia per avere uno spunto:"
      ],
      "metadata": {
        "id": "s2PQ5Gi9r8p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. test_comptage_img.py\n",
        "Scopo:\n",
        "Carica un modello YOLO addestrato e conta quanti oggetti della classe 0 (qui chiamati \"oiseaux\" = uccelli, ma tu potresti adattare a \"persone\") vengono rilevati in una singola immagine."
      ],
      "metadata": {
        "id": "lzv2hzIFr7ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO # Importa la libreria Ultralytics YOLO\n",
        "\n",
        "# Chargement du modèle entraîné\n",
        "model = YOLO(\"../runs/detect/train4/weights/best.pt\") # Carica il modello YOLO addestrato dal file best.pt (specificare il percorso giusto)\n",
        "\n",
        "# Prédiction sur une image\n",
        "results = model.predict(\"test4.jpeg\") # Esegue la predizione sull'immagine \"test4.jpeg\"\n",
        "\n",
        "# Compte des oiseaux (classe 0)\n",
        "bird_count = sum(1 for cls in results[0].boxes.cls if int(cls) == 0) # Conta quante bounding box appartengono alla classe 0\n",
        "\n",
        "print(f\"Nombre d'oiseaux : {bird_count}\") # Stampa il numero di oggetti (classe 0) rilevati\n"
      ],
      "metadata": {
        "id": "s6eE8sQJiKyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota:\n",
        "\n",
        "Puoi cambiare \"classe 0\" con \"persona\" se il tuo modello rileva persone come classe 0."
      ],
      "metadata": {
        "id": "ZZ65X9g6sFWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. test_comptage_video.py\n",
        "Scopo:\n",
        "Carica un modello YOLO addestrato, effettua il tracking (con ByteTrack) e conta quanti oggetti della classe 0 (\"oiseaux\") entrano in un rettangolo centrale all'interno di un video.\n",
        "Annota la video con bounding box, ID, conta corrente e totale degli oggetti unici che sono entrati nel rettangolo."
      ],
      "metadata": {
        "id": "s-AiXfKWsImY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO                # Importa YOLO da Ultralytics\n",
        "import cv2                                  # Importa OpenCV per gestione video e immagini\n",
        "import os                                   # Importa os (qui non usato, ma spesso per path)\n",
        "\n",
        "# Charger le modèle\n",
        "model = YOLO(\"/Users/alexiagaido--amoros/Desktop/UPV-test/entrainement_serveur/runs/detect/train9/weights/best.pt\")\n",
        "# Carica il modello YOLO addestrato (specifica percorso)\n",
        "\n",
        "# Chemin de la vidéo\n",
        "video_path = \"img_video/video_test_1.mp4\"   # Path della video da analizzare\n",
        "output_path = \"output_video.mp4\"            # Path della video annotata in output\n",
        "\n",
        "# Distance des bords pour le rectangle de contact (en pixels)\n",
        "border_distance = 50                        # Margine dai bordi (pixels) per il rettangolo centrale\n",
        "\n",
        "# Ouvrir la vidéo\n",
        "cap = cv2.VideoCapture(video_path)          # Apre la video\n",
        "if not cap.isOpened():\n",
        "    print(\"Erreur : Impossible d'ouvrir la vidéo\")   # Se non apre la video, errore\n",
        "    exit()\n",
        "\n",
        "# Obtenir les propriétés de la vidéo\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))       # Ottiene larghezza frame\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))     # Ottiene altezza frame\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))                # Ottiene fps\n",
        "\n",
        "# Définir les coordonnées du rectangle de contact\n",
        "rect_x1 = border_distance                           # Coordinate x1 del rettangolo\n",
        "rect_y1 = border_distance                           # Coordinate y1\n",
        "rect_x2 = width - border_distance                   # Coordinate x2\n",
        "rect_y2 = height - border_distance                  # Coordinate y2\n",
        "\n",
        "# Configurer la sortie vidéo\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")            # Codec video per output\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # Oggetto per scrivere la video annotata\n",
        "\n",
        "# Ensemble pour stocker les IDs uniques des oiseaux dans le rectangle\n",
        "unique_bird_ids = set()                             # Insieme per salvare gli ID unici degli oggetti che sono passati nel rettangolo\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()                         # Leggi un frame\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Effectuer l'inférence avec suivi\n",
        "    results = model.track(frame, conf=0.5, tracker=\"bytetrack.yaml\", persist=True)\n",
        "    # Fa inferenza + tracking, usa ByteTrack, restituisce risultati con ID di tracking\n",
        "\n",
        "    # Compter les oiseaux dans cette frame\n",
        "    bird_count = 0\n",
        "    if results[0].boxes.id is not None:             # Se ci sono ID di tracking\n",
        "        for box, box_id in zip(results[0].boxes, results[0].boxes.id):   # Scorri bounding box e relativi ID\n",
        "            # Vérifier si le centre de la bounding box est dans le rectangle\n",
        "            x_center = (box.xyxy[0][0] + box.xyxy[0][2]) / 2            # Calcola centro x\n",
        "            y_center = (box.xyxy[0][1] + box.xyxy[0][3]) / 2            # Calcola centro y\n",
        "            if rect_x1 < x_center < rect_x2 and rect_y1 < y_center < rect_y2:   # Se centro box dentro rettangolo centrale\n",
        "                unique_bird_ids.add(box_id.item())                      # Aggiungi ID a set (oggetti unici che sono passati)\n",
        "                bird_count += 1                                         # Conta per questa frame\n",
        "\n",
        "    # Annoter l'image avec les détections et IDs\n",
        "    annotated_frame = results[0].plot()              # Disegna box e ID sul frame\n",
        "\n",
        "    # Dessiner le rectangle de contact\n",
        "    cv2.rectangle(\n",
        "        annotated_frame,\n",
        "        (rect_x1, rect_y1),\n",
        "        (rect_x2, rect_y2),\n",
        "        (255, 0, 0),  # Blu\n",
        "        2,            # Spessore linea\n",
        "    )\n",
        "\n",
        "    # Afficher le nombre d'oiseaux dans cette frame et le total unique\n",
        "    cv2.putText(\n",
        "        annotated_frame,\n",
        "        f\"Oiseaux dans cette frame : {bird_count}\",\n",
        "        (10, 30),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),  # Verde\n",
        "        2,\n",
        "    )\n",
        "    cv2.putText(\n",
        "        annotated_frame,\n",
        "        f\"Oiseaux uniques : {len(unique_bird_ids)}\",\n",
        "        (10, 60),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),\n",
        "        2,\n",
        "    )\n",
        "\n",
        "    # Écrire l'image annotée dans la vidéo de sortie\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "    # Afficher l'image en temps réel\n",
        "    cv2.imshow(\"YOLO Tracking\", annotated_frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Premere 'q' per uscire\n",
        "        break\n",
        "\n",
        "# Afficher le total des oiseaux uniques détectés\n",
        "print(f\"Nombre total d'oiseaux uniques détectés dans la vidéo : {len(unique_bird_ids)}\")\n",
        "\n",
        "# Libérer les ressources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "JHyF-bdzsQ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerazioni tecniche\n",
        "Classe 0: Il codice è pensato per oggetti \"oiseaux\" (uccelli) = classe 0. Se tu hai persone come classe 0, funziona identico.\n",
        "\n",
        "Tracking (ByteTrack): Permette di assegnare un ID a ogni oggetto/persona che attraversa l’area, così da contarli solo una volta anche se si fermano/muovono nella scena.\n",
        "\n",
        "Rettangolo di interesse: Conta solo gli oggetti il cui centro entra in una zona centrale, utile ad esempio per contare solo chi passa in una certa area (adattabile per ingressi, uscite, ecc).\n",
        "\n",
        "Salvataggio video annotato: Il risultato è un video con box, ID e conteggi stampati sopra."
      ],
      "metadata": {
        "id": "-wo_C17TscOk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OOpgi3HsdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyIzSYwtiKv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}