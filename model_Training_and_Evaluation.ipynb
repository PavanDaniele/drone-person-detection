{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNDiHEx2CZNdg/tgZqcGply",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/model_Training_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "IZxG26MjG3e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafrlWt-G9_y",
        "outputId": "56255c70-4e11-4e57-ca1f-b368a16cc8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZRM0tKnG-Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization and Data Augmentation"
      ],
      "metadata": {
        "id": "7Ao_yVlokodf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per i modelli leggeri ottimizzati come quelli per Jetson Nano, la normalizzazione delle immagini è quasi sempre richiesta prima di passarle al modello."
      ],
      "metadata": {
        "id": "9C7z114snij9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I modelli in PyTorch lavorano SOLO con tensori, NON con immagini PIL o array NumPy.\n",
        "- ToTensor() converte un’immagine (PIL o NumPy) in un tensore PyTorch di tipo float32, formato [C, H, W] (canale, altezza, larghezza).\n",
        "\n",
        "- Inoltre, scala i valori dei pixel da [0,255] a [0,1] automaticamente."
      ],
      "metadata": {
        "id": "1eGZSmm-vJu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La normalizzazione (Normalize) funziona SOLO su tensori.\n",
        "La funzione Normalize(mean, std) richiede input già in formato tensore (float) e applica lo shift/scala canale per canale.\n",
        "\n",
        "Se provi a normalizzare un’immagine PIL o un NumPy array direttamente, ottieni errore o comportamenti inattesi."
      ],
      "metadata": {
        "id": "SQHO73rAvSjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quindi: la sequenza è SEMPRE\n",
        "(Opzionale) Resize\n",
        "\n",
        "ToTensor()   →  Converte e scala [0,255] in [0,1]\n",
        "\n",
        "Normalize()  →  Normalizza ogni canale secondo mean/std richiesto dal modello\n",
        "\n",
        "\n",
        "\\\n",
        " In sintesi:\n",
        "ToTensor è indispensabile, non è solo per PyTorch, ma anche perché la normalizzazione funziona SOLO su tensori, non su immagini raw!\n",
        "\n",
        "La normalizzazione NON sostituisce ToTensor: lavora sopra i dati già convertiti."
      ],
      "metadata": {
        "id": "MSu5W7y6vYNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La sequenza ToTensor() + normalizzazione è quasi sempre necessaria, ma i dettagli della normalizzazione (mean, std, range pixel) possono cambiare in base al modello.\n",
        "Vediamo la situazione per i tuoi modelli:\n",
        "-\n"
      ],
      "metadata": {
        "id": "vmgApfAOv1WA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgXMByxOkmur"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bj7d7CD6l4ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPyrMYl5l5rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0wGCZRNl5pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ByXrXUIl5mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1rpgCMYl5hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aihrQ2E5l5dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7H-43kkl5Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-4Imwmofl6No"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21x3EQDjl7bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Models"
      ],
      "metadata": {
        "id": "fdevgpOAl8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAinZBNZiKdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkU0tydFiK4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynr0wqnUiK03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizzo il codice di Alexia per avere uno spunto:"
      ],
      "metadata": {
        "id": "s2PQ5Gi9r8p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. test_comptage_img.py\n",
        "Scopo:\n",
        "Carica un modello YOLO addestrato e conta quanti oggetti della classe 0 (qui chiamati \"oiseaux\" = uccelli, ma tu potresti adattare a \"persone\") vengono rilevati in una singola immagine."
      ],
      "metadata": {
        "id": "lzv2hzIFr7ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO # Importa la libreria Ultralytics YOLO\n",
        "\n",
        "# Chargement du modèle entraîné\n",
        "model = YOLO(\"../runs/detect/train4/weights/best.pt\") # Carica il modello YOLO addestrato dal file best.pt (specificare il percorso giusto)\n",
        "\n",
        "# Prédiction sur une image\n",
        "results = model.predict(\"test4.jpeg\") # Esegue la predizione sull'immagine \"test4.jpeg\"\n",
        "\n",
        "# Compte des oiseaux (classe 0)\n",
        "bird_count = sum(1 for cls in results[0].boxes.cls if int(cls) == 0) # Conta quante bounding box appartengono alla classe 0\n",
        "\n",
        "print(f\"Nombre d'oiseaux : {bird_count}\") # Stampa il numero di oggetti (classe 0) rilevati\n"
      ],
      "metadata": {
        "id": "s6eE8sQJiKyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota:\n",
        "\n",
        "Puoi cambiare \"classe 0\" con \"persona\" se il tuo modello rileva persone come classe 0."
      ],
      "metadata": {
        "id": "ZZ65X9g6sFWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. test_comptage_video.py\n",
        "Scopo:\n",
        "Carica un modello YOLO addestrato, effettua il tracking (con ByteTrack) e conta quanti oggetti della classe 0 (\"oiseaux\") entrano in un rettangolo centrale all'interno di un video.\n",
        "Annota la video con bounding box, ID, conta corrente e totale degli oggetti unici che sono entrati nel rettangolo."
      ],
      "metadata": {
        "id": "s-AiXfKWsImY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO                # Importa YOLO da Ultralytics\n",
        "import cv2                                  # Importa OpenCV per gestione video e immagini\n",
        "import os                                   # Importa os (qui non usato, ma spesso per path)\n",
        "\n",
        "# Charger le modèle\n",
        "model = YOLO(\"/Users/alexiagaido--amoros/Desktop/UPV-test/entrainement_serveur/runs/detect/train9/weights/best.pt\")\n",
        "# Carica il modello YOLO addestrato (specifica percorso)\n",
        "\n",
        "# Chemin de la vidéo\n",
        "video_path = \"img_video/video_test_1.mp4\"   # Path della video da analizzare\n",
        "output_path = \"output_video.mp4\"            # Path della video annotata in output\n",
        "\n",
        "# Distance des bords pour le rectangle de contact (en pixels)\n",
        "border_distance = 50                        # Margine dai bordi (pixels) per il rettangolo centrale\n",
        "\n",
        "# Ouvrir la vidéo\n",
        "cap = cv2.VideoCapture(video_path)          # Apre la video\n",
        "if not cap.isOpened():\n",
        "    print(\"Erreur : Impossible d'ouvrir la vidéo\")   # Se non apre la video, errore\n",
        "    exit()\n",
        "\n",
        "# Obtenir les propriétés de la vidéo\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))       # Ottiene larghezza frame\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))     # Ottiene altezza frame\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))                # Ottiene fps\n",
        "\n",
        "# Définir les coordonnées du rectangle de contact\n",
        "rect_x1 = border_distance                           # Coordinate x1 del rettangolo\n",
        "rect_y1 = border_distance                           # Coordinate y1\n",
        "rect_x2 = width - border_distance                   # Coordinate x2\n",
        "rect_y2 = height - border_distance                  # Coordinate y2\n",
        "\n",
        "# Configurer la sortie vidéo\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")            # Codec video per output\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # Oggetto per scrivere la video annotata\n",
        "\n",
        "# Ensemble pour stocker les IDs uniques des oiseaux dans le rectangle\n",
        "unique_bird_ids = set()                             # Insieme per salvare gli ID unici degli oggetti che sono passati nel rettangolo\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()                         # Leggi un frame\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Effectuer l'inférence avec suivi\n",
        "    results = model.track(frame, conf=0.5, tracker=\"bytetrack.yaml\", persist=True)\n",
        "    # Fa inferenza + tracking, usa ByteTrack, restituisce risultati con ID di tracking\n",
        "\n",
        "    # Compter les oiseaux dans cette frame\n",
        "    bird_count = 0\n",
        "    if results[0].boxes.id is not None:             # Se ci sono ID di tracking\n",
        "        for box, box_id in zip(results[0].boxes, results[0].boxes.id):   # Scorri bounding box e relativi ID\n",
        "            # Vérifier si le centre de la bounding box est dans le rectangle\n",
        "            x_center = (box.xyxy[0][0] + box.xyxy[0][2]) / 2            # Calcola centro x\n",
        "            y_center = (box.xyxy[0][1] + box.xyxy[0][3]) / 2            # Calcola centro y\n",
        "            if rect_x1 < x_center < rect_x2 and rect_y1 < y_center < rect_y2:   # Se centro box dentro rettangolo centrale\n",
        "                unique_bird_ids.add(box_id.item())                      # Aggiungi ID a set (oggetti unici che sono passati)\n",
        "                bird_count += 1                                         # Conta per questa frame\n",
        "\n",
        "    # Annoter l'image avec les détections et IDs\n",
        "    annotated_frame = results[0].plot()              # Disegna box e ID sul frame\n",
        "\n",
        "    # Dessiner le rectangle de contact\n",
        "    cv2.rectangle(\n",
        "        annotated_frame,\n",
        "        (rect_x1, rect_y1),\n",
        "        (rect_x2, rect_y2),\n",
        "        (255, 0, 0),  # Blu\n",
        "        2,            # Spessore linea\n",
        "    )\n",
        "\n",
        "    # Afficher le nombre d'oiseaux dans cette frame et le total unique\n",
        "    cv2.putText(\n",
        "        annotated_frame,\n",
        "        f\"Oiseaux dans cette frame : {bird_count}\",\n",
        "        (10, 30),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),  # Verde\n",
        "        2,\n",
        "    )\n",
        "    cv2.putText(\n",
        "        annotated_frame,\n",
        "        f\"Oiseaux uniques : {len(unique_bird_ids)}\",\n",
        "        (10, 60),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),\n",
        "        2,\n",
        "    )\n",
        "\n",
        "    # Écrire l'image annotée dans la vidéo de sortie\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "    # Afficher l'image en temps réel\n",
        "    cv2.imshow(\"YOLO Tracking\", annotated_frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Premere 'q' per uscire\n",
        "        break\n",
        "\n",
        "# Afficher le total des oiseaux uniques détectés\n",
        "print(f\"Nombre total d'oiseaux uniques détectés dans la vidéo : {len(unique_bird_ids)}\")\n",
        "\n",
        "# Libérer les ressources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "JHyF-bdzsQ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerazioni tecniche\n",
        "Classe 0: Il codice è pensato per oggetti \"oiseaux\" (uccelli) = classe 0. Se tu hai persone come classe 0, funziona identico.\n",
        "\n",
        "Tracking (ByteTrack): Permette di assegnare un ID a ogni oggetto/persona che attraversa l’area, così da contarli solo una volta anche se si fermano/muovono nella scena.\n",
        "\n",
        "Rettangolo di interesse: Conta solo gli oggetti il cui centro entra in una zona centrale, utile ad esempio per contare solo chi passa in una certa area (adattabile per ingressi, uscite, ecc).\n",
        "\n",
        "Salvataggio video annotato: Il risultato è un video con box, ID e conteggi stampati sopra."
      ],
      "metadata": {
        "id": "-wo_C17TscOk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OOpgi3HsdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyIzSYwtiKv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}