{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlIyER7/igrhb8zXEQ6uuR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/model_Training_and_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "IZxG26MjG3e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Information:** We need to activate the GPU on Colab (_Runtime --> Change runtime type_). \\\n",
        "Every time you start a new session (or reopen the notebook after a few hours) check that the GPU is still active. If we are not using the GPU it can take up to tens of hours to train the models. \\\n",
        "_GPU T4 is the best choice._"
      ],
      "metadata": {
        "id": "Wn4hsdQa0oyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafrlWt-G9_y",
        "outputId": "7a17ae15-00f7-48ad-e10f-23bc95a6bfe6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics # Installation of Ultralytics for YOLO models"
      ],
      "metadata": {
        "id": "cZRM0tKnG-Oj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f8d75b-9b47-4a75-b0d8-1b2bc0b5d205"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.167-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.167-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.167 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO # Import of Ultralytics for YOLO models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7lwWkpiu0JY",
        "outputId": "d16c66b8-6231-4106-c04e-d0617a974920"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Explanation"
      ],
      "metadata": {
        "id": "DlwU4zYYYxnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backbone"
      ],
      "metadata": {
        "id": "yOtSOJRWpnEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Computer Vision, a _Backbone_ is the part of a convolutional neural network responsible for extracting the main features from an image. \\\n",
        "It serves as the shared base upon which subsequent modules are built (such as heads for classification, object detection, segmentation, etc.).\n",
        "\n",
        "\\\n",
        "Each backbone has been pre-trained on specific datasets (e.g., ImageNet) using particular preprocessing steps, input dimensions, normalization, and augmentation techniques, which should ideally be replicated during fine-tuning to maintain compatibility and achieve optimal performance."
      ],
      "metadata": {
        "id": "uNetjej6sKWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loader"
      ],
      "metadata": {
        "id": "cWlCNAmDfJPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a deep learning model, it is essential to properly handle data loading and preparation. This is the task of the _Data Loader_, a component responsible for:\n",
        "- Loading images and their corresponding annotations (e.g., .txt or .json) from the dataset.\n",
        "- Applying preprocessing operations, such as resizing, normalization, data augmentation, etc.\n",
        "- Organizing data into batches to feed the model during training.\n",
        "\n",
        "\\\n",
        "Considering the limited resources of my development environment, at first I decided to perform the image and annotation resizing in a separate phase (prior to training), in order to:\n",
        "- Reduce the workload of the data loader during fine-tuning;\n",
        "- Increase data loading and training speed;\n",
        "- Ensure consistency between images and annotations.\n",
        "\n",
        "But due to the different type of scaling technique, I want to try to fine-tuning the model without any pre-scaling."
      ],
      "metadata": {
        "id": "R0R1TXAeOOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other transformations, instead, are handled by the model-specific data loader, since each model uses different preprocessing and normalization techniques. \\\n",
        "Moreover, some models require specific transformations to achieve optimal performance, and the libraries that provide the models (e.g., Ultralytics for YOLO, torchvision for EfficientDet/SSD) already implement loaders that are properly configured and optimized."
      ],
      "metadata": {
        "id": "2RCnQiNkOdt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizzazione"
      ],
      "metadata": {
        "id": "eSgaMN1SZHeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image normalization consists in scaling pixel values from the range [0, 255] to a more suitable interval (e.g., [0, 1] or [-1, 1]), often based on the mean and standard deviation of the pre-training dataset, with the goal of:\n",
        "- Avoiding overly large values in the tensors;\n",
        "- Making the model more stable during training;\n",
        "- Speeding up convergence.\n",
        "\n",
        "\\\n",
        "Normalization helps maintain a consistent pixel range and distribution, which is essential for pre-trained models."
      ],
      "metadata": {
        "id": "QxCOR7b1PV1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentations"
      ],
      "metadata": {
        "id": "2tw6aba8f75J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation consists of random transformations (e.g., rotations, flips, crops, brightness changes, etc.) applied during training. Their purpose is to:\n",
        "- Simulate new visual conditions;\n",
        "- Increase dataset variety;\n",
        "- Reduce overfitting by improving the model’s ability to generalize.\n",
        "\n",
        "\\\n",
        "In practice, the semantic content of the image doesn't change (e.g., a person remains a person), but its visual appearance is altered to help the model \"learn better.\""
      ],
      "metadata": {
        "id": "GPcoqopoPxS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "My goal is to evaluate the real-world performance of each model in its ideal scenario, in order to select the most suitable one for deployment on the Jetson Nano. \\\n",
        "For this reason, each model is trained using its native augmentations, meaning the ones that were designed and optimized as part of its original architecture. \\\n",
        "It wouldn’t make sense to disable them or enforce a uniform setup across models, because what we want to observe is the maximum potential of each model, working in the way it was designed to perform best."
      ],
      "metadata": {
        "id": "-2QIAGYbgIhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Model"
      ],
      "metadata": {
        "id": "AX7HKNc7udFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv8n"
      ],
      "metadata": {
        "id": "SdC8RvUw1F_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create the data.yaml file, which YOLO uses to know:\n",
        "- the path to the training, validation, and test images\n",
        "- the number of classes (nc)\n",
        "- the names of the classes (names)\n",
        "\n",
        "\\\n",
        "This file is used by YOLO to locate the images and their annotations."
      ],
      "metadata": {
        "id": "wtwQcmy01Odt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YAML dataset (edit routes)\n",
        "data_yaml = \"\"\"\n",
        "train: /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/train/images\n",
        "val:   /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/val/images\n",
        "test:  /content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/images\n",
        "\n",
        "nc: 1\n",
        "names: ['person']\n",
        "\"\"\"\n",
        "open('data.yaml', 'w').write(data_yaml)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6VlyhGe1N10",
        "outputId": "30b5f80e-6817-4310-acfe-9d8d93e13a39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect, we have correctly written the data.yaml file for the AERALIS_YOLOv8n dataset. Let's continue with the loading of the model:"
      ],
      "metadata": {
        "id": "QmvOfl4IslZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the pre-trained model we want to use as a starting point\n",
        "\n",
        "model = YOLO('yolov8n.pt') # it is the model that will be fine-tuned on the custom dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6PJD4WHubXR",
        "outputId": "17adb2fc-4019-4acd-ea8c-e6f39d906336"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 80.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now downloaded the pre-trained model from the official Ultralytics repository."
      ],
      "metadata": {
        "id": "67c-lmU6sdXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Batch size** is the number of images processed simultaneously in each training step. With 4-8GB of RAM, a batch size of 8 (or even less) is recommended, so we'll start with that value and reduce it if necessary.\n",
        "\n",
        "**Early Stopping** is a technique that automatically stops the training process if the model stops improving after a certain number of epochs. This helps prevent overfitting and saves time.\n",
        "\n",
        "**Workers** are the parallel processes used to load and preprocess data while the model is training. However, due to our limited resources, we’ll start with 2 workers, and if data loading errors occur, we'll reduce this number to 1 or even 0.\n"
      ],
      "metadata": {
        "id": "f0gsz0CG6PPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the available GPU\n",
        "import torch\n",
        "print(torch.cuda.is_available()) # True = you have GPU --> if False then use device='cpu'\n",
        "print(torch.cuda.device_count()) # Name of GPU\n",
        "\n",
        "# If True and at least 1, you can use device=0.\n",
        "# If you don't have GPU: use device='cpu' (much slower).\n",
        "# Locally (not Colab): check with nvidia-smi from terminal."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYQFf9Vf8Jwf",
        "outputId": "49ea609a-197b-4fdb-c45b-ffe94979a175"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have confirmation that the GPU is active we can train the model:"
      ],
      "metadata": {
        "id": "PmQfxSxns7I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine‑tuning\n",
        "results = model.train(\n",
        "  data='data.yaml', # use the newly created yaml file\n",
        "  epochs=100, # Maximum number of training epochs\n",
        "  imgsz=640, # Image input size (recommended for YOLO).\n",
        "  batch=8,  # Batch size\n",
        "  patience=20, # Early stopping if the metrics do not improve for 20 epochs\n",
        "  workers=2, # Number of workers for the dataloader\n",
        "  device=0, # Use GPU 0 (or put 'cpu' if you don't have GPU)\n",
        "  project='runs_finetune', # Folder where it will save the results of the experiments (the folder will be created automatically)\n",
        "  name='person_yolov8n' # Subfolder/name specific to our experiment\n",
        ")\n",
        "\n",
        "# results -->  will contain metrics, logs, and the path of the best weights found during the training"
      ],
      "metadata": {
        "id": "YMKnsnJl3Ywr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to evaluate the trained model using the Test set defined in data.yaml. \\\n",
        "YOLO does not compute standard accuracy, because in object detection True Negatives (TN) are not counted. Therefore, traditional accuracy is not applicable or useful.\n"
      ],
      "metadata": {
        "id": "2fyPjFuZeghX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we will compute:\n",
        "- **Precision**: how correct your detected positives are\n",
        "- **Recall**: how many of the real objects you detected\n",
        "- **mAP50**: mean Average Precision with IoU ≥ 0.5 (how accurate the predictions are)\n",
        "- **mAP50-95**: average over various IoU thresholds, a more strict metric\n",
        "- **F1_score**: combination of precision and recall (you can compute it as: 2 * (P * R) / (P + R))"
      ],
      "metadata": {
        "id": "aYLO2bxBfHoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates the trained model using the TEST SET defined in data.yaml\n",
        "\n",
        "metrics = model.val(data='data.yaml', split='test') # returns accuracy metrics (e.g., mAP, precision, recall, etc.) on the test set\n",
        "print(\"Test metrics:\", metrics)"
      ],
      "metadata": {
        "id": "aWuTdRMH3Yt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence (confidence) is the probability estimated by the model that a detected object is actually real (i.e., not a false positive)."
      ],
      "metadata": {
        "id": "wmdyUMPKd_V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Inference (practical use of the fine-tuned model)\n",
        "\n",
        "# I load the best weights found\n",
        "best = results.best or f'runs_finetune/person_yolov8n/weights/best.pt' # or '...': is a manual backup in case results.best does not exist\n",
        "\n",
        "model_inf = YOLO(best) # Creates a new model instance by loading the best weights\n",
        "\n",
        "# Performs inference on one or more images, or on a video, by specifying the path in the source parameter.\n",
        "# conf=0.25 → Confidence threshold for considering a detection valid.\n",
        "preds = model_inf.predict(\n",
        "  source='/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/images',\n",
        "  conf=0.25\n",
        ")\n",
        "preds.show() # displays the predictions (eventually you can also save them)"
      ],
      "metadata": {
        "id": "y2s3bR6w3Yjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ePX2671YgHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOv11n"
      ],
      "metadata": {
        "id": "7n4EVx_z57Io"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YvX-tUhgYhK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carica il modello pre-addestrato che vogliamo usare come punto di partenza\n",
        "\n",
        "model = YOLO('yolov11n.pt') # è il modello che verrà fine-tunato sul dataset custom"
      ],
      "metadata": {
        "id": "SsT26pMWYhGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBBPkeikYhDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eg0cTtIOYg_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization and Data Augmentation"
      ],
      "metadata": {
        "id": "7Ao_yVlokodf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per i modelli leggeri ottimizzati come quelli per Jetson Nano, la normalizzazione delle immagini è quasi sempre richiesta prima di passarle al modello."
      ],
      "metadata": {
        "id": "9C7z114snij9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I modelli in PyTorch lavorano SOLO con tensori, NON con immagini PIL o array NumPy.\n",
        "- ToTensor() converte un’immagine (PIL o NumPy) in un tensore PyTorch di tipo float32, formato [C, H, W] (canale, altezza, larghezza).\n",
        "\n",
        "- Inoltre, scala i valori dei pixel da [0,255] a [0,1] automaticamente."
      ],
      "metadata": {
        "id": "1eGZSmm-vJu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La normalizzazione (Normalize) funziona SOLO su tensori.\n",
        "La funzione Normalize(mean, std) richiede input già in formato tensore (float) e applica lo shift/scala canale per canale.\n",
        "\n",
        "Se provi a normalizzare un’immagine PIL o un NumPy array direttamente, ottieni errore o comportamenti inattesi."
      ],
      "metadata": {
        "id": "SQHO73rAvSjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quindi: la sequenza è SEMPRE\n",
        "(Opzionale) Resize\n",
        "\n",
        "ToTensor()   →  Converte e scala [0,255] in [0,1]\n",
        "\n",
        "Normalize()  →  Normalizza ogni canale secondo mean/std richiesto dal modello\n",
        "\n",
        "\n",
        "\\\n",
        " In sintesi:\n",
        "ToTensor è indispensabile, non è solo per PyTorch, ma anche perché la normalizzazione funziona SOLO su tensori, non su immagini raw!\n",
        "\n",
        "La normalizzazione NON sostituisce ToTensor: lavora sopra i dati già convertiti."
      ],
      "metadata": {
        "id": "MSu5W7y6vYNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La sequenza ToTensor() + normalizzazione è quasi sempre necessaria, ma i dettagli della normalizzazione (mean, std, range pixel) possono cambiare in base al modello.\n",
        "Vediamo la situazione per i tuoi modelli:\n",
        "-\n"
      ],
      "metadata": {
        "id": "vmgApfAOv1WA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgXMByxOkmur"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bj7d7CD6l4ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPyrMYl5l5rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0wGCZRNl5pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ByXrXUIl5mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1rpgCMYl5hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aihrQ2E5l5dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7H-43kkl5Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-4Imwmofl6No"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21x3EQDjl7bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Models (Alexia)"
      ],
      "metadata": {
        "id": "fdevgpOAl8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAinZBNZiKdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkU0tydFiK4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynr0wqnUiK03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizzo il codice di Alexia per avere uno spunto:"
      ],
      "metadata": {
        "id": "s2PQ5Gi9r8p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. test_comptage_img.py\n",
        "Scopo:\n",
        "Carica un modello YOLO addestrato e conta quanti oggetti della classe 0 (qui chiamati \"oiseaux\" = uccelli, ma tu potresti adattare a \"persone\") vengono rilevati in una singola immagine."
      ],
      "metadata": {
        "id": "lzv2hzIFr7ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO # Importa la libreria Ultralytics YOLO\n",
        "\n",
        "# Chargement du modèle entraîné\n",
        "model = YOLO(\"../runs/detect/train4/weights/best.pt\") # Carica il modello YOLO addestrato dal file best.pt (specificare il percorso giusto)\n",
        "\n",
        "# Prédiction sur une image\n",
        "results = model.predict(\"test4.jpeg\") # Esegue la predizione sull'immagine \"test4.jpeg\"\n",
        "\n",
        "# Compte des oiseaux (classe 0)\n",
        "bird_count = sum(1 for cls in results[0].boxes.cls if int(cls) == 0) # Conta quante bounding box appartengono alla classe 0\n",
        "\n",
        "print(f\"Nombre d'oiseaux : {bird_count}\") # Stampa il numero di oggetti (classe 0) rilevati\n"
      ],
      "metadata": {
        "id": "s6eE8sQJiKyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota:\n",
        "\n",
        "Puoi cambiare \"classe 0\" con \"persona\" se il tuo modello rileva persone come classe 0."
      ],
      "metadata": {
        "id": "ZZ65X9g6sFWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. test_comptage_video.py\n",
        "Scopo:\n",
        "Carica un modello YOLO addestrato, effettua il tracking (con ByteTrack) e conta quanti oggetti della classe 0 (\"oiseaux\") entrano in un rettangolo centrale all'interno di un video.\n",
        "Annota la video con bounding box, ID, conta corrente e totale degli oggetti unici che sono entrati nel rettangolo."
      ],
      "metadata": {
        "id": "s-AiXfKWsImY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO                # Importa YOLO da Ultralytics\n",
        "import cv2                                  # Importa OpenCV per gestione video e immagini\n",
        "import os                                   # Importa os (qui non usato, ma spesso per path)\n",
        "\n",
        "# Charger le modèle\n",
        "model = YOLO(\"/Users/alexiagaido--amoros/Desktop/UPV-test/entrainement_serveur/runs/detect/train9/weights/best.pt\")\n",
        "# Carica il modello YOLO addestrato (specifica percorso)\n",
        "\n",
        "# Chemin de la vidéo\n",
        "video_path = \"img_video/video_test_1.mp4\"   # Path della video da analizzare\n",
        "output_path = \"output_video.mp4\"            # Path della video annotata in output\n",
        "\n",
        "# Distance des bords pour le rectangle de contact (en pixels)\n",
        "border_distance = 50                        # Margine dai bordi (pixels) per il rettangolo centrale\n",
        "\n",
        "# Ouvrir la vidéo\n",
        "cap = cv2.VideoCapture(video_path)          # Apre la video\n",
        "if not cap.isOpened():\n",
        "    print(\"Erreur : Impossible d'ouvrir la vidéo\")   # Se non apre la video, errore\n",
        "    exit()\n",
        "\n",
        "# Obtenir les propriétés de la vidéo\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))       # Ottiene larghezza frame\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))     # Ottiene altezza frame\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))                # Ottiene fps\n",
        "\n",
        "# Définir les coordonnées du rectangle de contact\n",
        "rect_x1 = border_distance                           # Coordinate x1 del rettangolo\n",
        "rect_y1 = border_distance                           # Coordinate y1\n",
        "rect_x2 = width - border_distance                   # Coordinate x2\n",
        "rect_y2 = height - border_distance                  # Coordinate y2\n",
        "\n",
        "# Configurer la sortie vidéo\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")            # Codec video per output\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))  # Oggetto per scrivere la video annotata\n",
        "\n",
        "# Ensemble pour stocker les IDs uniques des oiseaux dans le rectangle\n",
        "unique_bird_ids = set()                             # Insieme per salvare gli ID unici degli oggetti che sono passati nel rettangolo\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()                         # Leggi un frame\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Effectuer l'inférence avec suivi\n",
        "    results = model.track(frame, conf=0.5, tracker=\"bytetrack.yaml\", persist=True)\n",
        "    # Fa inferenza + tracking, usa ByteTrack, restituisce risultati con ID di tracking\n",
        "\n",
        "    # Compter les oiseaux dans cette frame\n",
        "    bird_count = 0\n",
        "    if results[0].boxes.id is not None:             # Se ci sono ID di tracking\n",
        "        for box, box_id in zip(results[0].boxes, results[0].boxes.id):   # Scorri bounding box e relativi ID\n",
        "            # Vérifier si le centre de la bounding box est dans le rectangle\n",
        "            x_center = (box.xyxy[0][0] + box.xyxy[0][2]) / 2            # Calcola centro x\n",
        "            y_center = (box.xyxy[0][1] + box.xyxy[0][3]) / 2            # Calcola centro y\n",
        "            if rect_x1 < x_center < rect_x2 and rect_y1 < y_center < rect_y2:   # Se centro box dentro rettangolo centrale\n",
        "                unique_bird_ids.add(box_id.item())                      # Aggiungi ID a set (oggetti unici che sono passati)\n",
        "                bird_count += 1                                         # Conta per questa frame\n",
        "\n",
        "    # Annoter l'image avec les détections et IDs\n",
        "    annotated_frame = results[0].plot()              # Disegna box e ID sul frame\n",
        "\n",
        "    # Dessiner le rectangle de contact\n",
        "    cv2.rectangle(\n",
        "        annotated_frame,\n",
        "        (rect_x1, rect_y1),\n",
        "        (rect_x2, rect_y2),\n",
        "        (255, 0, 0),  # Blu\n",
        "        2,            # Spessore linea\n",
        "    )\n",
        "\n",
        "    # Afficher le nombre d'oiseaux dans cette frame et le total unique\n",
        "    cv2.putText(\n",
        "        annotated_frame,\n",
        "        f\"Oiseaux dans cette frame : {bird_count}\",\n",
        "        (10, 30),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),  # Verde\n",
        "        2,\n",
        "    )\n",
        "    cv2.putText(\n",
        "        annotated_frame,\n",
        "        f\"Oiseaux uniques : {len(unique_bird_ids)}\",\n",
        "        (10, 60),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (0, 255, 0),\n",
        "        2,\n",
        "    )\n",
        "\n",
        "    # Écrire l'image annotée dans la vidéo de sortie\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "    # Afficher l'image en temps réel\n",
        "    cv2.imshow(\"YOLO Tracking\", annotated_frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):  # Premere 'q' per uscire\n",
        "        break\n",
        "\n",
        "# Afficher le total des oiseaux uniques détectés\n",
        "print(f\"Nombre total d'oiseaux uniques détectés dans la vidéo : {len(unique_bird_ids)}\")\n",
        "\n",
        "# Libérer les ressources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "JHyF-bdzsQ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerazioni tecniche\n",
        "Classe 0: Il codice è pensato per oggetti \"oiseaux\" (uccelli) = classe 0. Se tu hai persone come classe 0, funziona identico.\n",
        "\n",
        "Tracking (ByteTrack): Permette di assegnare un ID a ogni oggetto/persona che attraversa l’area, così da contarli solo una volta anche se si fermano/muovono nella scena.\n",
        "\n",
        "Rettangolo di interesse: Conta solo gli oggetti il cui centro entra in una zona centrale, utile ad esempio per contare solo chi passa in una certa area (adattabile per ingressi, uscite, ecc).\n",
        "\n",
        "Salvataggio video annotato: Il risultato è un video con box, ID e conteggi stampati sopra."
      ],
      "metadata": {
        "id": "-wo_C17TscOk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OOpgi3HsdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jyIzSYwtiKv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}