{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLz0CObDqV9NOX4o63PQh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/dataset_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "_JEsHziX0SRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7N2nFzq0QrO",
        "outputId": "b38efc11-47bd-43fa-e51d-c5071e51936c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this snippet Just one time, to install packages\n",
        "!pip install imagehash\n",
        "!pip install pillow"
      ],
      "metadata": {
        "id": "gUXXRgaMu4gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d705f65-f576-4259-b6b2-05d037d05142"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.15.3)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.2\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imagehash\n",
        "import os\n",
        "from itertools import combinations # to generate all possible combinations of a number of elements from a set\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "# defaultdict is a special type of dictionary that automatically creates a default value if you access a nonexistent key\n",
        "# deque is a list-like structure, but optimized for quick additions and removals at both ends.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import Counter\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split # to partition the dataset with stratification\n",
        "\n",
        "import numpy as np\n",
        "import cv2 # OpenCV for image manipulation\n",
        "import xml.etree.ElementTree as ET # For parsing and editing XML files (annotations)"
      ],
      "metadata": {
        "id": "BzQ632JVvFYm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "In this notebook, I'm going to prepare the dataset for fine-tuning multiple deep learning models (e.g. YOLO, EfficientDet, SSD + MobileNetV2).\n",
        "The steps include similarity check, dataset splitting (train/val/test), optional image resizing, and bounding box adaptation.\n",
        "The goal is to generate separate, clean and model-ready datasets for each architecture to enable fair training and evaluation."
      ],
      "metadata": {
        "id": "UcbKdax60kLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\""
      ],
      "metadata": {
        "id": "n4X5QP7R-bPd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various metrics to calculate image similarity, such as **SSIM** (Structural Similarity Index), **PSNR** (Peak Signal-to-Noise Ratio), and **Cosine Similarity**. \\\n",
        "In our case, I chose to use **Perceptual Hashing** for an initial check because it is fast, robust, and does not require resizing (which is very important since my AERALIS dataset is composed of images from two different datasets)."
      ],
      "metadata": {
        "id": "G63l7aDrRe-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique reduces the image to a binary signature, and then the *Hamming Distance* is computed to compare the resulting binary hashes."
      ],
      "metadata": {
        "id": "golPihqWSwnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(folder_path): # To estract the images file (.jpg) and ignore the .xml and .csv files\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    folder_path: path to folder containing images\n",
        "  Returns:\n",
        "    list of paths to images\n",
        "  \"\"\"\n",
        "  return [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path)\n",
        "               if f.lower().endswith(('.jpg'))]"
      ],
      "metadata": {
        "id": "g7taLCl6-dMO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hash(img_path, method):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    img_path: path to image\n",
        "    method: hash method to use\n",
        "  Returns:\n",
        "    hash of image\n",
        "  \"\"\"\n",
        "  img = Image.open(img_path).convert(\"L\")  # Grayscale (because the hash algorithms works best when the image is in black and white)\n",
        "\n",
        "  if method == 'phash':\n",
        "    return imagehash.phash(img)\n",
        "  elif method == 'ahash':\n",
        "    return imagehash.average_hash(img)\n",
        "  elif method == 'dhash':\n",
        "    return imagehash.dhash(img)\n",
        "  else:\n",
        "    raise ValueError(f\"Hash method not supported: {method}\")"
      ],
      "metadata": {
        "id": "GhRdzrjt-7iJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_hashes(image_paths, methods): # Hash calculation for each images\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image_paths: list of paths to images\n",
        "    methods: list of hash methods to use\n",
        "  Returns:\n",
        "    dictionary of hashes\n",
        "  \"\"\"\n",
        "  hashes = {method: {} for method in methods} # to create a dictionary and for each method creates an empty sub-dictionary\n",
        "\n",
        "  for method in methods:\n",
        "    print(f\"\\nCalculation {method} for all images\")\n",
        "\n",
        "    for path in image_paths: # cycles over each image path in the image_paths list\n",
        "      try:\n",
        "        h = compute_hash(path, method)\n",
        "        hashes[method][path] = h # saves the calculated hash in the dictionary structure\n",
        "      except Exception as e:\n",
        "        print(f\"Error with {path}: {e}\")\n",
        "\n",
        "  return hashes"
      ],
      "metadata": {
        "id": "JwnEBLh4_eZK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_hashes(hashes, threshold): # Comparison of images in pairs\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "\n",
        "  for method in hashes:\n",
        "    print(f\"\\nRisultats with {method.upper()}:\") # .upper() is used to convert the characters to 'uppercase'\n",
        "    pairs = combinations(hashes[method].items(), 2) # combinations() is used to generate all the possible pairs without repetitions\n",
        "\n",
        "    for (path1, hash1), (path2, hash2) in pairs:\n",
        "      dist = hash1 - hash2\n",
        "      if dist <= threshold:\n",
        "        similar_images.append({\n",
        "          'method': method,\n",
        "          'image1': os.path.basename(path1),\n",
        "          'image2': os.path.basename(path2),\n",
        "          'distance': dist\n",
        "        })\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "kLnyPUaoABkT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hamming distance between the hashes of two images tells us how visually similar they are.\n",
        "The result depends on the threshold:\n",
        "\n",
        "- 1-2 (Very strict) → Only nearly identical images are detected\n",
        "- 3-5 (Good compromise) → Balances well between false positives and false negatives\n",
        "- 6-10 (More permissive) → More images are considered similar, but false positives increas"
      ],
      "metadata": {
        "id": "6eVBBGKlRYGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate one Hash at time:"
      ],
      "metadata": {
        "id": "5SumHPJqPz7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['phash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "image_paths = get_image_paths(image_folder_path)\n",
        "hashes_phash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_phash = compare_hashes(hashes_phash, HAMMING_THRESHOLD)\n",
        "\n",
        "# to see how many distine images are considered similar:\n",
        "img_set = set()\n",
        "for entry in similar_images_phash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hsW2q0cAl5P",
        "outputId": "c6e0a2bc-0f9a-442e-b2f9-8bde934c5ab4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation phash for all images\n",
            "\n",
            "Risultats with PHASH:\n",
            "Method: ['phash'] \n",
            "\n",
            "Number of similar distinct images: 1176\n",
            "Number of All images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['ahash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "hashes_ahash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_ahash = compare_hashes(hashes_ahash, HAMMING_THRESHOLD)\n",
        "\n",
        "img_set = set()\n",
        "for entry in similar_images_ahash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyYxIHEyPG-V",
        "outputId": "60c2373b-6d5a-4b8a-d5ab-7121e19dc33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation ahash for all images\n",
            "\n",
            "Risultats with AHASH:\n",
            "Method: ['ahash'] \n",
            "\n",
            "Number of similar distinct images: 2096\n",
            "Number of All images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['dhash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "hashes_dhash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_dhash = compare_hashes(hashes_dhash, HAMMING_THRESHOLD)\n",
        "\n",
        "img_set = set()\n",
        "for entry in similar_images_dhash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0vmBsTBPvs3",
        "outputId": "004c44ae-0638-443a-f388-260a0182455b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation dhash for all images\n",
            "\n",
            "Risultats with DHASH:\n",
            "Method: ['dhash'] \n",
            "\n",
            "Number of similar distinct images: 1368\n",
            "Number of All images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that the number of similar images is quite high.\n",
        "Instead of simply removing them (which would unnecessarily reduce the dataset size), we adopt a more conservative strategy: we will distribute these similar images carefully across the training, validation, and test sets, in order to prevent potential overfitting or data leakage."
      ],
      "metadata": {
        "id": "KfOmx2Dgy9H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose to use only perceptual hashing (pHash), as it is more robust to minor variations in images and less prone to false positives compared to other variants like aHash and dHash."
      ],
      "metadata": {
        "id": "reqBRtibzctE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But now I want to formulate a hypothesis:\n",
        "Are we sure that all the 1176 images identified as \"similar\" by pHash are truly similar to each other? \\\n",
        "It could be that these images do not all resemble each other directly, but instead form subgroups (clusters) of mutually similar images, while being different from those in other groups. \\\n",
        "Let’s try to test this assumption."
      ],
      "metadata": {
        "id": "KOfa7LzQWagn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To model this relationship, I built a data structure based on an undirected graph, where:\n",
        "\n",
        " - each node represents an image\n",
        " - an edge connects two images if they are considered similar\n",
        "\n",
        "We then extracted the connected components from this graph, which effectively represent the actual clusters of similar images. These groups will be used to perform a controlled split of the dataset."
      ],
      "metadata": {
        "id": "npWjdFeI0PLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructs an oriented graph in which each image is a node and each similar pair is an arc:\n",
        "def build_similarity_graph(similar_images):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    similar_images: list of similar images\n",
        "  Returns:\n",
        "    graph: dictionary of graph\n",
        "  \"\"\"\n",
        "  graph = defaultdict(set) # creates a dictionary (key: name_img, val: set_of_images)\n",
        "\n",
        "  for pair in similar_images: # scrolls each element(=list of dictionaries) of similar_images\n",
        "    img1 = pair['image1']\n",
        "    img2 = pair['image2']\n",
        "    graph[img1].add(img2) # builds the connection in both directions (undirected arc)\n",
        "    graph[img2].add(img1)\n",
        "\n",
        "  return graph"
      ],
      "metadata": {
        "id": "0FH39M_Hg-Su"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finds the groups (connected components) in the graph:\n",
        "def find_connected_components(graph):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    graph: dictionary of graph\n",
        "  Returns:\n",
        "    groups: list of groups\n",
        "  \"\"\"\n",
        "  visited = set() # keeps track of images already visited\n",
        "  groups = [] # will contain the final groups\n",
        "\n",
        "  for node in sorted(graph): # scrolls each node(=image) in the graph (is important to order the nodes to ensure stability)\n",
        "    if node not in visited: # If the image has not yet been visited, then start a new group\n",
        "      group = []\n",
        "\n",
        "      # Start a BFS (Breadth-First Search) with a queue\n",
        "      # adds the initial node to the queue and marks it as visited\n",
        "      queue = deque([node])\n",
        "      visited.add(node)\n",
        "\n",
        "      while queue: # as long as there are nodes in the tail\n",
        "        current = queue.popleft() # removes the knot from the head and adds it to the group\n",
        "        group.append(current)\n",
        "\n",
        "        # for each neighbor (similar image), if not already visited\n",
        "        for neighbor in sorted(graph[current]): # (is important to order the nodes to ensure stability)\n",
        "          if neighbor not in visited:\n",
        "            visited.add(neighbor) # marks it as visited\n",
        "            queue.append(neighbor) # puts it in the queue for trial\n",
        "\n",
        "      # Once the queue is exhausted, the group is complete and it is added to the groups\n",
        "      groups.append(group)\n",
        "  return groups"
      ],
      "metadata": {
        "id": "CCd1p_gQiP4U"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = build_similarity_graph(similar_images_phash)\n",
        "groups = find_connected_components(graph)\n",
        "\n",
        "print(f\"\\nFound {len(groups)} groups of similar images.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Vk1UPdiQRz",
        "outputId": "b94ce9be-0d8a-40a9-a928-37902830d3cc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 347 groups of similar images.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! We were right! \\\n",
        "Now let's do a brief analysis"
      ],
      "metadata": {
        "id": "xBPdoG29IFLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sizes = [len(group) for group in groups]\n",
        "\n",
        "size_counts = Counter(group_sizes) # count how many groups have size X\n",
        "\n",
        "# Sort and save to a DataFrame by display\n",
        "group_distribution = pd.DataFrame(sorted(size_counts.items()), columns=[\"Group Size\", \"Number of Groups\"])\n",
        "display(group_distribution)\n",
        "\n",
        "# Other useful statistics\n",
        "total_similar_images = sum(group_sizes)\n",
        "largest_group = max(group_sizes)\n",
        "average_group_size = total_similar_images / len(groups)\n",
        "\n",
        "print(f\"Total grups: {len(groups)}\")\n",
        "print(f\"Total similar images: {total_similar_images}\")\n",
        "print(f\"Average group size: {average_group_size:.2f}\")\n",
        "print(f\"Largest group: {largest_group} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "japx8S0hINtf",
        "outputId": "c39ba802-ccf1-4823-8b4a-633e2fcd3a39"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Group Size  Number of Groups\n",
              "0            2               221\n",
              "1            3                49\n",
              "2            4                24\n",
              "3            5                18\n",
              "4            6                10\n",
              "5            7                 5\n",
              "6            8                 5\n",
              "7            9                 2\n",
              "8           10                 2\n",
              "9           11                 1\n",
              "10          12                 2\n",
              "11          15                 1\n",
              "12          16                 1\n",
              "13          17                 1\n",
              "14          19                 1\n",
              "15          20                 1\n",
              "16          25                 1\n",
              "17          36                 1\n",
              "18          45                 1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Group Size</th>\n",
              "      <th>Number of Groups</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ea70625c-214b-4135-a064-2070039d702a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea70625c-214b-4135-a064-2070039d702a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ea70625c-214b-4135-a064-2070039d702a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_67da8cd5-4642-4337-aae6-006e3b864ad9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('group_distribution')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_67da8cd5-4642-4337-aae6-006e3b864ad9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('group_distribution');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "group_distribution",
              "summary": "{\n  \"name\": \"group_distribution\",\n  \"rows\": 19,\n  \"fields\": [\n    {\n      \"column\": \"Group Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 2,\n        \"max\": 45,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          2,\n          7,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number of Groups\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50,\n        \"min\": 1,\n        \"max\": 221,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          49,\n          5,\n          221\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total grups: 347\n",
            "Total similar images: 1176\n",
            "Average group size: 3.39\n",
            "Largest group: 45 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = total_similar_images / len(image_paths) * 100\n",
        "print(f\"Percentage of similar images in the dataset: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiPpGBEaP3n2",
        "outputId": "23915760-599a-462f-9303-0454fa27b4d2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of similar images in the dataset: 34.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, 34.33% of all images in our AERALIS dataset are similar. This is not ideal. \\\n",
        "But don't worry! We can keep all the images and still avoid overfitting or data leakage by using another technique: *Group-Aware Splitting*."
      ],
      "metadata": {
        "id": "Amm_VzC3VkCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is similar to the more classical Stratified Sampling, but it is more suitable for our case. \\\n",
        "So let’s start using this technique to properly create the Training, Validation, and Test sets. \\\n",
        "\n",
        "But before that, I think it could be interesting to see how the pHash results change when we adjust the similarity threshold."
      ],
      "metadata": {
        "id": "7q2l-8-vXmSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_similar_images(hashes, threshold): # filters similar images by threshold\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "  Returns:\n",
        "    similar_images: list of similar images\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "  pairs = combinations(hashes.items(), 2)\n",
        "\n",
        "  for (path1, hash1), (path2, hash2) in pairs:\n",
        "    dist = hash1 - hash2\n",
        "    if dist <= threshold:\n",
        "      similar_images.append({'image1': path1, 'image2': path2, 'distance': dist})\n",
        "\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "jaP14gkPahgx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see Just the pHash case\n",
        "HASH_METHODS = ['phash']\n",
        "thresholds_to_try = [3, 5, 7, 10]\n",
        "\n",
        "hashes_phash_all = compute_all_hashes(image_paths, HASH_METHODS)['phash'] # Extracts only 'phash' from the returned dictionary\n",
        "\n",
        "results = []\n",
        "for thresh in thresholds_to_try: # analyzes each threshold\n",
        "  similar_images = filter_similar_images(hashes_phash_all, thresh) # for each threshold, calculate similar images with that threshold\n",
        "\n",
        "  # Extracts all the images that appear at least once as similar (without duplicates):\n",
        "  img_set = set()\n",
        "  for pair in similar_images:\n",
        "    img_set.add(pair['image1'])\n",
        "    img_set.add(pair['image2'])\n",
        "\n",
        "  results.append({\n",
        "    \"Threshold\": thresh,\n",
        "    \"Num Similar Pairs\": len(similar_images),\n",
        "    \"Num Similar Distinct Images\": len(img_set),\n",
        "    \"Total Images\": len(image_paths),\n",
        "    \"Percent Similar (%)\": round(len(img_set) / len(image_paths) * 100, 2)\n",
        "  })\n",
        "\n",
        "\n",
        "# To see the results let's converts the list of results to a DataFrame pandas\n",
        "df_results = pd.DataFrame(results)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QYHuj26nZKRk",
        "outputId": "8f2b698a-eb5e-47a9-b266-02399f2dd085"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation phash for all images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Threshold  Num Similar Pairs  Num Similar Distinct Images  Total Images  \\\n",
              "0          3               1450                          839          3426   \n",
              "1          5               2307                         1176          3426   \n",
              "2          7               3161                         1494          3426   \n",
              "3         10               4960                         1971          3426   \n",
              "\n",
              "   Percent Similar (%)  \n",
              "0                24.49  \n",
              "1                34.33  \n",
              "2                43.61  \n",
              "3                57.53  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d0938d4-530e-4553-9651-e62abaf90f0d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Num Similar Pairs</th>\n",
              "      <th>Num Similar Distinct Images</th>\n",
              "      <th>Total Images</th>\n",
              "      <th>Percent Similar (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1450</td>\n",
              "      <td>839</td>\n",
              "      <td>3426</td>\n",
              "      <td>24.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2307</td>\n",
              "      <td>1176</td>\n",
              "      <td>3426</td>\n",
              "      <td>34.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>3161</td>\n",
              "      <td>1494</td>\n",
              "      <td>3426</td>\n",
              "      <td>43.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>4960</td>\n",
              "      <td>1971</td>\n",
              "      <td>3426</td>\n",
              "      <td>57.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d0938d4-530e-4553-9651-e62abaf90f0d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d0938d4-530e-4553-9651-e62abaf90f0d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d0938d4-530e-4553-9651-e62abaf90f0d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7a94ffa5-cc1a-44dd-b600-6cd10fc20031\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a94ffa5-cc1a-44dd-b600-6cd10fc20031')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7a94ffa5-cc1a-44dd-b600-6cd10fc20031 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_43727316-e02b-4229-aa0f-3a4677529cae\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_43727316-e02b-4229-aa0f-3a4677529cae button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Threshold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 3,\n        \"max\": 10,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          10,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Num Similar Pairs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1499,\n        \"min\": 1450,\n        \"max\": 4960,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2307,\n          4960,\n          1450\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Num Similar Distinct Images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 481,\n        \"min\": 839,\n        \"max\": 1971,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1176,\n          1971,\n          839\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3426,\n        \"max\": 3426,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3426\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Percent Similar (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.059890943152203,\n        \"min\": 24.49,\n        \"max\": 57.53,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          34.33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that as the threshold increases, the number of pairs considered similar also grows, and consequently, so does the percentage of images involved.\n",
        "\n",
        "Observations:\n",
        "- At lower thresholds (e.g., 3), only strongly similar images are identified, but many less obvious duplicates may be missed.\n",
        "\n",
        "- At higher thresholds (e.g., 10), there's a risk of including different images that only share generic visual elements (false positives).\n",
        "\n",
        "- Threshold 5 proves to be a good compromise, balancing precision and coverage."
      ],
      "metadata": {
        "id": "41lbIz8GjUP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now create a function that uses the group_aware splitting technique:"
      ],
      "metadata": {
        "id": "ObR-Eb5b5nuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divides the groups of similar images into training, validation, and test sets, keeping each group together\n",
        "#  (no similar images end up in different sets).\n",
        "def group_aware_split(groups, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    groups: list of groups\n",
        "    train_ratio: ratio of images to be assigned to the training set\n",
        "    val_ratio: ratio of images to be assigned to the validation set\n",
        "    test_ratio: ratio of images to be assigned to the test set\n",
        "    seed: seed for the random number generator\n",
        "\n",
        "  Returns:\n",
        "    assignments: dictionary with the split of images\n",
        "  \"\"\"\n",
        "  # checks whether the sets add up to 1\n",
        "  try:\n",
        "    total_ratio = train_ratio + val_ratio + test_ratio\n",
        "    if not 0.99 <= total_ratio <= 1.01:\n",
        "      raise ValueError(\"The proportions do not add up to 1! You must correct the values.\")\n",
        "  except ValueError as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    return None\n",
        "\n",
        "  split_ratios = {'train': train_ratio, 'val': val_ratio, 'test': test_ratio}\n",
        "\n",
        "  random.seed(seed) # to initialize the random number generator\n",
        "  random.shuffle(groups) # to make randomization reproducible\n",
        "\n",
        "  # dictionary comprehension\n",
        "  total_images = sum(len(g) for g in groups) # to figure out how many images should be assigned to that split\n",
        "  target_counts = {k: int(v * total_images) for k, v in split_ratios.items()}\n",
        "  current_counts = defaultdict(int) # number of images already assigned to each split\n",
        "  assignments = defaultdict(list) # number of images actually assigned to each split as final output\n",
        "\n",
        "  for group in groups:\n",
        "    # Find the split with the lowest saturation ratio\n",
        "    best_split = min(\n",
        "      target_counts.items(),\n",
        "      key=lambda item: current_counts[item[0]] / item[1] if item[1] > 0 else float('inf')\n",
        "    )[0] # this line is used to take the key (‘train’, ‘val’, ‘test’) of the best split\n",
        "\n",
        "    assignments[best_split].extend(group) # adds all images in the group to the selected split\n",
        "    current_counts[best_split] += len(group) # update the counter to know how many images are now in that split\n",
        "\n",
        "  return assignments"
      ],
      "metadata": {
        "id": "se5sCN7Lele7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function we get a *assignments* dictionary structured so that each list contains the names of the images assigned to the split (train, val, test), keeping similar images together."
      ],
      "metadata": {
        "id": "O5wxLy2P6X32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# invokes the function to divide the groups of similar images into the different sets\n",
        "assignments = group_aware_split(groups, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42)\n",
        "\n",
        "# check the counts\n",
        "print({k: len(v) for k, v in assignments.items()})\n"
      ],
      "metadata": {
        "id": "bVs4GxlHcg-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70f4710-b817-40bf-8205-733233061822"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 820, 'val': 178, 'test': 178}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partitioning the dataset into: \\\n",
        "- Training set = 70%,\n",
        "- Validation set = 15%,\n",
        "- Test set = 15%\n",
        "\n",
        "and we obtain a distribution according to:\n",
        "- 820 images out of 1176 for the Training set\n",
        "- 178 images out of 1176 for the Validation set\n",
        "- 178 images out of 1176 for the Test set"
      ],
      "metadata": {
        "id": "eTI4FyCt8R2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "Now we need to use a *Stratified Split* that ensures that the distribution of classes in the dataset is proportionally balanced across the divisions of the three sets. \\\n",
        "We prefer to use a **Stratified Sampling** technique because we already know that our dataset is somewhat unbalanced, as it contains more images with people than images without.\n"
      ],
      "metadata": {
        "id": "vFX11HDvBHuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, of course, we want to maintain the partitioning we just did for similar images:"
      ],
      "metadata": {
        "id": "SKakySqXCwJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divides the AERALIS dataset in a layered manner and copies images/.xml to train/val/test.\n",
        "#   - Maintains similar image assignments (from group_aware_split).\n",
        "#   - Stratifies remainder split based on CSV ‘class’ column.\n",
        "#   - Saves images, annotations and generates CSV for each set with all original columns.\n",
        "def stratified_split(assignments, image_folder_path, output_base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    assignments: dictionary with the split of images\n",
        "    image_folder_path: path to the folder containing images (and CSV file)\n",
        "    output_base_path: path to the output folder\n",
        "    train_ratio, val_ratio, test_ratio: desired proportions of images to be assigned to the training, validation and test set\n",
        "    seed: seed for the random number generator\n",
        "  \"\"\"\n",
        "\n",
        "  # 1. Create train/val/test folders with subfolders images/ and annotations/\n",
        "  for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(output_base_path, split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_base_path, split, \"annotations\"), exist_ok=True)\n",
        "\n",
        "  # 2. Upload the full CSV\n",
        "  csv_path = os.path.join(image_folder_path, \"aeralis_person_labels.csv\")\n",
        "  df_full = pd.read_csv(csv_path)\n",
        "  df_full = df_full[df_full['filename'].str.lower().str.endswith(('.jpg', '.jpeg', '.png'))] # we really only need '.jpg'\n",
        "  df = df_full.drop_duplicates(subset='filename', keep='first').copy()\n",
        "\n",
        "  # df = pd.read_csv(csv_path)\n",
        "  # df = df[df['filename'].str.lower().str.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "  # 3. Removes images already assigned (similar)\n",
        "  already_assigned = set(sum(assignments.values(), []))\n",
        "  df_unassigned = df[~df['filename'].isin(already_assigned)].copy()\n",
        "\n",
        "  # 4. Split is stratified according to y labels, class balance is maintained\n",
        "  X = df_unassigned['filename'].values\n",
        "  y = df_unassigned['class'].values\n",
        "\n",
        "  X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    train_size = train_ratio,\n",
        "    stratify = y,\n",
        "    random_state = seed\n",
        "  )\n",
        "  # X_train, y_train: part that will go into the training set\n",
        "  # X_temp, y_temp: remaining images to be split still in validation and testing\n",
        "\n",
        "  # After removing the train part, we need to divide X_temp into val and test:\n",
        "  val_ratio_adjusted = val_ratio / (val_ratio + test_ratio) # we calculate the new proportion of validation to the remaining total\n",
        "\n",
        "  # We divide X_temp and y_temp into validation and test:\n",
        "  X_val, X_test = train_test_split(\n",
        "    X_temp, train_size = val_ratio_adjusted,\n",
        "    stratify = y_temp,\n",
        "    random_state = seed\n",
        "  )\n",
        "\n",
        "  # 5. Adds assignments to the dictionary, avoiding duplicates\n",
        "  # assignments is the dictionary initially created with the similar groups assigned via Group-Aware Splitting\n",
        "  # X_train, X_val, X_test are the non-similar, stratified image assignments\n",
        "  for split, split_X in zip(['train', 'val', 'test'], [X_train, X_val, X_test]):\n",
        "    new_imgs = [x for x in split_X if x not in already_assigned]\n",
        "    assignments[split].extend(new_imgs)\n",
        "    already_assigned.update(new_imgs)\n",
        "\n",
        "\n",
        "\n",
        "  # 6. Copy file and generate final CSV\n",
        "  for split, file_list in assignments.items():\n",
        "    # split_df = df[df['filename'].isin(file_list)].copy()\n",
        "    split_df = df_full[df_full['filename'].isin(file_list)].copy()\n",
        "\n",
        "    for fname in file_list:\n",
        "      img_path = os.path.join(image_folder_path, fname)\n",
        "      xml_name = os.path.splitext(fname)[0] + \".xml\"\n",
        "      xml_path = os.path.join(image_folder_path, xml_name)\n",
        "\n",
        "      dst_img = os.path.join(output_base_path, split, \"images\", fname)\n",
        "      dst_xml = os.path.join(output_base_path, split, \"annotations\", xml_name)\n",
        "\n",
        "      if os.path.exists(img_path):\n",
        "        shutil.copy2(img_path, dst_img)\n",
        "      if os.path.exists(xml_path):\n",
        "        shutil.copy2(xml_path, dst_xml)\n",
        "\n",
        "    # save detailed CSV for split\n",
        "    split_df.to_csv(os.path.join(output_base_path, f\"{split}_set.csv\"), index=False)\n",
        "\n",
        "  # 7. Report\n",
        "  print(\"Stratified split completed and files copied.\")\n",
        "  print(f\"Train: {len(assignments['train'])} images\")\n",
        "  print(f\"Val:   {len(assignments['val'])} images\")\n",
        "  print(f\"Test:  {len(assignments['test'])} images\")"
      ],
      "metadata": {
        "id": "CSPJq6NMvEh4"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function we are going to create a new folder containing 3 subfolders for the Training, Validation and Test phases, each of which will contain a folder with images and one with their respective annotations. At the same time a CSV file describing the set will be created."
      ],
      "metadata": {
        "id": "L3p41XS5C4-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\"\n",
        "\n",
        "output_base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "# Initialize assignments (only if is not initialized) with: assignments = group_aware_split(gruppi_simili)\n",
        "\n",
        "stratified_split(assignments, image_folder_path, output_base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeYrKkd6m0PF",
        "outputId": "1b38c920-e6ac-4915-d8d8-6bad07801373"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified split completed and files copied.\n",
            "Train: 2395 images\n",
            "Val:   515 images\n",
            "Test:  516 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if the numbers matches\n",
        "total_all = len(assignments['train']) + len(assignments['val']) + len(assignments['test'])\n",
        "unique_total = len(set(assignments['train'] + assignments['val'] + assignments['test']))\n",
        "\n",
        "print(f\"Total images in split (sum): {total_all}\")\n",
        "print(f\"Total unique images: {unique_total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJucdFYFpQOQ",
        "outputId": "70199705-fdf3-418e-dece-d379af6f84de"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in split (sum): 3426\n",
            "Total unique images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now perform a quick check to see if the split did not cause inconsistencies in the data:"
      ],
      "metadata": {
        "id": "PCt5LTQ-0Sga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = ['train', 'val', 'test']\n",
        "base_path = output_base_path  # già definito\n",
        "\n",
        "for split in splits:\n",
        "  img_dir = os.path.join(base_path, split, 'images')\n",
        "  ann_dir = os.path.join(base_path, split, 'annotations')\n",
        "\n",
        "  images = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "  missing_xml = []\n",
        "\n",
        "  for img in images:\n",
        "    xml_name = os.path.splitext(img)[0] + \".xml\"\n",
        "    if not os.path.exists(os.path.join(ann_dir, xml_name)):\n",
        "      missing_xml.append(xml_name)\n",
        "\n",
        "  print(f\"{split.upper()} Images: {len(images)}, Missing XML: {len(missing_xml)}\")\n",
        "  if missing_xml:\n",
        "    print(\"\\nMissing XML files:\")\n",
        "    for x in missing_xml:\n",
        "      print(\"   \", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtbLNpZtqY-a",
        "outputId": "ba8c569b-4e11-4f80-f1c4-121c230c5aad"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN Images: 2395, Missing XML: 0\n",
            "VAL Images: 515, Missing XML: 0\n",
            "TEST Images: 516, Missing XML: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in splits:\n",
        "  csv_path = os.path.join(base_path, f\"{split}_set.csv\")\n",
        "  img_dir = os.path.join(base_path, split, \"images\")\n",
        "\n",
        "  df_split = pd.read_csv(csv_path)\n",
        "  csv_filenames = set(df_split['filename'].str.lower())\n",
        "  actual_images = set(f.lower() for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
        "\n",
        "  missing_in_csv = actual_images - csv_filenames # files present in folder but NOT in CSV\n",
        "  missing_in_dir = csv_filenames - actual_images # files present in CSV but NOT in folder\n",
        "\n",
        "  print(f\"{split.upper()} — CSV: {len(csv_filenames)}, IMG DIR: {len(actual_images)}\")\n",
        "  if missing_in_csv:\n",
        "    print(f\"   {len(missing_in_csv)} images in folder not listed in CSV.\")\n",
        "  if missing_in_dir:\n",
        "    print(f\"   {len(missing_in_dir)} images in CSV not found in folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Atvs9VjpXEx",
        "outputId": "0b4fc9a9-fe46-4912-f3e4-67c32cf05c06"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN — CSV: 2395, IMG DIR: 2395\n",
            "VAL — CSV: 515, IMG DIR: 515\n",
            "TEST — CSV: 516, IMG DIR: 516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! There are no inconsistencies resulting from the split. \\\n",
        " We now proceed to create copies of the folder we just created, *AERALIS_SPLITTED*, as we want to ensure that the future study of the models' performance is not affected by different splits. Therefore, we will use the same Train, Val, and Test proportions for all of them, as we have just created"
      ],
      "metadata": {
        "id": "zUwwwXtj4LQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remember: base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "model_versions = [\"YOLOv8n\", \"YOLOv11n\", \"EfficientDet_D0\", \"EfficientDet_D1\", \"EfficientDet_D2\", \"MobileNetV2_SSD\"]\n",
        "\n",
        "for v in model_versions:\n",
        "    dst = f\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_{v}\" # constructs the destination path\n",
        "    shutil.copytree(base_path, dst)\n"
      ],
      "metadata": {
        "id": "aC8VT3Ki65Qm"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resizing\n"
      ],
      "metadata": {
        "id": "0NiC2UuhSrDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we are going to resize all the images to the correct size for the model version. \\\n",
        "It will also be necessary to edit the Bounding Boxes so that there are no inconsistencies, as well as the values in the csv files."
      ],
      "metadata": {
        "id": "RuiW_Bw7T6bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per i modelli di detection che ho scelto, e immagini dovranno essere ridimensionate nel seguente modo:\n",
        "- YOLOv8n e YOLOv11n richiedono dimensioni di immagini 640x640\n",
        "- EfficientDet D0 richiede dimensioni di immagini 512x512\n",
        "- EfficientDet D1 richiede dimensioni di immagini 640x640\n",
        "- EfficientDet D2 richiede dimensioni di immagini 768x768\n",
        "- MobileNetV2 + SSD richiede dimensioni di immagini 300x300\n",
        "\n",
        "Tutto ciò per garantire le migliori performance durante i test di fine-tuning ed analisi. Nonostante alcuni modelli siano più flessibili od alcune implementazioni degli stessi permettano un resizing dinamico, ho preferito ridimensionare manualmente i dati in modo da avere un maggior controllo."
      ],
      "metadata": {
        "id": "3FTLPaLXW0ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the detection models I selected, the images will be resized as follows:\n",
        "\n",
        "- YOLOv8n and YOLOv11n require image dimensions of 640×640\n",
        "- EfficientDet D0 requires 512×512\n",
        "- EfficientDet D1 requires 640×640\n",
        "- EfficientDet D2 requires 768×768\n",
        "- MobileNetV2 + SSD requires 300×300\n",
        "\n",
        "This is done to ensure optimal performance during fine-tuning and evaluation.\n",
        "Although some models are more flexible and certain implementations allow dynamic resizing, I preferred to manually resize the data to maintain greater control over the process."
      ],
      "metadata": {
        "id": "hzuodc0tb4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bog2pHtK6lgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA"
      ],
      "metadata": {
        "id": "Ptn7ggFsNDOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform letterbox resize: resize with retained aspect ratio and padding\n",
        "def letterbox_resize(image, target_size=(640, 640), color=(114, 114, 114)):\n",
        "  orig_h, orig_w = image.shape[:2] # original height and width\n",
        "  target_w, target_h = target_size # desired height and width\n",
        "\n",
        "  scale = min(target_w / orig_w, target_h / orig_h) # maintaining aspect ratio\n",
        "  new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "  resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR) # resize image\n",
        "  pad_x = (target_w - new_w) // 2 # horizontal padding\n",
        "  pad_y = (target_h - new_h) // 2 # vertical padding\n",
        "\n",
        "  # Adding padding to get target size image\n",
        "  padded = cv2.copyMakeBorder(\n",
        "    resized,\n",
        "    pad_y, target_h - new_h - pad_y,\n",
        "    pad_x, target_w - new_w - pad_x,\n",
        "    borderType = cv2.BORDER_CONSTANT, value=color # grey padding\n",
        "  )\n",
        "\n",
        "  return padded, scale, (pad_x, pad_y)"
      ],
      "metadata": {
        "id": "pAktUPQ-NA1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process a single split of the dataset\n",
        "def process_one_folder(img_dir, xml_dir, out_img_dir, out_xml_dir, target_size=(640, 640)):\n",
        "  os.makedirs(out_img_dir, exist_ok=True) # folder output images\n",
        "  os.makedirs(out_xml_dir, exist_ok=True) # folder output xml files\n",
        "\n",
        "  for fname in os.listdir(img_dir):\n",
        "    is_image = fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    xml_filename = os.path.splitext(fname)[0] + \".xml\"\n",
        "    xml_path = os.path.join(xml_dir, xml_filename)\n",
        "    xml_exists = os.path.exists(xml_path)\n",
        "    image = cv2.imread(img_path) if is_image else None # read image if it exist\n",
        "\n",
        "    if is_image and xml_exists and image is not None:\n",
        "      resized_img, scale, (pad_x, pad_y) = letterbox_resize(image, target_size)\n",
        "      cv2.imwrite(os.path.join(out_img_dir, fname), resized_img) # save resized image\n",
        "\n",
        "      # Update XML\n",
        "      tree = ET.parse(xml_path)\n",
        "      root = tree.getroot()\n",
        "\n",
        "      for obj in root.findall('object'): # for each object (bbox) in the XML file\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(float(bbox.find('xmin').text))\n",
        "        ymin = int(float(bbox.find('ymin').text))\n",
        "        xmax = int(float(bbox.find('xmax').text))\n",
        "        ymax = int(float(bbox.find('ymax').text))\n",
        "\n",
        "        # apply scaling + padding to the bbox\n",
        "        xmin = int(xmin * scale + pad_x)\n",
        "        xmax = int(xmax * scale + pad_x)\n",
        "        ymin = int(ymin * scale + pad_y)\n",
        "        ymax = int(ymax * scale + pad_y)\n",
        "\n",
        "        # clamp of values (avoids out-of-picture bbox)\n",
        "        bbox.find('xmin').text = str(max(0, min(xmin, target_size[0])))\n",
        "        bbox.find('ymin').text = str(max(0, min(ymin, target_size[1])))\n",
        "        bbox.find('xmax').text = str(max(0, min(xmax, target_size[0])))\n",
        "        bbox.find('ymax').text = str(max(0, min(ymax, target_size[1])))\n",
        "\n",
        "      # update size in <size> tag\n",
        "      size_tag = root.find('size')\n",
        "      size_tag.find('width').text = str(target_size[0])\n",
        "      size_tag.find('height').text = str(target_size[1])\n",
        "\n",
        "      # Save the new XML file\n",
        "      tree.write(os.path.join(out_xml_dir, xml_filename))\n",
        "\n",
        "    # Error handling/ignora invalid files.\n",
        "    else:\n",
        "      if not is_image: # It is not an image file\n",
        "        print(f\"Ignored file (not image): {img_path}\")\n",
        "      elif not xml_exists:\n",
        "        print(f\"Missing file XML for: {fname}\")\n",
        "      elif image is None:\n",
        "        print(f\"Reading error: {img_path}\")"
      ],
      "metadata": {
        "id": "GzRjT-MXNHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the entire dataset (Training, Validation, Testing)\n",
        "def process_entire_dataset(base_input_dir, base_output_dir, splits=('train', 'val', 'test'), target_size=(640, 640)):\n",
        "  for split in splits:\n",
        "    print(f\"\\n Processing split: {split}\")\n",
        "    # Input/output paths for images and annotations.\n",
        "    img_dir = os.path.join(base_input_dir, split, 'images')\n",
        "    xml_dir = os.path.join(base_input_dir, split, 'annotations')\n",
        "    out_img_dir = os.path.join(base_output_dir, split, 'images')\n",
        "    out_xml_dir = os.path.join(base_output_dir, split, 'annotations')\n",
        "\n",
        "    # Process a single split\n",
        "    process_one_folder(img_dir, xml_dir, out_img_dir, out_xml_dir, target_size)\n",
        "\n",
        "  print(\"\\nAll images and annotations have been processed.\")"
      ],
      "metadata": {
        "id": "3TCeW88NNJFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iQvXjS7zNB2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ESEMPIO D'USO\n",
        "\n",
        "process_entire_dataset(\n",
        "    base_input_dir=\"/content/drive/MyDrive/dataset_originale\",\n",
        "    base_output_dir=\"/content/drive/MyDrive/dataset_resized\",\n",
        "    target_size=(640, 640)\n",
        ")"
      ],
      "metadata": {
        "id": "WTeWdZXAM07o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prova per vedere"
      ],
      "metadata": {
        "id": "BVvsyVI8Uvao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA 2"
      ],
      "metadata": {
        "id": "JEVLPr4DUuAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mp01fcomUu2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9g-VaBkTU5Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCRIPT 2"
      ],
      "metadata": {
        "id": "lnQAtMo5UzyT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZyzx1trUupU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgR4fAYlU-bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}