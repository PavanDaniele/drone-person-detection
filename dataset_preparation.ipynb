{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM62caoaWTZ/EJvynx6yD4V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/dataset_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "_JEsHziX0SRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7N2nFzq0QrO",
        "outputId": "fe035575-6158-4417-aa0b-a3682156e012"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this snippet Just one time, to install packages\n",
        "!pip install imagehash\n",
        "!pip install pillow"
      ],
      "metadata": {
        "id": "gUXXRgaMu4gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e557dc2-35ab-4511-ee0a-dc65be74ca50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.15.3)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.2\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imagehash\n",
        "import os\n",
        "from itertools import combinations # to generate all possible combinations of a number of elements from a set\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "# defaultdict is a special type of dictionary that automatically creates a default value if you access a nonexistent key\n",
        "# deque is a list-like structure, but optimized for quick additions and removals at both ends.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import Counter\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split # to partition the dataset with stratification\n",
        "\n",
        "import numpy as np\n",
        "import cv2 # OpenCV for image manipulation\n",
        "import xml.etree.ElementTree as ET # For parsing and editing XML files (annotations)\n",
        "\n",
        "from pathlib import Path # to manage file paths more robustly\n",
        "import json # to create the final .json file in COCO format\n",
        "from typing import Dict, List # types to improve readability and autocomplete\n",
        "from tqdm import tqdm\n",
        "import re # regular expressions, used to extract numbers from the image name\n"
      ],
      "metadata": {
        "id": "BzQ632JVvFYm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "In this notebook, I'm going to prepare the dataset for fine-tuning multiple deep learning models (e.g. YOLO, EfficientDet, SSD + MobileNetV2).\n",
        "The steps include similarity check, dataset splitting (train/val/test), optional image resizing, and bounding box adaptation.\n",
        "The goal is to generate separate, clean and model-ready datasets for each architecture to enable fair training and evaluation."
      ],
      "metadata": {
        "id": "UcbKdax60kLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\""
      ],
      "metadata": {
        "id": "n4X5QP7R-bPd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various metrics to calculate image similarity, such as **SSIM** (Structural Similarity Index), **PSNR** (Peak Signal-to-Noise Ratio), and **Cosine Similarity**. \\\n",
        "In our case, I chose to use **Perceptual Hashing** for an initial check because it is fast, robust, and does not require resizing (which is very important since my AERALIS dataset is composed of images from two different datasets)."
      ],
      "metadata": {
        "id": "G63l7aDrRe-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique reduces the image to a binary signature, and then the *Hamming Distance* is computed to compare the resulting binary hashes."
      ],
      "metadata": {
        "id": "golPihqWSwnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(folder_path): # To estract the images file (.jpg) and ignore the .xml and .csv files\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    folder_path: path to folder containing images\n",
        "\n",
        "  Returns:\n",
        "    list of paths to images\n",
        "  \"\"\"\n",
        "  return [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path)\n",
        "               if f.lower().endswith(('.jpg'))]"
      ],
      "metadata": {
        "id": "g7taLCl6-dMO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hash(img_path, method):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    img_path: path to image\n",
        "    method: hash method to use\n",
        "\n",
        "  Returns:\n",
        "    hash of image\n",
        "  \"\"\"\n",
        "  img = Image.open(img_path).convert(\"L\")  # Grayscale (because the hash algorithms works best when the image is in black and white)\n",
        "\n",
        "  if method == 'phash':\n",
        "    return imagehash.phash(img)\n",
        "  elif method == 'ahash':\n",
        "    return imagehash.average_hash(img)\n",
        "  elif method == 'dhash':\n",
        "    return imagehash.dhash(img)\n",
        "  else:\n",
        "    raise ValueError(f\"Hash method not supported: {method}\")"
      ],
      "metadata": {
        "id": "GhRdzrjt-7iJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_hashes(image_paths, methods): # Hash calculation for each images\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image_paths: list of paths to images\n",
        "    methods: list of hash methods to use\n",
        "\n",
        "  Returns:\n",
        "    dictionary of hashes\n",
        "  \"\"\"\n",
        "  hashes = {method: {} for method in methods} # to create a dictionary and for each method creates an empty sub-dictionary\n",
        "\n",
        "  for method in methods:\n",
        "    print(f\"\\nCalculation {method} for all images\")\n",
        "\n",
        "    for path in image_paths: # cycles over each image path in the image_paths list\n",
        "      try:\n",
        "        h = compute_hash(path, method)\n",
        "        hashes[method][path] = h # saves the calculated hash in the dictionary structure\n",
        "      except Exception as e:\n",
        "        print(f\"Error with {path}: {e}\")\n",
        "\n",
        "  return hashes"
      ],
      "metadata": {
        "id": "JwnEBLh4_eZK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_hashes(hashes, threshold): # Comparison of images in pairs\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "\n",
        "  for method in hashes:\n",
        "    print(f\"\\nRisultats with {method.upper()}:\") # .upper() is used to convert the characters to 'uppercase'\n",
        "    pairs = combinations(hashes[method].items(), 2) # combinations() is used to generate all the possible pairs without repetitions\n",
        "\n",
        "    for (path1, hash1), (path2, hash2) in pairs:\n",
        "      dist = hash1 - hash2\n",
        "      if dist <= threshold:\n",
        "        similar_images.append({\n",
        "          'method': method,\n",
        "          'image1': os.path.basename(path1),\n",
        "          'image2': os.path.basename(path2),\n",
        "          'distance': dist\n",
        "        })\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "kLnyPUaoABkT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hamming distance between the hashes of two images tells us how visually similar they are.\n",
        "The result depends on the threshold:\n",
        "\n",
        "- 1-2 (Very strict) → Only nearly identical images are detected\n",
        "- 3-5 (Good compromise) → Balances well between false positives and false negatives\n",
        "- 6-10 (More permissive) → More images are considered similar, but false positives increas"
      ],
      "metadata": {
        "id": "6eVBBGKlRYGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate one Hash at time:"
      ],
      "metadata": {
        "id": "5SumHPJqPz7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['phash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "image_paths = get_image_paths(image_folder_path)\n",
        "hashes_phash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_phash = compare_hashes(hashes_phash, HAMMING_THRESHOLD)\n",
        "\n",
        "# to see how many distine images are considered similar:\n",
        "img_set = set()\n",
        "for entry in similar_images_phash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hsW2q0cAl5P",
        "outputId": "c6e0a2bc-0f9a-442e-b2f9-8bde934c5ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation phash for all images\n",
            "\n",
            "Risultats with PHASH:\n",
            "Method: ['phash'] \n",
            "\n",
            "Number of similar distinct images: 1176\n",
            "Number of All images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['ahash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "hashes_ahash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_ahash = compare_hashes(hashes_ahash, HAMMING_THRESHOLD)\n",
        "\n",
        "img_set = set()\n",
        "for entry in similar_images_ahash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyYxIHEyPG-V",
        "outputId": "60c2373b-6d5a-4b8a-d5ab-7121e19dc33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation ahash for all images\n",
            "\n",
            "Risultats with AHASH:\n",
            "Method: ['ahash'] \n",
            "\n",
            "Number of similar distinct images: 2096\n",
            "Number of All images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['dhash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "hashes_dhash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_dhash = compare_hashes(hashes_dhash, HAMMING_THRESHOLD)\n",
        "\n",
        "img_set = set()\n",
        "for entry in similar_images_dhash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0vmBsTBPvs3",
        "outputId": "004c44ae-0638-443a-f388-260a0182455b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation dhash for all images\n",
            "\n",
            "Risultats with DHASH:\n",
            "Method: ['dhash'] \n",
            "\n",
            "Number of similar distinct images: 1368\n",
            "Number of All images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that the number of similar images is quite high.\n",
        "Instead of simply removing them (which would unnecessarily reduce the dataset size), we adopt a more conservative strategy: we will distribute these similar images carefully across the training, validation, and test sets, in order to prevent potential overfitting or data leakage."
      ],
      "metadata": {
        "id": "KfOmx2Dgy9H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose to use only perceptual hashing (pHash), as it is more robust to minor variations in images and less prone to false positives compared to other variants like aHash and dHash."
      ],
      "metadata": {
        "id": "reqBRtibzctE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But now I want to formulate a hypothesis:\n",
        "Are we sure that all the 1176 images identified as \"similar\" by pHash are truly similar to each other? \\\n",
        "It could be that these images do not all resemble each other directly, but instead form subgroups (clusters) of mutually similar images, while being different from those in other groups. \\\n",
        "Let’s try to test this assumption."
      ],
      "metadata": {
        "id": "KOfa7LzQWagn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To model this relationship, I built a data structure based on an undirected graph, where:\n",
        "\n",
        " - each node represents an image\n",
        " - an edge connects two images if they are considered similar\n",
        "\n",
        "We then extracted the connected components from this graph, which effectively represent the actual clusters of similar images. These groups will be used to perform a controlled split of the dataset.\n",
        "\n",
        "*Remember: a connected component is a maximal subset of a set (space) in which all points (nodes) are connected to each other.*"
      ],
      "metadata": {
        "id": "npWjdFeI0PLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructs an oriented graph in which each image is a node and each similar pair is an arc:\n",
        "def build_similarity_graph(similar_images):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    similar_images: list of similar images\n",
        "\n",
        "  Returns:\n",
        "    graph: dictionary of graph\n",
        "  \"\"\"\n",
        "  graph = defaultdict(set) # creates a dictionary (key: name_img, val: set_of_images)\n",
        "\n",
        "  for pair in similar_images: # scrolls each element(=list of dictionaries) of similar_images\n",
        "    img1 = pair['image1']\n",
        "    img2 = pair['image2']\n",
        "    graph[img1].add(img2) # builds the connection in both directions (undirected arc)\n",
        "    graph[img2].add(img1)\n",
        "\n",
        "  return graph"
      ],
      "metadata": {
        "id": "0FH39M_Hg-Su"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finds the groups (connected components) in the graph:\n",
        "def find_connected_components(graph):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    graph: dictionary of graph\n",
        "\n",
        "  Returns:\n",
        "    groups: list of groups\n",
        "  \"\"\"\n",
        "  visited = set() # keeps track of images already visited\n",
        "  groups = [] # will contain the final groups\n",
        "\n",
        "  for node in sorted(graph): # scrolls each node(=image) in the graph (is important to order the nodes to ensure stability)\n",
        "    if node not in visited: # If the image has not yet been visited, then start a new group\n",
        "      group = []\n",
        "\n",
        "      # Start a BFS (Breadth-First Search) with a queue\n",
        "      # adds the initial node to the queue and marks it as visited\n",
        "      queue = deque([node])\n",
        "      visited.add(node)\n",
        "\n",
        "      while queue: # as long as there are nodes in the tail\n",
        "        current = queue.popleft() # removes the knot from the head and adds it to the group\n",
        "        group.append(current)\n",
        "\n",
        "        # for each neighbor (similar image), if not already visited\n",
        "        for neighbor in sorted(graph[current]): # (is important to order the nodes to ensure stability)\n",
        "          if neighbor not in visited:\n",
        "            visited.add(neighbor) # marks it as visited\n",
        "            queue.append(neighbor) # puts it in the queue for trial\n",
        "\n",
        "      # Once the queue is exhausted, the group is complete and it is added to the groups\n",
        "      groups.append(group)\n",
        "  return groups"
      ],
      "metadata": {
        "id": "CCd1p_gQiP4U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = build_similarity_graph(similar_images_phash)\n",
        "groups = find_connected_components(graph)\n",
        "\n",
        "print(f\"\\nFound {len(groups)} groups of similar images.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Vk1UPdiQRz",
        "outputId": "b94ce9be-0d8a-40a9-a928-37902830d3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 347 groups of similar images.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! We were right! \\\n",
        "Now let's do a brief analysis"
      ],
      "metadata": {
        "id": "xBPdoG29IFLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sizes = [len(group) for group in groups]\n",
        "\n",
        "size_counts = Counter(group_sizes) # count how many groups have size X\n",
        "\n",
        "# Sort and save to a DataFrame by display\n",
        "group_distribution = pd.DataFrame(sorted(size_counts.items()), columns=[\"Group Size\", \"Number of Groups\"])\n",
        "display(group_distribution)\n",
        "\n",
        "# Other useful statistics\n",
        "total_similar_images = sum(group_sizes)\n",
        "largest_group = max(group_sizes)\n",
        "average_group_size = total_similar_images / len(groups)\n",
        "\n",
        "print(f\"Total grups: {len(groups)}\")\n",
        "print(f\"Total similar images: {total_similar_images}\")\n",
        "print(f\"Average group size: {average_group_size:.2f}\")\n",
        "print(f\"Largest group: {largest_group} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "japx8S0hINtf",
        "outputId": "c39ba802-ccf1-4823-8b4a-633e2fcd3a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Group Size  Number of Groups\n",
              "0            2               221\n",
              "1            3                49\n",
              "2            4                24\n",
              "3            5                18\n",
              "4            6                10\n",
              "5            7                 5\n",
              "6            8                 5\n",
              "7            9                 2\n",
              "8           10                 2\n",
              "9           11                 1\n",
              "10          12                 2\n",
              "11          15                 1\n",
              "12          16                 1\n",
              "13          17                 1\n",
              "14          19                 1\n",
              "15          20                 1\n",
              "16          25                 1\n",
              "17          36                 1\n",
              "18          45                 1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Group Size</th>\n",
              "      <th>Number of Groups</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a9d8c98-882c-4ad9-ad8a-93eae7752b17');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ea70625c-214b-4135-a064-2070039d702a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea70625c-214b-4135-a064-2070039d702a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ea70625c-214b-4135-a064-2070039d702a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_67da8cd5-4642-4337-aae6-006e3b864ad9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('group_distribution')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_67da8cd5-4642-4337-aae6-006e3b864ad9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('group_distribution');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "group_distribution",
              "summary": "{\n  \"name\": \"group_distribution\",\n  \"rows\": 19,\n  \"fields\": [\n    {\n      \"column\": \"Group Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 2,\n        \"max\": 45,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          2,\n          7,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number of Groups\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50,\n        \"min\": 1,\n        \"max\": 221,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          49,\n          5,\n          221\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total grups: 347\n",
            "Total similar images: 1176\n",
            "Average group size: 3.39\n",
            "Largest group: 45 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = total_similar_images / len(image_paths) * 100\n",
        "print(f\"Percentage of similar images in the dataset: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiPpGBEaP3n2",
        "outputId": "23915760-599a-462f-9303-0454fa27b4d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of similar images in the dataset: 34.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, 34.33% of all images in our AERALIS dataset are similar. This is not ideal. \\\n",
        "But don't worry! We can keep all the images and still avoid overfitting or data leakage by using another technique: *Group-Aware Splitting*."
      ],
      "metadata": {
        "id": "Amm_VzC3VkCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is similar to the more classical Stratified Sampling, but it is more suitable for our case. \\\n",
        "So let’s start using this technique to properly create the Training, Validation, and Test sets. \\\n",
        "\n",
        "But before that, I think it could be interesting to see how the pHash results change when we adjust the similarity threshold."
      ],
      "metadata": {
        "id": "7q2l-8-vXmSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_similar_images(hashes, threshold): # filters similar images by threshold\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "\n",
        "  Returns:\n",
        "    similar_images: list of similar images\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "  pairs = combinations(hashes.items(), 2)\n",
        "\n",
        "  for (path1, hash1), (path2, hash2) in pairs:\n",
        "    dist = hash1 - hash2\n",
        "    if dist <= threshold:\n",
        "      similar_images.append({'image1': path1, 'image2': path2, 'distance': dist})\n",
        "\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "jaP14gkPahgx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see Just the pHash case\n",
        "HASH_METHODS = ['phash']\n",
        "thresholds_to_try = [3, 5, 7, 10]\n",
        "\n",
        "hashes_phash_all = compute_all_hashes(image_paths, HASH_METHODS)['phash'] # Extracts only 'phash' from the returned dictionary\n",
        "\n",
        "results = []\n",
        "for thresh in thresholds_to_try: # analyzes each threshold\n",
        "  similar_images = filter_similar_images(hashes_phash_all, thresh) # for each threshold, calculate similar images with that threshold\n",
        "\n",
        "  # Extracts all the images that appear at least once as similar (without duplicates):\n",
        "  img_set = set()\n",
        "  for pair in similar_images:\n",
        "    img_set.add(pair['image1'])\n",
        "    img_set.add(pair['image2'])\n",
        "\n",
        "  results.append({\n",
        "    \"Threshold\": thresh,\n",
        "    \"Num Similar Pairs\": len(similar_images),\n",
        "    \"Num Similar Distinct Images\": len(img_set),\n",
        "    \"Total Images\": len(image_paths),\n",
        "    \"Percent Similar (%)\": round(len(img_set) / len(image_paths) * 100, 2)\n",
        "  })\n",
        "\n",
        "\n",
        "# To see the results let's converts the list of results to a DataFrame pandas\n",
        "df_results = pd.DataFrame(results)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QYHuj26nZKRk",
        "outputId": "8f2b698a-eb5e-47a9-b266-02399f2dd085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation phash for all images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Threshold  Num Similar Pairs  Num Similar Distinct Images  Total Images  \\\n",
              "0          3               1450                          839          3426   \n",
              "1          5               2307                         1176          3426   \n",
              "2          7               3161                         1494          3426   \n",
              "3         10               4960                         1971          3426   \n",
              "\n",
              "   Percent Similar (%)  \n",
              "0                24.49  \n",
              "1                34.33  \n",
              "2                43.61  \n",
              "3                57.53  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d0938d4-530e-4553-9651-e62abaf90f0d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Num Similar Pairs</th>\n",
              "      <th>Num Similar Distinct Images</th>\n",
              "      <th>Total Images</th>\n",
              "      <th>Percent Similar (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1450</td>\n",
              "      <td>839</td>\n",
              "      <td>3426</td>\n",
              "      <td>24.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2307</td>\n",
              "      <td>1176</td>\n",
              "      <td>3426</td>\n",
              "      <td>34.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>3161</td>\n",
              "      <td>1494</td>\n",
              "      <td>3426</td>\n",
              "      <td>43.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>4960</td>\n",
              "      <td>1971</td>\n",
              "      <td>3426</td>\n",
              "      <td>57.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d0938d4-530e-4553-9651-e62abaf90f0d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d0938d4-530e-4553-9651-e62abaf90f0d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d0938d4-530e-4553-9651-e62abaf90f0d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7a94ffa5-cc1a-44dd-b600-6cd10fc20031\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a94ffa5-cc1a-44dd-b600-6cd10fc20031')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7a94ffa5-cc1a-44dd-b600-6cd10fc20031 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_43727316-e02b-4229-aa0f-3a4677529cae\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_43727316-e02b-4229-aa0f-3a4677529cae button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Threshold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 3,\n        \"max\": 10,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          10,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Num Similar Pairs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1499,\n        \"min\": 1450,\n        \"max\": 4960,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2307,\n          4960,\n          1450\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Num Similar Distinct Images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 481,\n        \"min\": 839,\n        \"max\": 1971,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1176,\n          1971,\n          839\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3426,\n        \"max\": 3426,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3426\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Percent Similar (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.059890943152203,\n        \"min\": 24.49,\n        \"max\": 57.53,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          34.33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that as the threshold increases, the number of pairs considered similar also grows, and consequently, so does the percentage of images involved.\n",
        "\n",
        "Observations:\n",
        "- At lower thresholds (e.g., 3), only strongly similar images are identified, but many less obvious duplicates may be missed.\n",
        "\n",
        "- At higher thresholds (e.g., 10), there's a risk of including different images that only share generic visual elements (false positives).\n",
        "\n",
        "- Threshold 5 proves to be a good compromise, balancing precision and coverage."
      ],
      "metadata": {
        "id": "41lbIz8GjUP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now create a function that uses the group_aware splitting technique:"
      ],
      "metadata": {
        "id": "ObR-Eb5b5nuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divides the groups of similar images into training, validation, and test sets, keeping each group together\n",
        "#  (no similar images end up in different sets).\n",
        "def group_aware_split(groups, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    groups: list of groups\n",
        "    train_ratio: ratio of images to be assigned to the training set\n",
        "    val_ratio: ratio of images to be assigned to the validation set\n",
        "    test_ratio: ratio of images to be assigned to the test set\n",
        "    seed: seed for the random number generator\n",
        "\n",
        "  Returns:\n",
        "    assignments: dictionary with the split of images\n",
        "  \"\"\"\n",
        "  # checks whether the sets add up to 1\n",
        "  try:\n",
        "    total_ratio = train_ratio + val_ratio + test_ratio\n",
        "    if not 0.99 <= total_ratio <= 1.01:\n",
        "      raise ValueError(\"The proportions do not add up to 1! You must correct the values.\")\n",
        "  except ValueError as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    return None\n",
        "\n",
        "  split_ratios = {'train': train_ratio, 'val': val_ratio, 'test': test_ratio}\n",
        "\n",
        "  random.seed(seed) # to initialize the random number generator\n",
        "  random.shuffle(groups) # to make randomization reproducible\n",
        "\n",
        "  # dictionary comprehension\n",
        "  total_images = sum(len(g) for g in groups) # to figure out how many images should be assigned to that split\n",
        "  target_counts = {k: int(v * total_images) for k, v in split_ratios.items()}\n",
        "  current_counts = defaultdict(int) # number of images already assigned to each split\n",
        "  assignments = defaultdict(list) # number of images actually assigned to each split as final output\n",
        "\n",
        "  for group in groups:\n",
        "    # Find the split with the lowest saturation ratio\n",
        "    best_split = min(\n",
        "      target_counts.items(),\n",
        "      key=lambda item: current_counts[item[0]] / item[1] if item[1] > 0 else float('inf')\n",
        "    )[0] # this line is used to take the key (‘train’, ‘val’, ‘test’) of the best split\n",
        "\n",
        "    assignments[best_split].extend(group) # adds all images in the group to the selected split\n",
        "    current_counts[best_split] += len(group) # update the counter to know how many images are now in that split\n",
        "\n",
        "  return assignments"
      ],
      "metadata": {
        "id": "se5sCN7Lele7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function we get a *assignments* dictionary structured so that each list contains the names of the images assigned to the split (train, val, test), keeping similar images together."
      ],
      "metadata": {
        "id": "O5wxLy2P6X32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# invokes the function to divide the groups of similar images into the different sets\n",
        "assignments = group_aware_split(groups, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42)\n",
        "\n",
        "# check the counts\n",
        "print({k: len(v) for k, v in assignments.items()})\n"
      ],
      "metadata": {
        "id": "bVs4GxlHcg-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70f4710-b817-40bf-8205-733233061822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 820, 'val': 178, 'test': 178}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partitioning the dataset into: \\\n",
        "- Training set = 70%,\n",
        "- Validation set = 15%,\n",
        "- Test set = 15%\n",
        "\n",
        "and we obtain a distribution according to:\n",
        "- 820 images out of 1176 for the Training set\n",
        "- 178 images out of 1176 for the Validation set\n",
        "- 178 images out of 1176 for the Test set"
      ],
      "metadata": {
        "id": "eTI4FyCt8R2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "Now we need to use a *Stratified Split* that ensures that the distribution of classes in the dataset is proportionally balanced across the divisions of the three sets. \\\n",
        "We prefer to use a **Stratified Sampling** technique because we already know that our dataset is somewhat unbalanced, as it contains more images with people than images without.\n"
      ],
      "metadata": {
        "id": "vFX11HDvBHuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, of course, we want to maintain the partitioning we just did for similar images:"
      ],
      "metadata": {
        "id": "SKakySqXCwJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divides the AERALIS dataset in a layered manner and copies images/.xml to train/val/test.\n",
        "#   - Maintains similar image assignments (from group_aware_split).\n",
        "#   - Stratifies remainder split based on CSV ‘class’ column.\n",
        "#   - Saves images, annotations and generates CSV for each set with all original columns.\n",
        "def stratified_split(assignments, image_folder_path, output_base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    assignments: dictionary with the split of images\n",
        "    image_folder_path: path to the folder containing images (and CSV file)\n",
        "    output_base_path: path to the output folder\n",
        "    train_ratio, val_ratio, test_ratio: desired proportions of images to be assigned to the training, validation and test set\n",
        "    seed: seed for the random number generator\n",
        "  \"\"\"\n",
        "\n",
        "  # 1. Create train/val/test folders with subfolders images/ and annotations/\n",
        "  for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(output_base_path, split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_base_path, split, \"annotations\"), exist_ok=True)\n",
        "\n",
        "  # 2. Upload the full CSV\n",
        "  csv_path = os.path.join(image_folder_path, \"aeralis_person_labels.csv\")\n",
        "  df_full = pd.read_csv(csv_path)\n",
        "  df_full = df_full[df_full['filename'].str.lower().str.endswith(('.jpg', '.jpeg', '.png'))] # we really only need '.jpg'\n",
        "  df = df_full.drop_duplicates(subset='filename', keep='first').copy()\n",
        "\n",
        "  # df = pd.read_csv(csv_path)\n",
        "  # df = df[df['filename'].str.lower().str.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "  # 3. Removes images already assigned (similar)\n",
        "  already_assigned = set(sum(assignments.values(), []))\n",
        "  df_unassigned = df[~df['filename'].isin(already_assigned)].copy()\n",
        "\n",
        "  # 4. Split is stratified according to y labels, class balance is maintained\n",
        "  X = df_unassigned['filename'].values\n",
        "  y = df_unassigned['class'].values\n",
        "\n",
        "  X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    train_size = train_ratio,\n",
        "    stratify = y,\n",
        "    random_state = seed\n",
        "  )\n",
        "  # X_train, y_train: part that will go into the training set\n",
        "  # X_temp, y_temp: remaining images to be split still in validation and testing\n",
        "\n",
        "  # After removing the train part, we need to divide X_temp into val and test:\n",
        "  val_ratio_adjusted = val_ratio / (val_ratio + test_ratio) # we calculate the new proportion of validation to the remaining total\n",
        "\n",
        "  # We divide X_temp and y_temp into validation and test:\n",
        "  X_val, X_test = train_test_split(\n",
        "    X_temp, train_size = val_ratio_adjusted,\n",
        "    stratify = y_temp,\n",
        "    random_state = seed\n",
        "  )\n",
        "\n",
        "  # 5. Adds assignments to the dictionary, avoiding duplicates\n",
        "  # assignments is the dictionary initially created with the similar groups assigned via Group-Aware Splitting\n",
        "  # X_train, X_val, X_test are the non-similar, stratified image assignments\n",
        "  for split, split_X in zip(['train', 'val', 'test'], [X_train, X_val, X_test]):\n",
        "    new_imgs = [x for x in split_X if x not in already_assigned]\n",
        "    assignments[split].extend(new_imgs)\n",
        "    already_assigned.update(new_imgs)\n",
        "\n",
        "\n",
        "\n",
        "  # 6. Copy file and generate final CSV\n",
        "  for split, file_list in assignments.items():\n",
        "    # split_df = df[df['filename'].isin(file_list)].copy()\n",
        "    split_df = df_full[df_full['filename'].isin(file_list)].copy()\n",
        "\n",
        "    for fname in file_list:\n",
        "      img_path = os.path.join(image_folder_path, fname)\n",
        "      xml_name = os.path.splitext(fname)[0] + \".xml\"\n",
        "      xml_path = os.path.join(image_folder_path, xml_name)\n",
        "\n",
        "      dst_img = os.path.join(output_base_path, split, \"images\", fname)\n",
        "      dst_xml = os.path.join(output_base_path, split, \"annotations\", xml_name)\n",
        "\n",
        "      if os.path.exists(img_path):\n",
        "        shutil.copy2(img_path, dst_img)\n",
        "      if os.path.exists(xml_path):\n",
        "        shutil.copy2(xml_path, dst_xml)\n",
        "\n",
        "    # save detailed CSV for split\n",
        "    split_df.to_csv(os.path.join(output_base_path, f\"{split}_set.csv\"), index=False)\n",
        "\n",
        "  # 7. Report\n",
        "  print(\"Stratified split completed and files copied.\")\n",
        "  print(f\"Train: {len(assignments['train'])} images\")\n",
        "  print(f\"Val:   {len(assignments['val'])} images\")\n",
        "  print(f\"Test:  {len(assignments['test'])} images\")"
      ],
      "metadata": {
        "id": "CSPJq6NMvEh4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function we are going to create a new folder containing 3 subfolders for the Training, Validation and Test phases, each of which will contain a folder with images and one with their respective annotations. At the same time a CSV file describing the set will be created."
      ],
      "metadata": {
        "id": "L3p41XS5C4-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\"\n",
        "\n",
        "output_base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "# Initialize assignments (only if is not initialized) with: assignments = group_aware_split(gruppi_simili)\n",
        "\n",
        "stratified_split(assignments, image_folder_path, output_base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeYrKkd6m0PF",
        "outputId": "1b38c920-e6ac-4915-d8d8-6bad07801373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified split completed and files copied.\n",
            "Train: 2395 images\n",
            "Val:   515 images\n",
            "Test:  516 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if the numbers matches\n",
        "total_all = len(assignments['train']) + len(assignments['val']) + len(assignments['test'])\n",
        "unique_total = len(set(assignments['train'] + assignments['val'] + assignments['test']))\n",
        "\n",
        "print(f\"Total images in split (sum): {total_all}\")\n",
        "print(f\"Total unique images: {unique_total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJucdFYFpQOQ",
        "outputId": "70199705-fdf3-418e-dece-d379af6f84de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in split (sum): 3426\n",
            "Total unique images: 3426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now perform a quick check to see if the split did not cause inconsistencies in the data:"
      ],
      "metadata": {
        "id": "PCt5LTQ-0Sga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = ['train', 'val', 'test']\n",
        "base_path = output_base_path  # già definito\n",
        "\n",
        "for split in splits:\n",
        "  img_dir = os.path.join(base_path, split, 'images')\n",
        "  ann_dir = os.path.join(base_path, split, 'annotations')\n",
        "\n",
        "  images = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "  missing_xml = []\n",
        "\n",
        "  for img in images:\n",
        "    xml_name = os.path.splitext(img)[0] + \".xml\"\n",
        "    if not os.path.exists(os.path.join(ann_dir, xml_name)):\n",
        "      missing_xml.append(xml_name)\n",
        "\n",
        "  print(f\"{split.upper()} Images: {len(images)}, Missing XML: {len(missing_xml)}\")\n",
        "  if missing_xml:\n",
        "    print(\"\\nMissing XML files:\")\n",
        "    for x in missing_xml:\n",
        "      print(\"   \", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtbLNpZtqY-a",
        "outputId": "ba8c569b-4e11-4f80-f1c4-121c230c5aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN Images: 2395, Missing XML: 0\n",
            "VAL Images: 515, Missing XML: 0\n",
            "TEST Images: 516, Missing XML: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in splits:\n",
        "  csv_path = os.path.join(base_path, f\"{split}_set.csv\")\n",
        "  img_dir = os.path.join(base_path, split, \"images\")\n",
        "\n",
        "  df_split = pd.read_csv(csv_path)\n",
        "  csv_filenames = set(df_split['filename'].str.lower())\n",
        "  actual_images = set(f.lower() for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
        "\n",
        "  missing_in_csv = actual_images - csv_filenames # files present in folder but NOT in CSV\n",
        "  missing_in_dir = csv_filenames - actual_images # files present in CSV but NOT in folder\n",
        "\n",
        "  print(f\"{split.upper()} — CSV: {len(csv_filenames)}, IMG DIR: {len(actual_images)}\")\n",
        "  if missing_in_csv:\n",
        "    print(f\"   {len(missing_in_csv)} images in folder not listed in CSV.\")\n",
        "  if missing_in_dir:\n",
        "    print(f\"   {len(missing_in_dir)} images in CSV not found in folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Atvs9VjpXEx",
        "outputId": "0b4fc9a9-fe46-4912-f3e4-67c32cf05c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN — CSV: 2395, IMG DIR: 2395\n",
            "VAL — CSV: 515, IMG DIR: 515\n",
            "TEST — CSV: 516, IMG DIR: 516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! There are no inconsistencies resulting from the split. \\\n",
        " We now proceed to create copies of the folder we just created, *AERALIS_SPLITTED*, as we want to ensure that the future study of the models' performance is not affected by different splits. Therefore, we will use the same Train, Val, and Test proportions for all of them, as we have just created"
      ],
      "metadata": {
        "id": "zUwwwXtj4LQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remember: base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "model_YOLO_versions = [\"YOLOv8n\", \"YOLOv11n\"]\n",
        "\n",
        "for v in model_YOLO_versions:\n",
        "    dst = f\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_{v}\" # constructs the destination path\n",
        "    shutil.copytree(base_path, dst)"
      ],
      "metadata": {
        "id": "aC8VT3Ki65Qm",
        "outputId": "976ed477-1875-4035-ed23-49fdb4a98566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-23775955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_YOLO_versions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_{v}\"\u001b[0m \u001b[0;31m# constructs the destination path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    574\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_EfficientDet_versions = [\"EfficientDet_D0\", \"EfficientDet_D1\", \"EfficientDet_D2\"]\n",
        "\n",
        "for v in model_EfficientDet_versions:\n",
        "    dst = f\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_{v}\" # constructs the destination path\n",
        "    shutil.copytree(base_path, dst)"
      ],
      "metadata": {
        "id": "tNVTkCikAWIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_SSD_versions = [\"MobileNetV2_SSD\", \"MobileNetV3_SSD\"]\n",
        "\n",
        "for v in model_SSD_versions:\n",
        "    dst = f\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_{v}\" # constructs the destination path\n",
        "    shutil.copytree(base_path, dst)"
      ],
      "metadata": {
        "id": "Po5BqeOWAXoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next session, we will resize the images and their corresponding annotations for each set. This will eliminate any variability caused by the automatic resizing performed by the different models. \\\n",
        "Each model is designed for inputs of specific dimensions and, for this initial phase of “offline” fine-tuning (i.e., in my development environment), I prefer to test each model in optimal conditions, making the most of its capabilities."
      ],
      "metadata": {
        "id": "o2QXt1OZMmYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resizing\n"
      ],
      "metadata": {
        "id": "0NiC2UuhSrDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I discuss the preprocessing strategies for adapting the dataset to the input requirements of various detection models. \\\n",
        "\n",
        "Originally, I planned to resize all images to the specific size required by each model version (see table below), and to update the bounding boxes and annotation files accordingly. \\\n",
        "This approach, especially with letterbox resize, ensures that images are adapted while maintaining the correct aspect ratio (an essential requirement for YOLO models), which are trained on letterboxed data."
      ],
      "metadata": {
        "id": "RuiW_Bw7T6bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During this phase, it is very important not to resize the images by *stretching* them.\n",
        "We therefore use **Letterbox Resize** to avoid distortions. \\\n",
        "Letterbox resize is a preprocessing technique that:\n",
        "\n",
        " - Allows resizing the image while maintaining the original aspect ratio (i.e., the ratio between the width and height of an image).\n",
        "\n",
        " - Adds padding (black border or any other color) on the remaining sides to exactly match the target size.\n",
        "\n",
        "This ensures that object proportions are preserved and the bounding boxes remain unchanged."
      ],
      "metadata": {
        "id": "Ptn7ggFsNDOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model input sizes:\n",
        "\n",
        "- YOLOv8n and YOLOv11n require image dimensions of 640×640\n",
        "- EfficientDet D0 requires 512×512\n",
        "- EfficientDet D1 requires 640×640\n",
        "- EfficientDet D2 requires 768×768\n",
        "- MobileNetV2 + SSD requires 300×300\n",
        "- MobileNetV3 + SSDLite requires 320×320"
      ],
      "metadata": {
        "id": "hzuodc0tb4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform letterbox resize: resize with retained aspect ratio and padding\n",
        "def letterbox_resize(image, target_size=(640, 640), color=(114, 114, 114)):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image: image to be resized\n",
        "    target_size: tuple = (width, height)\n",
        "    color: tuple = (r, g, b)\n",
        "\n",
        "  Returns:\n",
        "    resized: resized image\n",
        "    scale: scale factor\n",
        "    (pad_x, pad_y): padding values\n",
        "  \"\"\"\n",
        "  orig_h, orig_w = image.shape[:2] # original height and width\n",
        "  target_w, target_h = target_size # desired height and width\n",
        "\n",
        "  scale = min(target_w / orig_w, target_h / orig_h) # maintaining aspect ratio\n",
        "  new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "  resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR) # resize image\n",
        "  pad_x = (target_w - new_w) // 2 # horizontal padding\n",
        "  pad_y = (target_h - new_h) // 2 # vertical padding\n",
        "\n",
        "  # Adding padding to get target size image\n",
        "  padded = cv2.copyMakeBorder(\n",
        "    resized,\n",
        "    pad_y, target_h - new_h - pad_y,\n",
        "    pad_x, target_w - new_w - pad_x,\n",
        "    borderType = cv2.BORDER_CONSTANT, value=color # grey padding\n",
        "  )\n",
        "\n",
        "  return padded, scale, (pad_x, pad_y)"
      ],
      "metadata": {
        "id": "pAktUPQ-NA1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process a single split of the dataset\n",
        "def process_one_folder(img_dir, xml_dir, out_img_dir, out_xml_dir, target_size=(640, 640)):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    img_dir: path to input image folder\n",
        "    xml_dir: path to input xml folder\n",
        "    out_img_dir: path to output image folder\n",
        "    out_xml_dir: path to output xml folder\n",
        "    target_size: tuple = (width, height)\n",
        "  \"\"\"\n",
        "  os.makedirs(out_img_dir, exist_ok=True) # folder output images\n",
        "  os.makedirs(out_xml_dir, exist_ok=True) # folder output xml files\n",
        "\n",
        "  for fname in os.listdir(img_dir):\n",
        "    is_image = fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    xml_filename = os.path.splitext(fname)[0] + \".xml\"\n",
        "    xml_path = os.path.join(xml_dir, xml_filename)\n",
        "    xml_exists = os.path.exists(xml_path)\n",
        "    image = cv2.imread(img_path) if is_image else None # read image if it exist\n",
        "\n",
        "    if is_image and xml_exists and image is not None:\n",
        "      resized_img, scale, (pad_x, pad_y) = letterbox_resize(image, target_size)\n",
        "      cv2.imwrite(os.path.join(out_img_dir, fname), resized_img) # save resized image\n",
        "\n",
        "      # Update XML\n",
        "      tree = ET.parse(xml_path)\n",
        "      root = tree.getroot()\n",
        "\n",
        "      for obj in root.findall('object'): # for each object (bbox) in the XML file\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin = int(float(bbox.find('xmin').text))\n",
        "        ymin = int(float(bbox.find('ymin').text))\n",
        "        xmax = int(float(bbox.find('xmax').text))\n",
        "        ymax = int(float(bbox.find('ymax').text))\n",
        "\n",
        "        # apply scaling + padding to the bbox\n",
        "        xmin = int(xmin * scale + pad_x)\n",
        "        xmax = int(xmax * scale + pad_x)\n",
        "        ymin = int(ymin * scale + pad_y)\n",
        "        ymax = int(ymax * scale + pad_y)\n",
        "\n",
        "        # clamp of values (avoids out-of-picture bbox)\n",
        "        bbox.find('xmin').text = str(max(0, min(xmin, target_size[0])))\n",
        "        bbox.find('ymin').text = str(max(0, min(ymin, target_size[1])))\n",
        "        bbox.find('xmax').text = str(max(0, min(xmax, target_size[0])))\n",
        "        bbox.find('ymax').text = str(max(0, min(ymax, target_size[1])))\n",
        "\n",
        "      # update size in <size> tag\n",
        "      size_tag = root.find('size')\n",
        "      size_tag.find('width').text = str(target_size[0])\n",
        "      size_tag.find('height').text = str(target_size[1])\n",
        "\n",
        "      # Save the new XML file\n",
        "      tree.write(os.path.join(out_xml_dir, xml_filename))\n",
        "\n",
        "    # Error handling/ignora invalid files.\n",
        "    else:\n",
        "      if not is_image: # It is not an image file\n",
        "        print(f\"Ignored file (not image): {img_path}\")\n",
        "      elif not xml_exists:\n",
        "        print(f\"Missing file XML for: {fname}\")\n",
        "      elif image is None:\n",
        "        print(f\"Reading error: {img_path}\")"
      ],
      "metadata": {
        "id": "GzRjT-MXNHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the entire dataset (Training, Validation, Testing)\n",
        "def process_entire_dataset(base_input_dir, base_output_dir, splits=('train', 'val', 'test'), target_size=(640, 640)):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    base_input_dir: path to input folder\n",
        "    base_output_dir: path to output folder\n",
        "    splits: tuple = ('train', 'val', 'test')\n",
        "    target_size: tuple = (width, height)\n",
        "  \"\"\"\n",
        "  for split in splits:\n",
        "    print(f\"\\n Processing split: {split}\")\n",
        "    # Input/output paths for images and annotations.\n",
        "    img_dir = os.path.join(base_input_dir, split, 'images')\n",
        "    xml_dir = os.path.join(base_input_dir, split, 'annotations')\n",
        "    out_img_dir = os.path.join(base_output_dir, split, 'images')\n",
        "    out_xml_dir = os.path.join(base_output_dir, split, 'annotations')\n",
        "\n",
        "    # Process a single split\n",
        "    process_one_folder(img_dir, xml_dir, out_img_dir, out_xml_dir, target_size)\n",
        "\n",
        "  print(\"\\nAll images and annotations have been processed.\")"
      ],
      "metadata": {
        "id": "3TCeW88NNJFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, through detailed analysis, I realized that not all models handle resizing in the same way. \\\n",
        "\n",
        "- YOLO requires and expects letterbox resize, as this is the format used during its training.\n",
        "\n",
        "- EfficientDet and MobileNetV2/V3+SSDLite do not use letterbox: they expect a standard (\"stretch\") resize, potentially with cropping, but no padding. If letterbox is applied, it introduces artificial borders the models have never seen in training, potentially reducing detection accuracy, especially for objects at the image edges."
      ],
      "metadata": {
        "id": "6y-VlKaqoVr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although I have implemented functions for letterbox resizing and verified their correctness, I have decided not to apply any resizing in advance at this stage.\n",
        "Instead, I will perform the fine-tuning directly on the original dataset, letting each official library handle the resizing \"on the fly\" according to the correct pipeline for each model. \\\n",
        "This strategy reduces the risk of inconsistencies, ensures full compatibility with each model's expectations, and simplifies future updates or changes in input size requirements. \\"
      ],
      "metadata": {
        "id": "duRKSlC6pJ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the resizing\n",
        "# base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "# AERALIS_YOLOv8n (640 x 640)\n",
        "# process_entire_dataset(\n",
        "#  base_input_dir = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n\", # input\n",
        "#  base_output_dir = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized\", # output\n",
        "#  splits=('train', 'val', 'test'), # default (can be omitted)\n",
        "#  target_size=(640, 640)\n",
        "#)"
      ],
      "metadata": {
        "id": "WTeWdZXAM07o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If, during the experiments, I find that training becomes too slow or resource-intensive due to the large original images, I will reconsider applying resizing in advance, always ensuring to use the correct method for each model."
      ],
      "metadata": {
        "id": "p4m9nEl4pMVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion"
      ],
      "metadata": {
        "id": "KY5fBoM2WWWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to convert the annotations. \\\n",
        "To ensure consistency, we need to convert all the .xml files (which contain the bounding box annotations for each image) into the appropriate format required by each model.\n",
        "\n",
        "The choice of annotation format mostly depends on the *implementation* of the model we plan to use, not on intrinsic format limitations. \\"
      ],
      "metadata": {
        "id": "fgN68L_dfiM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Documentation vs Implementation**\n",
        " - The official documentation of each training framework describes the required annotation format, but\n",
        "\n",
        " - In practice, it is the training scripts (or the APIs of libraries such as Ultralytics YOLO, TensorFlow Object Detection API, PyTorch Lightning, etc.) that enforce that format."
      ],
      "metadata": {
        "id": "WWzNQePdiut-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLO TXT Format** \\\n",
        "According to the official Ultralytics documentation for YOLO:\n",
        "  - *“One text file per image: Each image in the dataset has a corresponding text file with the same name as the image file and the '.txt' extension.”* \\\n",
        "  (link: https://docs.ultralytics.com/datasets/segment/#supported-dataset-formats)\n",
        "\n",
        "  - *“Convert these annotations into the YOLO .txt file format which Ultralytics supports.”* \\\n",
        "  (link: https://docs.ultralytics.com/datasets/#contribute-new-datasets)\n",
        "\n",
        "This means that if we use Ultralytics YOLO (as we plan to do), the .txt format is mandatory, and each file must contain annotations in normalized coordinates: `<class_id> <x_center> <y_center> <width> <height>`"
      ],
      "metadata": {
        "id": "QV1HwOJKom8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EfficientDet / SSD – COCO JSON Format** \\\n",
        "\n",
        "In PyTorch, the standard loader for object detection is *torchvision.datasets.CocoDetection*, which requires a COCO JSON file. \\\n",
        "\n",
        "Therefore, when using models like EfficientDet or MobileNetV2 + SSD, annotations must be converted to a single .json file in the COCO format. \\\n",
        "This is not optional, as the loader won’t work with .txt files or raw XMLs."
      ],
      "metadata": {
        "id": "Cakx6PPNw57_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*What is a Loader?* \\\n",
        "A loader is a component that:\n",
        " - Reads images and annotations from disk\n",
        " - Applies transforms (resize, padding, augmentation, normalization)\n",
        " - Batches them into tensors for training/inference\n",
        "\n",
        "In PyTorch, these are usually defined as subclasses of torch.utils.data.Dataset."
      ],
      "metadata": {
        "id": "-hQGjiMm6MVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note on EfficientDet** \\\n",
        "For fine-tuning EfficientDet, we will likely use the *rwightman/efficientdet-pytorch* implementation, a faithful and stable PyTorch port of Google's original model. \\\n",
        "\n",
        "I decided not to use the official TensorFlow version, as this project is based on PyTorch workflows. \\\n",
        "\n",
        "But more detailed discussion about model selection and fine-tuning strategy will be provided in a future notebook."
      ],
      "metadata": {
        "id": "GiQrg25s6jK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start to transform the annotations for the YOLO model:"
      ],
      "metadata": {
        "id": "VzW9PQdTWcL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_MAP = {\"person\": 0}  # maps the classes with a numeric ID"
      ],
      "metadata": {
        "id": "7tKqcgqzWa2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bbox(size, box): # Function to convert bounding box from absolute to normalized coordinates in YOLO format\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    size: tuple = (width, height)\n",
        "    box: tuple = (xmin, ymin, xmax, ymax)\n",
        "\n",
        "  Returns:\n",
        "    tuple = (x_center, y_center, w, h)\n",
        "  \"\"\"\n",
        "  dw, dh = 1.0/size[0], 1.0/size[1] # size = (width, height)\n",
        "  x_center = (box[0] + box[2]) / 2.0 * dw # box = (xmin, ymin, xmax, ymax)\n",
        "  y_center = (box[1] + box[3]) / 2.0 * dh\n",
        "  w = (box[2] - box[0]) * dw\n",
        "  h = (box[3] - box[1]) * dh\n",
        "\n",
        "  return x_center, y_center, w, h"
      ],
      "metadata": {
        "id": "h9Cs5h_-xyVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_to_yolo(xml_path: Path, txt_path: Path): # converts a single .xml file to a .txt file in the YOLO style\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_path: Path to input XML annotation file\n",
        "    txt_path: Path to output YOLO .txt annotation file\n",
        "  \"\"\"\n",
        "  tree = ET.parse(xml_path)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  w = int(root.find('size/width').text)\n",
        "  h = int(root.find('size/height').text)\n",
        "\n",
        "  lines = []\n",
        "\n",
        "  for obj in root.findall('object'):\n",
        "    cls = obj.find('name').text\n",
        "    if cls in CLASS_MAP:  # solo se la classe è mappata\n",
        "      xmin = float(obj.find('bndbox/xmin').text)\n",
        "      ymin = float(obj.find('bndbox/ymin').text)\n",
        "      xmax = float(obj.find('bndbox/xmax').text)\n",
        "      ymax = float(obj.find('bndbox/ymax').text)\n",
        "      bbox = convert_bbox((w, h), (xmin, ymin, xmax, ymax))\n",
        "      line = f\"{CLASS_MAP[cls]} \" + \" \".join(f\"{v:.6f}\" for v in bbox)\n",
        "      lines.append(line)\n",
        "\n",
        "  txt_path.write_text(\"\\n\".join(lines) + (\"\\n\" if lines else \"\"))"
      ],
      "metadata": {
        "id": "pDZLZdYKxrcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_convert(xml_dir: Path, txt_dir: Path): # to convert all xml files to one directory\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_dir: Path to input directory containing .xml annotations\n",
        "    txt_dir: Path to output directory for .txt annotations\n",
        "  \"\"\"\n",
        "  txt_dir.mkdir(parents=True, exist_ok=True)\n",
        "  for xml in xml_dir.rglob(\"*.xml\"):\n",
        "    xml_to_yolo(xml, txt_dir / f\"{xml.stem}.txt\")"
      ],
      "metadata": {
        "id": "gdQIWSCFxnpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try for YOLOv8n:"
      ],
      "metadata": {
        "id": "6oSTuGKaUx8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts for TRAIN\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/train/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/train/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for VAL\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/val/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/val/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for TEST\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/test/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/test/labels\") # output\n",
        ")"
      ],
      "metadata": {
        "id": "wrpteaWobg4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now for YOLOv11n as well:"
      ],
      "metadata": {
        "id": "AxhkOKG3Wsee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts for TRAIN\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/train/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/train/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for VAL\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/val/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/val/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for TEST\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/test/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/test/labels\") # output\n",
        ")"
      ],
      "metadata": {
        "id": "gEytdfWoWjro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check everything:"
      ],
      "metadata": {
        "id": "pxt7ee5VX0l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to verify that for each .xml file in a directory there is a corresponding .txt file in the directory of converted YOLO labels:\n",
        "def check_yolo_conversion(xml_dir: Path, txt_dir: Path):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_dir: Path to input directory containing .xml annotations\n",
        "    txt_dir: Path to input directory containing .txt annotations\n",
        "  \"\"\"\n",
        "  xml_files = sorted([f.stem for f in xml_dir.glob(\"*.xml\")])\n",
        "  txt_files = sorted([f.stem for f in txt_dir.glob(\"*.txt\")])\n",
        "\n",
        "  missing_txt = [f for f in xml_files if f not in txt_files]\n",
        "  extra_txt = [f for f in txt_files if f not in xml_files]\n",
        "\n",
        "  if not missing_txt and not extra_txt:\n",
        "    print(\"All .xml files have the corresponding .txt file.\")\n",
        "\n",
        "  else:\n",
        "    print(\"Some matches are not correct:\")\n",
        "\n",
        "    if missing_txt:\n",
        "      print(f\"The following .txt files are missing for: {missing_txt}\")\n",
        "    if extra_txt:\n",
        "      print(f\"Excess .txt file (no corresponding .xml): {extra_txt}\")"
      ],
      "metadata": {
        "id": "meZOX7tCX2fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check\n",
        "\n",
        "# YOLOv8n\n",
        "\n",
        "# TRAIN\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/train/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/train/labels\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/val/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/val/labels\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/test/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized/test/labels\")\n",
        ")"
      ],
      "metadata": {
        "id": "4h1uqcOzYWNu",
        "outputId": "7540885d-377f-43cc-c52b-22555e0f19f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv11n\n",
        "\n",
        "# TRAIN\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/train/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/train/labels\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/val/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/val/labels\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/test/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n_resized/test/labels\")\n",
        ")"
      ],
      "metadata": {
        "id": "QJ1xcra_ZFuO",
        "outputId": "64189201-6a3b-4f4c-cd1a-784d50228740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convert_bbox function and the xml_to_yolo, batch_convert scripts are based on standard methodologies for converting Pascal VOC -> YOLO annotations, as described for example in tutorials on Medium and in open GitHub repositories."
      ],
      "metadata": {
        "id": "AkHmCLgLFVDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can convert also the annotations for the EfficientDet model:"
      ],
      "metadata": {
        "id": "0XMwjGh9xZnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a Python dictionary that associates each class name with a numeric ID\n",
        "def get_label2id(labels_path: str) -> Dict[str, int]:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    labels_path: path to file containing class names\n",
        "\n",
        "  Returns:\n",
        "    dict: dictionary that maps class names to class IDs\n",
        "  \"\"\"\n",
        "  with open(labels_path, 'r') as f:\n",
        "    labels_str = f.read().split()\n",
        "  labels_ids = list(range(1, len(labels_str)+1))\n",
        "\n",
        "  return dict(zip(labels_str, labels_ids))"
      ],
      "metadata": {
        "id": "ZNyS9ao8M01t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Builds a list of complete paths to the annotation files (XML)\n",
        "def get_annpaths(ann_dir_path: str, ann_ids_path: str, ext: str = '') -> List[str]:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    ann_dir_path: path to directory containing annotation files\n",
        "    ann_ids_path: path to file containing annotation IDs\n",
        "    ext: extension of annotation files\n",
        "\n",
        "  Returns:\n",
        "    list: list of complete paths to annotation files\n",
        "  \"\"\"\n",
        "  ext_with_dot = '.' + ext if ext else ''\n",
        "  with open(ann_ids_path, 'r') as f:\n",
        "    ann_ids = f.read().split()\n",
        "\n",
        "  return [os.path.join(ann_dir_path, aid + ext_with_dot) for aid in ann_ids]"
      ],
      "metadata": {
        "id": "c_gM2cAzNAEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracts image information (name, size, ID) from an XML tree\n",
        "def get_image_info(annotation_root, extract_num_from_imgid=True):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    annotation_root: XML tree\n",
        "    extract_num_from_imgid: boolean\n",
        "\n",
        "  Returns:\n",
        "    dict: image information\n",
        "  \"\"\"\n",
        "  path = annotation_root.findtext('path')\n",
        "  filename = os.path.basename(path) if path else annotation_root.findtext('filename')\n",
        "  img_name = os.path.basename(filename)\n",
        "  img_id = os.path.splitext(img_name)[0]\n",
        "  if extract_num_from_imgid and isinstance(img_id, str):\n",
        "    img_id = int(re.findall(r'\\d+', img_id)[0])\n",
        "\n",
        "  size = annotation_root.find('size')\n",
        "  width = int(size.findtext('width'))\n",
        "  height = int(size.findtext('height'))\n",
        "\n",
        "  return {'file_name': filename, 'height': height, 'width': width, 'id': img_id}"
      ],
      "metadata": {
        "id": "1GlSwfssNILj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes an XML object (an <object>) and returns the annotation in COCO format\n",
        "def get_coco_annotation_from_obj(obj, label2id, found_labels=None):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    obj: XML object\n",
        "    label2id: dictionary that maps class names to class IDs\n",
        "    found_labels: list of class names\n",
        "\n",
        "  Returns:\n",
        "    dict: COCO annotation for the object\n",
        "  \"\"\"\n",
        "  label = obj.findtext('name')\n",
        "  if found_labels is not None:\n",
        "    found_labels.add(label)\n",
        "\n",
        "  assert label in label2id, f\"Label '{label}' not in label2id\"\n",
        "  category_id = label2id[label]\n",
        "  bndbox = obj.find('bndbox')\n",
        "  xmin = int(bndbox.findtext('xmin')) - 1\n",
        "  ymin = int(bndbox.findtext('ymin')) - 1\n",
        "  xmax = int(bndbox.findtext('xmax'))\n",
        "  ymax = int(bndbox.findtext('ymax'))\n",
        "  assert xmax > xmin and ymax > ymin\n",
        "  o_width = xmax - xmin\n",
        "  o_height = ymax - ymin\n",
        "\n",
        "  return {\n",
        "    'area': o_width * o_height,\n",
        "    'iscrowd': 0,\n",
        "    'bbox': [xmin, ymin, o_width, o_height],\n",
        "    'category_id': category_id,\n",
        "    'ignore': 0,\n",
        "    'segmentation': []\n",
        "  }"
      ],
      "metadata": {
        "id": "y0QUXRg2NNhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that converts annotations from Pascal VOC (.xml) format to COCO (.json) format\n",
        "def convert_xmls_to_cocojson(annotation_paths: List[str], label2id: Dict[str, int], output_jsonpath: str, extract_num_from_imgid: bool = True):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    annotation_paths: list of paths to annotation files\n",
        "    label2id: dictionary that maps class names to class IDs\n",
        "    output_jsonpath: path to output JSON file\n",
        "    extract_num_from_imgid: boolean\n",
        "  \"\"\"\n",
        "  output_json_dict = {\n",
        "    \"images\": [],\n",
        "    \"type\": \"instances\",\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "  }\n",
        "  bnd_id = 1\n",
        "  found_labels = set()\n",
        "  print('Start converting...')\n",
        "\n",
        "  for a_path in tqdm(annotation_paths):\n",
        "    ann_tree = ET.parse(a_path)\n",
        "    ann_root = ann_tree.getroot()\n",
        "\n",
        "    img_info = get_image_info(ann_root, extract_num_from_imgid)\n",
        "    img_id = img_info['id']\n",
        "    output_json_dict['images'].append(img_info)\n",
        "\n",
        "    for obj in ann_root.findall('object'):\n",
        "      ann = get_coco_annotation_from_obj(obj, label2id, found_labels)\n",
        "      ann.update({'image_id': img_id, 'id': bnd_id})\n",
        "      output_json_dict['annotations'].append(ann)\n",
        "      bnd_id += 1\n",
        "\n",
        "  missing_in_labels = found_labels - set(label2id.keys())\n",
        "  if missing_in_labels:\n",
        "    print(f\"Classes found in XMLs but not present in labels.txt: {missing_in_labels}\")\n",
        "  else:\n",
        "    print(\"All the classes found in the XMLs are present in labels.txt.\")\n",
        "\n",
        "  for label, label_id in label2id.items():\n",
        "    output_json_dict['categories'].append({'supercategory': 'none', 'id': label_id, 'name': label})\n",
        "\n",
        "  with open(output_jsonpath, 'w') as f:\n",
        "    json.dump(output_json_dict, f, indent=2)\n"
      ],
      "metadata": {
        "id": "wsrocwalxY9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reusable function that performs all steps on a base directory\n",
        "def convert_pascalvoc_to_coco(base_dir_path: str):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    base_dir_path: path to base directory containing the dataset\n",
        "  \"\"\"\n",
        "  base_dir = Path(base_dir_path)\n",
        "  splits = [\"train\", \"val\", \"test\"]\n",
        "  labels_path = base_dir / \"labels.txt\"\n",
        "\n",
        "  # Step 1: create labels.txt (for your project: only \"person\")\n",
        "  with open(labels_path, \"w\") as f:\n",
        "    f.write(\"person\\n\")\n",
        "\n",
        "  # Step 2: generate image_list.txt for each split\n",
        "  for split in splits:\n",
        "    xml_dir = base_dir / split / \"annotations\"\n",
        "    output_list = base_dir / split / \"image_list.txt\"\n",
        "\n",
        "    xml_files = sorted(xml_dir.glob(\"*.xml\"))\n",
        "    with open(output_list, \"w\") as f:\n",
        "      for xml_file in xml_files:\n",
        "        f.write(f\"{xml_file.stem}\\n\")\n",
        "    print(f\"{output_list.name} generated with {len(xml_files)} entries.\")\n",
        "\n",
        "  # Step 3: perform conversion from .xml to COCO JSON\n",
        "  label2id = get_label2id(str(labels_path))\n",
        "\n",
        "  for split in splits:\n",
        "    print(f\"\\nConverting {split.upper()} split to COCO JSON...\")\n",
        "    xml_dir = base_dir / split / \"annotations\"\n",
        "    ids_file = base_dir / split / \"image_list.txt\"\n",
        "    output_json = base_dir / f\"annotations_{split}.json\"\n",
        "\n",
        "    annotation_paths = get_annpaths(str(xml_dir), str(ids_file), ext=\"xml\")\n",
        "    convert_xmls_to_cocojson(annotation_paths, label2id, str(output_json))"
      ],
      "metadata": {
        "id": "ZF_qvElF1rRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it:"
      ],
      "metadata": {
        "id": "FpeyvRv32Bby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to each dataset EfficientDet o MobileNetV2/V3+SSD\n",
        "\n",
        "paths = [\n",
        "  \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0\",\n",
        "  \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1\",\n",
        "  \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2\",\n",
        "  \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD\",\n",
        "  \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD\"\n",
        "]\n",
        "\n",
        "for path in paths:\n",
        "  print(f\"\\nProcessing dataset: {path}\")\n",
        "  convert_pascalvoc_to_coco(path)\n"
      ],
      "metadata": {
        "id": "FV4lPho6192R",
        "outputId": "9b6d7315-9ad8-405e-e372-4ff70f7d30a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing dataset: /content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0_resized\n",
            "image_list.txt generated with 2395 entries.\n",
            "image_list.txt generated with 515 entries.\n",
            "image_list.txt generated with 516 entries.\n",
            "\n",
            "Converting TRAIN split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2395/2395 [01:38<00:00, 24.25it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting VAL split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 515/515 [02:35<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting TEST split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [02:42<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing dataset: /content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1_resized\n",
            "image_list.txt generated with 2395 entries.\n",
            "image_list.txt generated with 515 entries.\n",
            "image_list.txt generated with 516 entries.\n",
            "\n",
            "Converting TRAIN split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2395/2395 [01:32<00:00, 25.88it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting VAL split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 515/515 [02:42<00:00,  3.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting TEST split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [02:19<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing dataset: /content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2_resized\n",
            "image_list.txt generated with 2395 entries.\n",
            "image_list.txt generated with 515 entries.\n",
            "image_list.txt generated with 516 entries.\n",
            "\n",
            "Converting TRAIN split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2395/2395 [01:05<00:00, 36.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting VAL split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 515/515 [02:56<00:00,  2.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting TEST split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [02:07<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing dataset: /content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD_resized\n",
            "image_list.txt generated with 2395 entries.\n",
            "image_list.txt generated with 515 entries.\n",
            "image_list.txt generated with 516 entries.\n",
            "\n",
            "Converting TRAIN split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2395/2395 [01:01<00:00, 39.17it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting VAL split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 515/515 [00:02<00:00, 180.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting TEST split to COCO JSON...\n",
            "Start converting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 516/516 [00:02<00:00, 209.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect, we now want to check if everything is ok:"
      ],
      "metadata": {
        "id": "vhe83Vd29qps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_coco_conversion(xml_dir: Path, json_path: Path):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_dir: Path to the folder containing .xml annotations\n",
        "    json_path: Path to the generated COCO JSON file\n",
        "  \"\"\"\n",
        "  # Read .xml files\n",
        "  xml_files = sorted([f.stem for f in xml_dir.glob(\"*.xml\")])\n",
        "\n",
        "  # Upload the COCO JSON file\n",
        "  with open(json_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "  # Takes the names (without extension) of the annotated images\n",
        "  json_files = sorted([Path(img['file_name']).stem for img in coco_data['images']])\n",
        "\n",
        "  missing_in_json = [f for f in xml_files if f not in json_files]\n",
        "  extra_in_json = [f for f in json_files if f not in xml_files]\n",
        "\n",
        "  if not missing_in_json and not extra_in_json:\n",
        "    print(\"All .xml files have a match in the COCO JSON.\")\n",
        "  else:\n",
        "    print(\"Some matches are not correct:\")\n",
        "    if missing_in_json:\n",
        "      print(f\"Missing from the JSON: {missing_in_json}\")\n",
        "    if extra_in_json:\n",
        "      print(f\"Present in JSON but not in XMLs: {extra_in_json}\")\n"
      ],
      "metadata": {
        "id": "KQAMUOfV9wG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check:\n",
        "\n",
        "# EfficientDet D0\n",
        "\n",
        "# TRAIN\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/train/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/annotations_train.json\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/val/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/annotations_val.json\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/test/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/annotations_test.json\")\n",
        ")"
      ],
      "metadata": {
        "id": "NwCEC1In_IAz",
        "outputId": "f96fd551-c052-4ad2-f2ac-2a95387a2873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientDet D1\n",
        "\n",
        "# TRAIN\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1/train/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1/annotations_train.json\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1/val/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1/annotations_val.json\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1/test/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D1/annotations_test.json\")\n",
        ")"
      ],
      "metadata": {
        "id": "1YJ8VYPkBN8F",
        "outputId": "34acf99b-a4ad-40d2-b5fb-16ff2d604c93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientDet D2\n",
        "\n",
        "# TRAIN\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2/train/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2/annotations_train.json\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2/val/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2/annotations_val.json\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2/test/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D2/annotations_test.json\")\n",
        ")"
      ],
      "metadata": {
        "id": "3vtpitx6BNyy",
        "outputId": "088aab4c-78f5-457c-a8dd-700a7c4f3a02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNetV2 + SSD\n",
        "\n",
        "# TRAIN\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD/train/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD/annotations_train.json\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD/val/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD/annotations_val.json\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD/test/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV2_SSD/annotations_test.json\")\n",
        ")"
      ],
      "metadata": {
        "id": "5v3UL-8qBNo6",
        "outputId": "eabf45a6-23ce-41be-fa22-ef3acb690331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n",
            "All .xml files have a match in the COCO JSON.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNetV3 + SSD\n",
        "\n",
        "# TRAIN\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD/train/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD/annotations_train.json\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD/val/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD/annotations_val.json\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_coco_conversion(\n",
        "    xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD/test/annotations\"),\n",
        "    json_path=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_MobileNetV3_SSD/annotations_test.json\")\n",
        ")"
      ],
      "metadata": {
        "id": "GExJUn1EwwAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acknowledgements:** Part of the code for converting annotations from PASCAL VOC (.xml) to COCO JSON is based on an open implementation of Roboflow / yukkyo (https://blog.roboflow.com/how-to-convert-annotations-from-voc-xml-to-coco-json/)."
      ],
      "metadata": {
        "id": "8hzB54z-DgDx"
      }
    }
  ]
}