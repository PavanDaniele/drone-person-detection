{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzfT9P7o8G8zfSX0Dwqte1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/dataset_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "_JEsHziX0SRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7N2nFzq0QrO",
        "outputId": "7476763e-11e9-48d6-fb75-3eefa705363c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this snippet Just one time, to install packages\n",
        "!pip install imagehash\n",
        "!pip install pillow"
      ],
      "metadata": {
        "id": "gUXXRgaMu4gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imagehash\n",
        "import os\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "BzQ632JVvFYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "In this notebook, I'm going to prepare the dataset for fine-tuning multiple deep learning models (e.g. YOLO, EfficientDet, SSD + MobileNetV2).\n",
        "The steps include similarity check, dataset splitting (train/val/test), optional image resizing, and bounding box adaptation.\n",
        "The goal is to generate separate, clean and model-ready datasets for each architecture to enable fair training and evaluation."
      ],
      "metadata": {
        "id": "UcbKdax60kLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\"\n",
        "\n",
        "HASH_METHODS = ['phash', 'ahash', 'dhash']\n",
        "HAMMING_THRESHOLD = 5"
      ],
      "metadata": {
        "id": "n4X5QP7R-bPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(folder_path): # To estract the images file (.jpg) and ignore the .xml and .csv files\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    folder_path: path to folder containing images\n",
        "  Returns:\n",
        "    list of paths to images\n",
        "  \"\"\"\n",
        "  return [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path)\n",
        "               if f.lower().endswith(('.jpg'))]"
      ],
      "metadata": {
        "id": "g7taLCl6-dMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hash(img_path, method):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    img_path: path to image\n",
        "    method: hash method to use\n",
        "  Returns:\n",
        "    hash of image\n",
        "  \"\"\"\n",
        "  img = Image.open(img_path).convert(\"L\")  # Grayscale (because the hash algorithms works best when the image is in black and white)\n",
        "\n",
        "  if method == 'phash':\n",
        "    return imagehash.phash(img)\n",
        "  elif method == 'ahash':\n",
        "    return imagehash.average_hash(img)\n",
        "  elif method == 'dhash':\n",
        "    return imagehash.dhash(img)\n",
        "  else:\n",
        "    raise ValueError(f\"Hash method not supported: {method}\")"
      ],
      "metadata": {
        "id": "GhRdzrjt-7iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_hashes(image_paths, methods): # Hash calculation for each images\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image_paths: list of paths to images\n",
        "    methods: list of hash methods to use\n",
        "  Returns:\n",
        "    dictionary of hashes\n",
        "  \"\"\"\n",
        "  hashes = {method: {} for method in methods} # to create a dictionary and for each method creates an empty sub-dictionary\n",
        "\n",
        "  for method in methods:\n",
        "    print(f\"\\nCalculation {method} for all images\")\n",
        "\n",
        "    for path in image_paths: # cycles over each image path in the image_paths list\n",
        "      try:\n",
        "        h = compute_hash(path, method)\n",
        "        hashes[method][path] = h # saves the calculated hash in the dictionary structure\n",
        "      except Exception as e:\n",
        "        print(f\"Error with {path}: {e}\")\n",
        "\n",
        "  return hashes"
      ],
      "metadata": {
        "id": "JwnEBLh4_eZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_hashes(hashes, threshold): # Comparison of images in pairs\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "\n",
        "  for method in hashes:\n",
        "    print(f\"\\nRisultats with {method.upper()}:\") # .upper() is used to convert the characters to 'uppercase'\n",
        "    pairs = combinations(hashes[method].items(), 2) # combinations() is used to generate all the possible pairs without repetitions\n",
        "\n",
        "    for (path1, hash1), (path2, hash2) in pairs:\n",
        "      dist = hash1 - hash2\n",
        "      if dist <= threshold:\n",
        "        similar_images.append({\n",
        "          'method': method,\n",
        "          'image1': os.path.basename(path1),\n",
        "          'image2': os.path.basename(path2),\n",
        "          'distance': dist\n",
        "        })\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "kLnyPUaoABkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = get_image_paths(image_folder_path)\n",
        "hashes = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images = compare_hashes(hashes, HAMMING_THRESHOLD)\n",
        "\n",
        "# to see how many distine images are considered similar:\n",
        "img_set = set()\n",
        "for entry in similar_images:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2hsW2q0cAl5P",
        "outputId": "c3cabc7d-92ff-4d2c-8428-03089994d715"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_image_paths' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2841535006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_all_hashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHASH_METHODS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcompare_hashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHAMMING_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# per capire quante immagini distine sono considerate simili:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_image_paths' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\"\n",
        "\n",
        "\n",
        "# Parameters\n",
        "HASH_METHODS = ['phash', 'ahash', 'dhash']  # to use pHash, aHash and dHash\n",
        "HAMMING_THRESHOLD = 5  # Soglia per considerare due immagini simili\n",
        "\n",
        "# To estract the images file (.jpg) and ignore the .xml and .csv files\n",
        "image_paths = [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path)\n",
        "               if f.lower().endswith(('.jpg'))]\n",
        "\n",
        "# Hash calculation for each images\n",
        "hashes = {method: {} for method in HASH_METHODS} # to create a dictionary and for each method creates an empty sub-dictionary\n",
        "for method in HASH_METHODS:\n",
        "  print(f\"\\nCalculation {method} for all images\")\n",
        "  for path in image_paths: # cycles over each image path in the image_paths list\n",
        "    try:\n",
        "      img = Image.open(path).convert(\"L\")  # Grayscale (because the hash algorithms works best when the image is in black and white)\n",
        "      if method == 'phash':\n",
        "        h = imagehash.phash(img)\n",
        "      elif method == 'ahash':\n",
        "        h = imagehash.average_hash(img)\n",
        "      elif method == 'dhash':\n",
        "        h = imagehash.dhash(img)\n",
        "      hashes[method][path] = h # saves the calculated hash in the dictionary structure\n",
        "    except Exception as e:\n",
        "      print(f\"Errore con {path}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Comparison of images in pairs\n",
        "for method in HASH_METHODS:\n",
        "    print(f\"\\nRisultats with {method.upper()}:\") # .upper() is used to convert the characters to 'uppercase'\n",
        "    pairs = combinations(hashes[method].items(), 2) # combinations() is used to generate all the possible pairs without repetitions\n",
        "    for (path1, hash1), (path2, hash2) in pairs:\n",
        "      dist = hash1 - hash2\n",
        "      if dist <= HAMMING_THRESHOLD:\n",
        "        print(f\"\\nSIMILAR (distance = {dist}):\")\n",
        "        print(f\" - {os.path.basename(path1)}\")\n",
        "        print(f\" - {os.path.basename(path2)}\")\n"
      ],
      "metadata": {
        "id": "828jVmPb0j58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Installa la libreria necessaria\n",
        "!pip install ImageHash\n",
        "\n",
        "# ⚙️ Import delle librerie\n",
        "import os\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# 📂 Cartella contenente le immagini\n",
        "image_folder = \"/content/drive/MyDrive/tuo_dataset/images\"  # cambia questo path\n",
        "\n",
        "# 🔧 Parametri\n",
        "hash_function = imagehash.phash      # puoi cambiare in ahash, dhash\n",
        "hamming_threshold = 5                # distanza massima per considerare due immagini \"simili\"\n",
        "\n",
        "# 🔍 Funzione per calcolare gli hash\n",
        "def compute_image_hashes(folder, hash_func):\n",
        "    hashes = {}\n",
        "    for fname in os.listdir(folder):\n",
        "        if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            path = os.path.join(folder, fname)\n",
        "            try:\n",
        "                img = Image.open(path)\n",
        "                img_hash = hash_func(img)\n",
        "                hashes[fname] = img_hash\n",
        "            except Exception as e:\n",
        "                print(f\"Errore su {fname}: {e}\")\n",
        "    return hashes\n",
        "\n",
        "# 🔎 Funzione per confrontare le immagini\n",
        "def find_similar_images(hashes, threshold):\n",
        "    similar = defaultdict(list)\n",
        "    files = list(hashes.items())\n",
        "    for i in range(len(files)):\n",
        "        name1, hash1 = files[i]\n",
        "        for j in range(i + 1, len(files)):\n",
        "            name2, hash2 = files[j]\n",
        "            distance = hash1 - hash2\n",
        "            if distance <= threshold:\n",
        "                similar[name1].append((name2, distance))\n",
        "    return similar\n",
        "\n",
        "# 🚀 Esecuzione\n",
        "hashes = compute_image_hashes(image_folder, hash_function)\n",
        "similar_images = find_similar_images(hashes, hamming_threshold)\n",
        "\n",
        "# 📊 Risultato in tabella\n",
        "rows = []\n",
        "for base, similars in similar_images.items():\n",
        "    for sim_name, dist in similars:\n",
        "        rows.append({\"Image 1\": base, \"Image 2\": sim_name, \"Hamming Distance\": dist})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.sort_values(\"Hamming Distance\")\n"
      ],
      "metadata": {
        "id": "HJ1fAkLC2Bj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}