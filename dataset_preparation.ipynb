{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOj+RIELIMtHOLuzd9he1eF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/dataset_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "_JEsHziX0SRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7N2nFzq0QrO",
        "outputId": "ac7c64c2-5d63-414e-f77c-d1af34f23394"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this snippet Just one time, to install packages\n",
        "!pip install imagehash\n",
        "!pip install pillow"
      ],
      "metadata": {
        "id": "gUXXRgaMu4gg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1afaea-cff3-45a9-e46f-1db209895e1f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imagehash) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.16.1)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.2\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import imagehash\n",
        "import os\n",
        "from itertools import combinations # to generate all possible combinations of a number of elements from a set\n",
        "\n",
        "from collections import defaultdict, deque\n",
        "# defaultdict is a special type of dictionary that automatically creates a default value if you access a nonexistent key\n",
        "# deque is a list-like structure, but optimized for quick additions and removals at both ends.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import Counter\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split # to partition the dataset with stratification\n",
        "\n",
        "import numpy as np\n",
        "import cv2 # OpenCV for image manipulation\n",
        "import xml.etree.ElementTree as ET # For parsing and editing XML files (annotations)\n",
        "\n",
        "from pathlib import Path # to manage file paths more robustly\n",
        "import json # to create the final .json file in COCO format\n",
        "from typing import Dict, List # types to improve readability and autocomplete\n",
        "from tqdm import tqdm\n",
        "import re # regular expressions, used to extract numbers from the image name\n"
      ],
      "metadata": {
        "id": "BzQ632JVvFYm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "In this notebook, I'm going to prepare the dataset for fine-tuning multiple deep learning models.\n",
        "The steps include similarity check, dataset splitting (train/val/test), optional image resizing, and bounding box adaptation.\n",
        "The goal is to generate separate, clean and model-ready datasets for each architecture to enable fair training and evaluation."
      ],
      "metadata": {
        "id": "UcbKdax60kLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\""
      ],
      "metadata": {
        "id": "n4X5QP7R-bPd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various metrics to calculate image similarity, such as **SSIM** (Structural Similarity Index), **PSNR** (Peak Signal-to-Noise Ratio), and **Cosine Similarity**. \\\n",
        "In our case, I chose to use **Perceptual Hashing** for an initial check because it is fast, robust, and does not require resizing (which is very important since my AERALIS dataset is composed of images from two different datasets)."
      ],
      "metadata": {
        "id": "G63l7aDrRe-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This technique reduces the image to a binary signature, and then the *Hamming Distance* is computed to compare the resulting binary hashes."
      ],
      "metadata": {
        "id": "golPihqWSwnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(folder_path): # To estract the images file (.jpg) and ignore the .xml and .csv files\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    folder_path: path to folder containing images\n",
        "\n",
        "  Returns:\n",
        "    list of paths to images\n",
        "  \"\"\"\n",
        "  return [os.path.join(folder_path, f)\n",
        "          for f in os.listdir(folder_path)\n",
        "          if f.lower().endswith(('.jpg'))]"
      ],
      "metadata": {
        "id": "g7taLCl6-dMO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_hash(img_path, method):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    img_path: path to image\n",
        "    method: hash method to use\n",
        "\n",
        "  Returns:\n",
        "    hash of image\n",
        "  \"\"\"\n",
        "  img = Image.open(img_path).convert(\"L\") # Grayscale (because the hash algorithms works best when the image is in black and white)\n",
        "\n",
        "  if method == 'phash':\n",
        "    return imagehash.phash(img)\n",
        "  elif method == 'ahash':\n",
        "    return imagehash.average_hash(img)\n",
        "  elif method == 'dhash':\n",
        "    return imagehash.dhash(img)\n",
        "  else:\n",
        "    raise ValueError(f\"Hash method not supported: {method}\")"
      ],
      "metadata": {
        "id": "GhRdzrjt-7iJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_all_hashes(image_paths, methods): # Hash calculation for each images\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image_paths: list of paths to images\n",
        "    methods: list of hash methods to use\n",
        "\n",
        "  Returns:\n",
        "    dictionary of hashes\n",
        "  \"\"\"\n",
        "  hashes = {method: {} for method in methods} # to create a dictionary and for each method creates an empty sub-dictionary\n",
        "\n",
        "  for method in methods:\n",
        "    print(f\"\\nCalculation {method} for all images\")\n",
        "\n",
        "    for path in image_paths: # cycles over each image path in the image_paths list\n",
        "      try:\n",
        "        h = compute_hash(path, method)\n",
        "        hashes[method][path] = h # saves the calculated hash in the dictionary structure\n",
        "      except Exception as e:\n",
        "        print(f\"Error with {path}: {e}\")\n",
        "\n",
        "  return hashes"
      ],
      "metadata": {
        "id": "JwnEBLh4_eZK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_hashes(hashes, threshold): # Comparison of images in pairs\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "\n",
        "  for method in hashes:\n",
        "    print(f\"\\nRisultats with {method.upper()}:\") # .upper() is used to convert the characters to 'uppercase'\n",
        "    pairs = combinations(hashes[method].items(), 2) # combinations() is used to generate all the possible pairs without repetitions\n",
        "\n",
        "    for (path1, hash1), (path2, hash2) in pairs:\n",
        "      dist = hash1 - hash2\n",
        "      if dist <= threshold:\n",
        "        similar_images.append({ 'method': method, 'image1': os.path.basename(path1), 'image2': os.path.basename(path2), 'distance': dist })\n",
        "\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "kLnyPUaoABkT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hamming distance between the hashes of two images tells us how visually similar they are.\n",
        "The result depends on the threshold:\n",
        "\n",
        "- 1-2 (Very strict) → Only nearly identical images are detected\n",
        "- 3-5 (Good compromise) → Balances well between false positives and false negatives\n",
        "- 6-10 (More permissive) → More images are considered similar, but false positives increas"
      ],
      "metadata": {
        "id": "6eVBBGKlRYGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate one Hash at time:"
      ],
      "metadata": {
        "id": "5SumHPJqPz7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['phash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "image_paths = get_image_paths(image_folder_path)\n",
        "hashes_phash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_phash = compare_hashes(hashes_phash, HAMMING_THRESHOLD)\n",
        "\n",
        "# to see how many distine images are considered similar:\n",
        "img_set = set()\n",
        "for entry in similar_images_phash:\n",
        "  img_set.add(entry['image1'])\n",
        "  img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "id": "4LQEK2SeZAo8",
        "outputId": "81cc77b9-1239-4285-ab2f-7d7e47f26724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation phash for all images\n",
            "\n",
            "Risultats with PHASH:\n",
            "Method: ['phash'] \n",
            "\n",
            "Number of similar distinct images: 1163\n",
            "Number of All images: 3446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['ahash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "hashes_ahash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_ahash = compare_hashes(hashes_ahash, HAMMING_THRESHOLD)\n",
        "\n",
        "img_set = set()\n",
        "for entry in similar_images_ahash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyYxIHEyPG-V",
        "outputId": "223d175e-2cf0-414f-a05e-fb1a6f56f464"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation ahash for all images\n",
            "\n",
            "Risultats with AHASH:\n",
            "Method: ['ahash'] \n",
            "\n",
            "Number of similar distinct images: 2079\n",
            "Number of All images: 3446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HASH_METHODS = ['dhash']\n",
        "HAMMING_THRESHOLD = 5\n",
        "\n",
        "hashes_dhash = compute_all_hashes(image_paths, HASH_METHODS)\n",
        "similar_images_dhash = compare_hashes(hashes_dhash, HAMMING_THRESHOLD)\n",
        "\n",
        "img_set = set()\n",
        "for entry in similar_images_dhash:\n",
        "    img_set.add(entry['image1'])\n",
        "    img_set.add(entry['image2'])\n",
        "\n",
        "print(f\"Method: {HASH_METHODS} \\n\")\n",
        "print(f\"Number of similar distinct images: {len(img_set)}\")\n",
        "print(f\"Number of All images: {len(image_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0vmBsTBPvs3",
        "outputId": "7688bb75-048f-43c1-9d99-2a2b92bf9d45"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation dhash for all images\n",
            "\n",
            "Risultats with DHASH:\n",
            "Method: ['dhash'] \n",
            "\n",
            "Number of similar distinct images: 1351\n",
            "Number of All images: 3446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that the number of similar images is quite high.\n",
        "Instead of simply removing them (which would unnecessarily reduce the dataset size), we adopt a more conservative strategy: we will distribute these similar images carefully across the training, validation, and test sets, in order to prevent potential overfitting or data leakage."
      ],
      "metadata": {
        "id": "KfOmx2Dgy9H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose to use only perceptual hashing (pHash), as it is more robust to minor variations in images and less prone to false positives compared to other variants like aHash and dHash."
      ],
      "metadata": {
        "id": "reqBRtibzctE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But now I want to formulate a hypothesis:\n",
        "Are we sure that all the 1176 images identified as \"similar\" by pHash are truly similar to each other? \\\n",
        "It could be that these images do not all resemble each other directly, but instead form subgroups (clusters) of mutually similar images, while being different from those in other groups. \\\n",
        "Let’s try to test this assumption."
      ],
      "metadata": {
        "id": "KOfa7LzQWagn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To model this relationship, I built a data structure based on an undirected graph, where:\n",
        "\n",
        " - each node represents an image\n",
        " - an edge connects two images if they are considered similar\n",
        "\n",
        "We then extracted the connected components from this graph, which effectively represent the actual clusters of similar images. These groups will be used to perform a controlled split of the dataset.\n",
        "\n",
        "*Remember: a connected component is a maximal subset of a set (space) in which all points (nodes) are connected to each other.*"
      ],
      "metadata": {
        "id": "npWjdFeI0PLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructs an undirected graph in which each image is a node and each similar pair is an arc:\n",
        "def build_similarity_graph(similar_images):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    similar_images: list of similar images\n",
        "\n",
        "  Returns:\n",
        "    graph: dictionary of graph\n",
        "  \"\"\"\n",
        "  graph = defaultdict(set) # creates a dictionary (key: name_img, val: set_of_images)\n",
        "\n",
        "  for pair in similar_images: # scrolls each element(=list of dictionaries) of similar_images\n",
        "    img1 = pair['image1']\n",
        "    img2 = pair['image2']\n",
        "    graph[img1].add(img2) # builds the connection in both directions (undirected arc)\n",
        "    graph[img2].add(img1)\n",
        "\n",
        "  return graph"
      ],
      "metadata": {
        "id": "0FH39M_Hg-Su"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finds the groups (connected components) in the graph:\n",
        "def find_connected_components(graph):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    graph: dictionary of graph\n",
        "\n",
        "  Returns:\n",
        "    groups: list of groups\n",
        "  \"\"\"\n",
        "  visited = set() # keeps track of images already visited\n",
        "  groups = [] # will contain the final groups\n",
        "\n",
        "  for node in sorted(graph): # scrolls each node(=image) in the graph (is important to order the nodes to ensure stability)\n",
        "    if node not in visited: # If the image has not yet been visited, then start a new group\n",
        "      group = []\n",
        "\n",
        "      # Start a BFS (Breadth-First Search) with a queue\n",
        "      # adds the initial node to the queue and marks it as visited\n",
        "      queue = deque([node])\n",
        "      visited.add(node)\n",
        "\n",
        "      while queue: # as long as there are nodes in the tail\n",
        "        current = queue.popleft() # removes the knot from the head and adds it to the group\n",
        "        group.append(current)\n",
        "\n",
        "        # for each neighbor (similar image), if not already visited\n",
        "        for neighbor in sorted(graph[current]): # (is important to order the nodes to ensure stability)\n",
        "          if neighbor not in visited:\n",
        "            visited.add(neighbor) # marks it as visited\n",
        "            queue.append(neighbor) # puts it in the queue for trial\n",
        "\n",
        "      # Once the queue is exhausted, the group is complete and it is added to the groups\n",
        "      groups.append(group)\n",
        "  return groups"
      ],
      "metadata": {
        "id": "CCd1p_gQiP4U"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = build_similarity_graph(similar_images_phash)\n",
        "groups = find_connected_components(graph)\n",
        "\n",
        "print(f\"\\nFound {len(groups)} groups of similar images.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Vk1UPdiQRz",
        "outputId": "3b65161b-348b-42ce-bba4-28e7c179bc28"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 346 groups of similar images.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! We were right! \\\n",
        "Now let's do a brief analysis"
      ],
      "metadata": {
        "id": "xBPdoG29IFLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group_sizes = [len(group) for group in groups]\n",
        "\n",
        "size_counts = Counter(group_sizes) # count how many groups have size X\n",
        "\n",
        "# Sort and save to a DataFrame by display\n",
        "group_distribution = pd.DataFrame(sorted(size_counts.items()), columns=[\"Group Size\", \"Number of Groups\"])\n",
        "display(group_distribution)\n",
        "\n",
        "# Other useful statistics\n",
        "total_similar_images = sum(group_sizes)\n",
        "largest_group = max(group_sizes)\n",
        "average_group_size = total_similar_images / len(groups)\n",
        "\n",
        "print(f\"Total grups: {len(groups)}\")\n",
        "print(f\"Total similar images: {total_similar_images}\")\n",
        "print(f\"Average group size: {average_group_size:.2f}\")\n",
        "print(f\"Largest group: {largest_group} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "id": "japx8S0hINtf",
        "outputId": "81a4a9d6-a7d2-4a07-b975-c74585bad762"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Group Size  Number of Groups\n",
              "0            2               220\n",
              "1            3                50\n",
              "2            4                23\n",
              "3            5                20\n",
              "4            6                 9\n",
              "5            7                 4\n",
              "6            8                 5\n",
              "7            9                 2\n",
              "8           10                 2\n",
              "9           11                 2\n",
              "10          12                 2\n",
              "11          15                 1\n",
              "12          16                 1\n",
              "13          19                 1\n",
              "14          20                 1\n",
              "15          25                 1\n",
              "16          35                 1\n",
              "17          45                 1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4259500f-54f6-4613-a965-6afd5009bd9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Group Size</th>\n",
              "      <th>Number of Groups</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4259500f-54f6-4613-a965-6afd5009bd9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4259500f-54f6-4613-a965-6afd5009bd9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4259500f-54f6-4613-a965-6afd5009bd9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9a17ff24-ece1-4af3-9877-ad69d51365d6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9a17ff24-ece1-4af3-9877-ad69d51365d6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9a17ff24-ece1-4af3-9877-ad69d51365d6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_0b308931-2a5e-4b55-9918-d73e9b5952d0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('group_distribution')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0b308931-2a5e-4b55-9918-d73e9b5952d0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('group_distribution');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "group_distribution",
              "summary": "{\n  \"name\": \"group_distribution\",\n  \"rows\": 18,\n  \"fields\": [\n    {\n      \"column\": \"Group Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 2,\n        \"max\": 45,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          2,\n          3,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number of Groups\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51,\n        \"min\": 1,\n        \"max\": 220,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          2,\n          50,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total grups: 346\n",
            "Total similar images: 1163\n",
            "Average group size: 3.36\n",
            "Largest group: 45 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = total_similar_images / len(image_paths) * 100\n",
        "print(f\"Percentage of similar images in the dataset: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiPpGBEaP3n2",
        "outputId": "2ef6b933-9831-4902-a2f9-9912acf6be96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of similar images in the dataset: 33.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, 34.33% of all images in our AERALIS dataset are similar. This is not ideal. \\\n",
        "But don't worry! We can keep all the images and still avoid overfitting or data leakage by using another technique: *Group-Aware Splitting*."
      ],
      "metadata": {
        "id": "Amm_VzC3VkCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is similar to the more classical Stratified Sampling, but it is more suitable for our case. \\\n",
        "So let’s start using this technique to properly create the Training, Validation, and Test sets. \\\n",
        "\n",
        "But before that, I think it could be interesting to see how the pHash results change when we adjust the similarity threshold."
      ],
      "metadata": {
        "id": "7q2l-8-vXmSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_similar_images(hashes, threshold): # filters similar images by threshold\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    hashes: dictionary of hashes\n",
        "    threshold: distance threshold to consider images as similar\n",
        "\n",
        "  Returns:\n",
        "    similar_images: list of similar images\n",
        "  \"\"\"\n",
        "  similar_images = []\n",
        "\n",
        "  for (img1, hash1), (img2, hash2) in combinations(sorted(hashes.items()), 2):\n",
        "    dist = hash1 - hash2\n",
        "    if dist <= threshold:\n",
        "      similar_images.append({'image1': img1, 'image2': img2, 'distance': dist})\n",
        "\n",
        "  return similar_images"
      ],
      "metadata": {
        "id": "jaP14gkPahgx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see Just the pHash case\n",
        "HASH_METHODS = ['phash']\n",
        "thresholds_to_try = [3, 5, 7, 10]\n",
        "\n",
        "hashes_phash_all = compute_all_hashes(image_paths, HASH_METHODS)['phash'] # Extracts only 'phash' from the returned dictionary\n",
        "\n",
        "results = []\n",
        "for thresh in thresholds_to_try: # analyzes each threshold\n",
        "  similar_images = filter_similar_images(hashes_phash_all, thresh) # for each threshold, calculate similar images with that threshold\n",
        "\n",
        "  # Extracts all the images that appear at least once as similar (without duplicates):\n",
        "  img_set = set()\n",
        "  for pair in similar_images:\n",
        "    img_set.add(pair['image1'])\n",
        "    img_set.add(pair['image2'])\n",
        "\n",
        "  results.append({\n",
        "    \"Threshold\": thresh,\n",
        "    \"Num Similar Pairs\": len(similar_images),\n",
        "    \"Num Similar Distinct Images\": len(img_set),\n",
        "    \"Total Images\": len(image_paths),\n",
        "    \"Percent Similar (%)\": round(len(img_set) / len(image_paths) * 100, 2)\n",
        "  })\n",
        "\n",
        "\n",
        "# To see the results let's converts the list of results to a DataFrame pandas\n",
        "df_results = pd.DataFrame(results)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "QYHuj26nZKRk",
        "outputId": "e9908eb1-01b0-46aa-ab2d-f555146c9aef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculation phash for all images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Threshold  Num Similar Pairs  Num Similar Distinct Images  Total Images  \\\n",
              "0          3               1414                          831          3446   \n",
              "1          5               2251                         1163          3446   \n",
              "2          7               3084                         1481          3446   \n",
              "3         10               4856                         1950          3446   \n",
              "\n",
              "   Percent Similar (%)  \n",
              "0                24.11  \n",
              "1                33.75  \n",
              "2                42.98  \n",
              "3                56.59  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bc84ece-f55f-485d-acea-5071ccbf3b39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Threshold</th>\n",
              "      <th>Num Similar Pairs</th>\n",
              "      <th>Num Similar Distinct Images</th>\n",
              "      <th>Total Images</th>\n",
              "      <th>Percent Similar (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1414</td>\n",
              "      <td>831</td>\n",
              "      <td>3446</td>\n",
              "      <td>24.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2251</td>\n",
              "      <td>1163</td>\n",
              "      <td>3446</td>\n",
              "      <td>33.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>3084</td>\n",
              "      <td>1481</td>\n",
              "      <td>3446</td>\n",
              "      <td>42.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>4856</td>\n",
              "      <td>1950</td>\n",
              "      <td>3446</td>\n",
              "      <td>56.59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bc84ece-f55f-485d-acea-5071ccbf3b39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8bc84ece-f55f-485d-acea-5071ccbf3b39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8bc84ece-f55f-485d-acea-5071ccbf3b39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8c2ce1f0-a598-482f-ab13-2beca1112cb1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c2ce1f0-a598-482f-ab13-2beca1112cb1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8c2ce1f0-a598-482f-ab13-2beca1112cb1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f80d1515-3ff1-447d-bf6e-89564c8f4ca8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f80d1515-3ff1-447d-bf6e-89564c8f4ca8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Threshold\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 3,\n        \"max\": 10,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          10,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Num Similar Pairs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1470,\n        \"min\": 1414,\n        \"max\": 4856,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2251,\n          4856,\n          1414\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Num Similar Distinct Images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 476,\n        \"min\": 831,\n        \"max\": 1950,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1163,\n          1950,\n          831\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total Images\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3446,\n        \"max\": 3446,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Percent Similar (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.832472362765332,\n        \"min\": 24.11,\n        \"max\": 56.59,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          33.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that as the threshold increases, the number of pairs considered similar also grows, and consequently, so does the percentage of images involved.\n",
        "\n",
        "Observations:\n",
        "- At lower thresholds (e.g., 3), only strongly similar images are identified, but many less obvious duplicates may be missed.\n",
        "\n",
        "- At higher thresholds (e.g., 10), there's a risk of including different images that only share generic visual elements (false positives).\n",
        "\n",
        "- Threshold 5 proves to be a good compromise, balancing precision and coverage."
      ],
      "metadata": {
        "id": "41lbIz8GjUP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now create a function that uses the group_aware splitting technique:"
      ],
      "metadata": {
        "id": "ObR-Eb5b5nuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divides the groups of similar images into training, validation, and test sets, keeping each group together\n",
        "#  (no similar images end up in different sets).\n",
        "def group_aware_split(groups, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    groups: list of groups\n",
        "    train_ratio: ratio of images to be assigned to the training set\n",
        "    val_ratio: ratio of images to be assigned to the validation set\n",
        "    test_ratio: ratio of images to be assigned to the test set\n",
        "    seed: seed for the random number generator\n",
        "\n",
        "  Returns:\n",
        "    assignments: dictionary with the split of images\n",
        "  \"\"\"\n",
        "  # checks whether the sets add up to 1\n",
        "  try:\n",
        "    total_ratio = train_ratio + val_ratio + test_ratio\n",
        "    if not 0.99 <= total_ratio <= 1.01:\n",
        "      raise ValueError(\"The proportions do not add up to 1! You must correct the values.\")\n",
        "  except ValueError as e:\n",
        "    print(f\"ERROR: {e}\")\n",
        "    return None\n",
        "\n",
        "  split_ratios = {'train': train_ratio, 'val': val_ratio, 'test': test_ratio}\n",
        "\n",
        "  random.seed(seed) # to initialize the random number generator\n",
        "  random.shuffle(groups) # to make randomization reproducible\n",
        "\n",
        "  # dictionary comprehension\n",
        "  total_images = sum(len(g) for g in groups) # to figure out how many images should be assigned to that split\n",
        "  target_counts = {k: int(v * total_images) for k, v in split_ratios.items()}\n",
        "  current_counts = defaultdict(int) # number of images already assigned to each split\n",
        "  assignments = defaultdict(list) # number of images actually assigned to each split as final output\n",
        "\n",
        "  for group in groups:\n",
        "    # Find the split with the lowest saturation ratio\n",
        "    best_split = min(\n",
        "      target_counts.items(),\n",
        "      key=lambda item: current_counts[item[0]] / item[1] if item[1] > 0 else float('inf')\n",
        "    )[0] # this line is used to take the key (‘train’, ‘val’, ‘test’) of the best split\n",
        "\n",
        "    assignments[best_split].extend(group) # adds all images in the group to the selected split\n",
        "    current_counts[best_split] += len(group) # update the counter to know how many images are now in that split\n",
        "\n",
        "  return assignments"
      ],
      "metadata": {
        "id": "se5sCN7Lele7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function we get a *assignments* dictionary structured so that each list contains the names of the images assigned to the split (train, val, test), keeping similar images together."
      ],
      "metadata": {
        "id": "O5wxLy2P6X32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# invokes the function to divide the groups of similar images into the different sets\n",
        "assignments = group_aware_split(groups, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42)\n",
        "\n",
        "# check the counts\n",
        "print({k: len(v) for k, v in assignments.items()})\n"
      ],
      "metadata": {
        "id": "bVs4GxlHcg-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb483b3-0cfe-4339-8b06-de767a3a7d46"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 813, 'val': 175, 'test': 175}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partitioning the dataset into: \\\n",
        "- Training set = 70%,\n",
        "- Validation set = 15%,\n",
        "- Test set = 15%\n",
        "\n",
        "and we obtain a distribution according to:\n",
        "- 820 images out of 1176 for the Training set\n",
        "- 178 images out of 1176 for the Validation set\n",
        "- 178 images out of 1176 for the Test set"
      ],
      "metadata": {
        "id": "eTI4FyCt8R2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "Now we need to use a *Stratified Split* that ensures that the distribution of classes in the dataset is proportionally balanced across the divisions of the three sets. \\\n",
        "We prefer to use a **Stratified Sampling** technique because we already know that our dataset is somewhat unbalanced, as it contains more images with people than images without.\n"
      ],
      "metadata": {
        "id": "vFX11HDvBHuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But, of course, we want to maintain the partitioning we just did for similar images:"
      ],
      "metadata": {
        "id": "SKakySqXCwJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divides the AERALIS dataset in a layered manner and copies images/.xml to train/val/test.\n",
        "#   - Maintains similar image assignments (from group_aware_split).\n",
        "#   - Stratifies remainder split based on CSV ‘class’ column.\n",
        "#   - Saves images, annotations and generates CSV for each set with all original columns.\n",
        "def stratified_split(assignments, image_folder_path, output_base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    assignments: dictionary with the split of images\n",
        "    image_folder_path: path to the folder containing images (and CSV file)\n",
        "    output_base_path: path to the output folder\n",
        "    train_ratio, val_ratio, test_ratio: desired proportions of images to be assigned to the training, validation and test set\n",
        "    seed: seed for the random number generator\n",
        "  \"\"\"\n",
        "\n",
        "  # 1. Create train/val/test folders with subfolders images/ and annotations/\n",
        "  for split in ['train', 'val', 'test']:\n",
        "    os.makedirs(os.path.join(output_base_path, split, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_base_path, split, \"annotations\"), exist_ok=True)\n",
        "\n",
        "  # 2. Upload the full CSV\n",
        "  csv_path = os.path.join(image_folder_path, \"aeralis_person_labels.csv\")\n",
        "  df_full = pd.read_csv(csv_path)\n",
        "  df_full = df_full[df_full['filename'].str.lower().str.endswith(('.jpg', '.jpeg', '.png'))] # we really only need '.jpg'\n",
        "  df = df_full.drop_duplicates(subset='filename', keep='first').copy()\n",
        "\n",
        "  # df = pd.read_csv(csv_path)\n",
        "  # df = df[df['filename'].str.lower().str.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "  # 3. Removes images already assigned (similar)\n",
        "  already_assigned = set(sum(assignments.values(), []))\n",
        "  df_unassigned = df[~df['filename'].isin(already_assigned)].copy()\n",
        "\n",
        "  # 4. Split is stratified according to y labels, class balance is maintained\n",
        "  X = df_unassigned['filename'].values\n",
        "  y = df_unassigned['class'].values\n",
        "\n",
        "  X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    train_size = train_ratio,\n",
        "    stratify = y,\n",
        "    random_state = seed\n",
        "  )\n",
        "  # X_train, y_train: part that will go into the training set\n",
        "  # X_temp, y_temp: remaining images to be split still in validation and testing\n",
        "\n",
        "  # After removing the train part, we need to divide X_temp into val and test:\n",
        "  val_ratio_adjusted = val_ratio / (val_ratio + test_ratio) # we calculate the new proportion of validation to the remaining total\n",
        "\n",
        "  # We divide X_temp and y_temp into validation and test:\n",
        "  X_val, X_test = train_test_split(\n",
        "    X_temp, train_size = val_ratio_adjusted,\n",
        "    stratify = y_temp,\n",
        "    random_state = seed\n",
        "  )\n",
        "\n",
        "  # 5. Adds assignments to the dictionary, avoiding duplicates\n",
        "  # assignments is the dictionary initially created with the similar groups assigned via Group-Aware Splitting\n",
        "  # X_train, X_val, X_test are the non-similar, stratified image assignments\n",
        "  for split, split_X in zip(['train', 'val', 'test'], [X_train, X_val, X_test]):\n",
        "    new_imgs = [x for x in split_X if x not in already_assigned]\n",
        "    assignments[split].extend(new_imgs)\n",
        "    already_assigned.update(new_imgs)\n",
        "\n",
        "\n",
        "\n",
        "  # 6. Copy file and generate final CSV\n",
        "  for split, file_list in assignments.items():\n",
        "    # split_df = df[df['filename'].isin(file_list)].copy()\n",
        "    split_df = df_full[df_full['filename'].isin(file_list)].copy()\n",
        "\n",
        "    for fname in file_list:\n",
        "      img_path = os.path.join(image_folder_path, fname)\n",
        "      xml_name = os.path.splitext(fname)[0] + \".xml\"\n",
        "      xml_path = os.path.join(image_folder_path, xml_name)\n",
        "\n",
        "      dst_img = os.path.join(output_base_path, split, \"images\", fname)\n",
        "      dst_xml = os.path.join(output_base_path, split, \"annotations\", xml_name)\n",
        "\n",
        "      if os.path.exists(img_path):\n",
        "        shutil.copy2(img_path, dst_img)\n",
        "      if os.path.exists(xml_path):\n",
        "        shutil.copy2(xml_path, dst_xml)\n",
        "\n",
        "    # save detailed CSV for split\n",
        "    split_df.to_csv(os.path.join(output_base_path, f\"{split}_set.csv\"), index=False)\n",
        "\n",
        "  # 7. Report\n",
        "  print(\"Stratified split completed and files copied.\")\n",
        "  print(f\"Train: {len(assignments['train'])} images\")\n",
        "  print(f\"Val:   {len(assignments['val'])} images\")\n",
        "  print(f\"Test:  {len(assignments['test'])} images\")"
      ],
      "metadata": {
        "id": "CSPJq6NMvEh4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this function we are going to create a new folder containing 3 subfolders for the Training, Validation and Test phases, each of which will contain a folder with images and one with their respective annotations. At the same time a CSV file describing the set will be created."
      ],
      "metadata": {
        "id": "L3p41XS5C4-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS\"\n",
        "\n",
        "output_base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "# Initialize assignments (only if is not initialized) with: assignments = group_aware_split(gruppi_simili)\n",
        "\n",
        "stratified_split(assignments, image_folder_path, output_base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15,seed=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeYrKkd6m0PF",
        "outputId": "a0d21249-b6ec-4ed7-bb9c-721e76fe4f39"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified split completed and files copied.\n",
            "Train: 2411 images\n",
            "Val:   517 images\n",
            "Test:  518 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if the numbers matches\n",
        "total_all = len(assignments['train']) + len(assignments['val']) + len(assignments['test'])\n",
        "unique_total = len(set(assignments['train'] + assignments['val'] + assignments['test']))\n",
        "\n",
        "print(f\"Total images in split (sum): {total_all}\")\n",
        "print(f\"Total unique images: {unique_total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJucdFYFpQOQ",
        "outputId": "88c1d481-3ec2-4951-d727-a43f4fb3605f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images in split (sum): 3446\n",
            "Total unique images: 3446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now perform a quick check to see if the split did not cause inconsistencies in the data:"
      ],
      "metadata": {
        "id": "PCt5LTQ-0Sga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = ['train', 'val', 'test']\n",
        "base_path = output_base_path  # già definito\n",
        "\n",
        "for split in splits:\n",
        "  img_dir = os.path.join(base_path, split, 'images')\n",
        "  ann_dir = os.path.join(base_path, split, 'annotations')\n",
        "\n",
        "  images = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "  missing_xml = []\n",
        "\n",
        "  for img in images:\n",
        "    xml_name = os.path.splitext(img)[0] + \".xml\"\n",
        "    if not os.path.exists(os.path.join(ann_dir, xml_name)):\n",
        "      missing_xml.append(xml_name)\n",
        "\n",
        "  print(f\"{split.upper()} Images: {len(images)}, Missing XML: {len(missing_xml)}\")\n",
        "  if missing_xml:\n",
        "    print(\"\\nMissing XML files:\")\n",
        "    for x in missing_xml:\n",
        "      print(\"   \", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtbLNpZtqY-a",
        "outputId": "dad0500a-3d06-4a3d-96ab-51dc5973981d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN Images: 2411, Missing XML: 0\n",
            "VAL Images: 517, Missing XML: 0\n",
            "TEST Images: 518, Missing XML: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split in splits:\n",
        "  csv_path = os.path.join(base_path, f\"{split}_set.csv\")\n",
        "  img_dir = os.path.join(base_path, split, \"images\")\n",
        "\n",
        "  df_split = pd.read_csv(csv_path)\n",
        "  csv_filenames = set(df_split['filename'].str.lower())\n",
        "  actual_images = set(f.lower() for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png')))\n",
        "\n",
        "  missing_in_csv = actual_images - csv_filenames # files present in folder but NOT in CSV\n",
        "  missing_in_dir = csv_filenames - actual_images # files present in CSV but NOT in folder\n",
        "\n",
        "  print(f\"{split.upper()} — CSV: {len(csv_filenames)}, IMG DIR: {len(actual_images)}\")\n",
        "  if missing_in_csv:\n",
        "    print(f\"   {len(missing_in_csv)} images in folder not listed in CSV.\")\n",
        "  if missing_in_dir:\n",
        "    print(f\"   {len(missing_in_dir)} images in CSV not found in folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Atvs9VjpXEx",
        "outputId": "8a4f2b67-fb94-48e3-e1d1-7ba18798161d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN — CSV: 2411, IMG DIR: 2411\n",
            "VAL — CSV: 517, IMG DIR: 517\n",
            "TEST — CSV: 518, IMG DIR: 518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! There are no inconsistencies resulting from the split. \\\n",
        " We now proceed to create copies of the folder we just created, *AERALIS_SPLITTED*, as we want to ensure that the future study of the models' performance is not affected by different splits. Therefore, we will use the same Train, Val, and Test proportions for all of them, as we have just created"
      ],
      "metadata": {
        "id": "zUwwwXtj4LQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remember: base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "model_YOLO_versions = [\"YOLOv8n\", \"YOLOv11n\"]\n",
        "\n",
        "for v in model_YOLO_versions:\n",
        "    dst = f\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_{v}\" # constructs the destination path\n",
        "    shutil.copytree(base_path, dst)"
      ],
      "metadata": {
        "id": "aC8VT3Ki65Qm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if the directories have been created\n",
        "print(os.path.exists(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n\"))\n",
        "print(os.path.exists(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n\"))\n",
        "\n",
        "# EXPECTED:\n",
        "# true, true"
      ],
      "metadata": {
        "id": "fC48eVtxO8Ag",
        "outputId": "e87b1b6d-0cd7-47ef-d127-ec07f3b11850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if the directories have been created\n",
        "print(os.path.exists(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n\"))\n",
        "print(os.path.exists(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n\"))\n",
        "\n",
        "# EXPECTED: all true"
      ],
      "metadata": {
        "id": "oiZJxNjaYCxv",
        "outputId": "5f963ae6-100a-49aa-d563-ff176f02201f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next session, we will resize the images and their corresponding annotations for each set. This will eliminate any variability caused by the automatic resizing performed by the different models. \\\n",
        "Each model is designed for inputs of specific dimensions and, for this initial phase of “offline” fine-tuning (i.e., in my development environment), I prefer to test each model in optimal conditions, making the most of its capabilities."
      ],
      "metadata": {
        "id": "o2QXt1OZMmYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resizing\n"
      ],
      "metadata": {
        "id": "0NiC2UuhSrDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, I discuss the preprocessing strategies for adapting the dataset to the input requirements of various detection models. \\\n",
        "\n",
        "Originally, I planned to resize all images to the specific size required by each model version (see below), and to update the bounding boxes and annotation files accordingly. \\\n",
        "This approach, especially with letterbox resize, ensures that images are adapted while maintaining the correct aspect ratio (an essential requirement for YOLO models), which are trained on letterboxed data."
      ],
      "metadata": {
        "id": "RuiW_Bw7T6bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During this phase, it is very important not to resize the images by *stretching* them.\n",
        "We therefore use **Letterbox Resize** to avoid distortions. \\\n",
        "Letterbox resize is a preprocessing technique that:\n",
        "\n",
        " - Allows resizing the image while maintaining the original aspect ratio (i.e., the ratio between the width and height of an image).\n",
        "\n",
        " - Adds padding (black border or any other color) on the remaining sides to exactly match the target size.\n",
        "\n",
        "This ensures that object proportions are preserved and the bounding boxes remain unchanged."
      ],
      "metadata": {
        "id": "Ptn7ggFsNDOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model input sizes: YOLOv8n and YOLOv11n require image dimensions of 640x640"
      ],
      "metadata": {
        "id": "hzuodc0tb4_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform letterbox resize: resize with retained aspect ratio and padding\n",
        "def letterbox_resize(image, target_size=(640, 640), color=(114, 114, 114)):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image: image to be resized\n",
        "    target_size: tuple = (width, height)\n",
        "    color: tuple = (r, g, b)\n",
        "\n",
        "  Returns:\n",
        "    resized: resized image\n",
        "    scale: scale factor\n",
        "    (pad_x, pad_y): padding values\n",
        "  \"\"\"\n",
        "  orig_h, orig_w = image.shape[:2] # original height and width\n",
        "  target_w, target_h = target_size # desired height and width\n",
        "\n",
        "  scale = min(target_w / orig_w, target_h / orig_h) # maintaining aspect ratio\n",
        "  new_w, new_h = int(orig_w * scale), int(orig_h * scale)\n",
        "\n",
        "  resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR) # resize image\n",
        "  pad_x = (target_w - new_w) // 2 # horizontal padding\n",
        "  pad_y = (target_h - new_h) // 2 # vertical padding\n",
        "\n",
        "  # Adding padding to get target size image\n",
        "  padded = cv2.copyMakeBorder(\n",
        "    resized,\n",
        "    pad_y, target_h - new_h - pad_y,\n",
        "    pad_x, target_w - new_w - pad_x,\n",
        "    borderType = cv2.BORDER_CONSTANT, value=color # grey padding\n",
        "  )\n",
        "\n",
        "  return padded, scale, (pad_x, pad_y)"
      ],
      "metadata": {
        "id": "pAktUPQ-NA1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_one_folder(img_dir, xml_dir, out_img_dir, out_xml_dir, target_size=(640, 640)):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    img_dir: path to input images\n",
        "    xml_dir: path to input annotations\n",
        "    out_img_dir: path to output images\n",
        "    out_xml_dir: path to output annotations\n",
        "    target_size: tuple = (width, height)\n",
        "  \"\"\"\n",
        "  os.makedirs(out_img_dir, exist_ok=True)\n",
        "  os.makedirs(out_xml_dir, exist_ok=True)\n",
        "\n",
        "  tgt_w, tgt_h = target_size\n",
        "\n",
        "  for fname in os.listdir(img_dir):\n",
        "    is_image = fname.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
        "    img_path = os.path.join(img_dir, fname)\n",
        "    xml_filename = os.path.splitext(fname)[0] + \".xml\"\n",
        "    xml_path = os.path.join(xml_dir, xml_filename)\n",
        "    xml_exists = os.path.exists(xml_path)\n",
        "\n",
        "    image = cv2.imread(img_path) if is_image else None\n",
        "\n",
        "    if is_image and xml_exists and image is not None:\n",
        "      # resize with letterbox\n",
        "      resized_img, scale, (pad_x, pad_y) = letterbox_resize(image, target_size)\n",
        "\n",
        "      # save image (quality only for JPEG)\n",
        "      out_img_path = os.path.join(out_img_dir, fname)\n",
        "      ext = os.path.splitext(fname)[1].lower()\n",
        "      if ext in ('.jpg', '.jpeg'):\n",
        "        # Save JPEG at quality=85 to cut file size/I/O with negligible impact on detection\n",
        "        cv2.imwrite(out_img_path, resized_img, [int(cv2.IMWRITE_JPEG_QUALITY), 85])\n",
        "      else:\n",
        "        cv2.imwrite(out_img_path, resized_img)\n",
        "\n",
        "      # update XML\n",
        "      tree = ET.parse(xml_path)\n",
        "      root = tree.getroot()\n",
        "\n",
        "      removed = 0\n",
        "      for obj in list(root.findall('object')):  # list() to iterate safely if we remove nodes\n",
        "        bbox = obj.find('bndbox')\n",
        "\n",
        "        # Validity status of the object\n",
        "        valid = True\n",
        "        xmin = ymin = xmax = ymax = None\n",
        "\n",
        "        # basic checks\n",
        "        if bbox is None:\n",
        "          valid = False\n",
        "        else:\n",
        "          try:\n",
        "            xmin = float(bbox.find('xmin').text)\n",
        "            ymin = float(bbox.find('ymin').text)\n",
        "            xmax = float(bbox.find('xmax').text)\n",
        "            ymax = float(bbox.find('ymax').text)\n",
        "          except Exception:\n",
        "            valid = False\n",
        "\n",
        "        # projection and clamp only if still valid\n",
        "        if valid:\n",
        "          # scale + pad (round to avoid downward bias)\n",
        "          xmin = int(round(xmin * scale + pad_x))\n",
        "          xmax = int(round(xmax * scale + pad_x))\n",
        "          ymin = int(round(ymin * scale + pad_y))\n",
        "          ymax = int(round(ymax * scale + pad_y))\n",
        "\n",
        "          # VOC 1 clamp included\n",
        "          xmin = max(1, min(xmin, tgt_w))\n",
        "          ymin = max(1, min(ymin, tgt_h))\n",
        "          xmax = max(1, min(xmax, tgt_w))\n",
        "          ymax = max(1, min(ymax, tgt_h))\n",
        "\n",
        "          # mandatory positive area\n",
        "          if xmax <= xmin or ymax <= ymin:\n",
        "            valid = False\n",
        "\n",
        "        # apply result\n",
        "        if valid:\n",
        "          bbox.find('xmin').text = str(xmin)\n",
        "          bbox.find('ymin').text = str(ymin)\n",
        "          bbox.find('xmax').text = str(xmax)\n",
        "          bbox.find('ymax').text = str(ymax)\n",
        "        else:\n",
        "          root.remove(obj)\n",
        "          removed += 1\n",
        "\n",
        "      # update <size>\n",
        "      size_tag = root.find('size')\n",
        "      if size_tag is None:\n",
        "        size_tag = ET.SubElement(root, 'size')\n",
        "        ET.SubElement(size_tag, 'width')\n",
        "        ET.SubElement(size_tag, 'height')\n",
        "        ET.SubElement(size_tag, 'depth')\n",
        "\n",
        "      # width/height in VOC are integers\n",
        "      size_tag.find('width').text = str(tgt_w)\n",
        "      size_tag.find('height').text = str(tgt_h)\n",
        "      depth_tag = size_tag.find('depth')\n",
        "      if depth_tag is not None:\n",
        "        # use resized image channels if available\n",
        "        ch = resized_img.shape[2] if resized_img.ndim == 3 else 1\n",
        "        depth_tag.text = str(int(ch))\n",
        "\n",
        "      # save updated XML\n",
        "      out_xml_path = os.path.join(out_xml_dir, xml_filename)\n",
        "      tree.write(out_xml_path)\n",
        "\n",
        "      # minimal log if we have removed something\n",
        "      if removed:\n",
        "        print(f\"{xml_filename}: removed {removed} degenerate bbox\")\n",
        "\n",
        "    else:\n",
        "      # logging soft: does not interrupt the batch\n",
        "      if not is_image:\n",
        "        print(f\"Ignored file (not image): {img_path}\")\n",
        "      elif not xml_exists:\n",
        "        print(f\"Missing XML for: {fname}\")\n",
        "      elif image is None:\n",
        "        print(f\"Reading error: {img_path}\")"
      ],
      "metadata": {
        "id": "GzRjT-MXNHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the entire dataset (Training, Validation, Testing)\n",
        "def process_entire_dataset(base_input_dir, base_output_dir, splits=('train', 'val', 'test'), target_size=(640, 640)):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    base_input_dir: path to input folder\n",
        "    base_output_dir: path to output folder\n",
        "    splits: tuple = ('train', 'val', 'test')\n",
        "    target_size: tuple = (width, height)\n",
        "  \"\"\"\n",
        "  for split in splits:\n",
        "    print(f\"\\n Processing split: {split}\")\n",
        "    # Input/output paths for images and annotations.\n",
        "    img_dir = os.path.join(base_input_dir, split, 'images')\n",
        "    xml_dir = os.path.join(base_input_dir, split, 'annotations')\n",
        "    out_img_dir = os.path.join(base_output_dir, split, 'images')\n",
        "    out_xml_dir = os.path.join(base_output_dir, split, 'annotations')\n",
        "\n",
        "    # Process a single split\n",
        "    process_one_folder(img_dir, xml_dir, out_img_dir, out_xml_dir, target_size)\n",
        "\n",
        "  print(\"\\nAll images and annotations have been processed.\")"
      ],
      "metadata": {
        "id": "3TCeW88NNJFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, through detailed analysis, I realized that not all models handle resizing in the same way. \\\n",
        "For instance: YOLO requires and expects letterbox resize, as this is the format used during its training."
      ],
      "metadata": {
        "id": "6y-VlKaqoVr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although I have implemented functions for letterbox resizing and verified their correctness, I have decided not to apply any resizing in advance at this stage.\n",
        "Instead, I will perform the fine-tuning directly on the original dataset, letting each official library handle the resizing \"on the fly\" according to the correct pipeline for each model. \\\n",
        "This strategy reduces the risk of inconsistencies, ensures full compatibility with each model's expectations, and simplifies future updates or changes in input size requirements. \\"
      ],
      "metadata": {
        "id": "duRKSlC6pJ6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the resizing\n",
        "# base_path = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_SPLITTED\"\n",
        "\n",
        "# AERALIS_YOLOv8n (640 x 640)\n",
        "# process_entire_dataset(\n",
        "#  base_input_dir = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n\", # input\n",
        "#  base_output_dir = \"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n_resized\", # output\n",
        "#  splits=('train', 'val', 'test'), # default (can be omitted)\n",
        "#  target_size=(640, 640)\n",
        "#)"
      ],
      "metadata": {
        "id": "WTeWdZXAM07o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If, during the experiments, I find that training becomes too slow or resource-intensive due to the large original images, I will reconsider applying resizing in advance, always ensuring to use the correct method for each model."
      ],
      "metadata": {
        "id": "p4m9nEl4pMVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion"
      ],
      "metadata": {
        "id": "KY5fBoM2WWWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to convert the annotations. \\\n",
        "To ensure consistency, we need to convert all the .xml files (which contain the bounding box annotations for each image) into the appropriate format required by each model.\n",
        "\n",
        "The choice of annotation format mostly depends on the *implementation* of the model we plan to use, not on intrinsic format limitations. \\"
      ],
      "metadata": {
        "id": "fgN68L_dfiM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Documentation vs Implementation**\n",
        " - The official documentation of each training framework describes the required annotation format, but\n",
        "\n",
        " - In practice, it is the training scripts (or the APIs of libraries such as Ultralytics YOLO, TensorFlow Object Detection API, PyTorch Lightning, etc.) that enforce that format."
      ],
      "metadata": {
        "id": "WWzNQePdiut-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLO TXT Format** \\\n",
        "According to the official Ultralytics documentation for YOLO:\n",
        "  - *“One text file per image: Each image in the dataset has a corresponding text file with the same name as the image file and the '.txt' extension.”* \\\n",
        "  (link: https://docs.ultralytics.com/datasets/segment/#supported-dataset-formats)\n",
        "\n",
        "  - *“Convert these annotations into the YOLO .txt file format which Ultralytics supports.”* \\\n",
        "  (link: https://docs.ultralytics.com/datasets/#contribute-new-datasets)\n",
        "\n",
        "This means that if we use Ultralytics YOLO (as we plan to do), the .txt format is mandatory, and each file must contain annotations in normalized coordinates: `<class_id> <x_center> <y_center> <width> <height>`"
      ],
      "metadata": {
        "id": "QV1HwOJKom8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start to transform the annotations for the YOLO model:"
      ],
      "metadata": {
        "id": "VzW9PQdTWcL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_MAP = {\"person\": 0}  # maps the classes with a numeric ID"
      ],
      "metadata": {
        "id": "7tKqcgqzWa2V"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert a VOC bbox (1-based inclusive if voc_one_indexed=True) to YOLO normalized (x_center, y_center, w, h)\n",
        "def voc_box_to_yolo(size, box, voc_one_indexed=True):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    size: tuple = (width, height)\n",
        "    box: tuple = (xmin, ymin, xmax, ymax)\n",
        "    voc_one_indexed: bool = True\n",
        "\n",
        "  Returns:\n",
        "    tuple = (x_center, y_center, w, h)\n",
        "  \"\"\"\n",
        "  W,H = size\n",
        "  xmin,ymin,xmax,ymax = box\n",
        "\n",
        "  if voc_one_indexed:\n",
        "    bw = (xmax - xmin + 1)\n",
        "    bh = (ymax - ymin + 1)\n",
        "    x0 = xmin - 1\n",
        "    y0 = ymin - 1\n",
        "    cx = (x0 + bw/2.0) / W\n",
        "    cy = (y0 + bh/2.0) / H\n",
        "\n",
        "    return cx, cy, bw / W, bh / H\n",
        "\n",
        "  else:\n",
        "    # 0-based already prepared\n",
        "    bw = (xmax - xmin)\n",
        "    bh = (ymax - ymin)\n",
        "    cx = (xmin + xmax) / 2.0 / W\n",
        "    cy = (ymin + ymax) / 2.0 / H\n",
        "\n",
        "    return cx, cy, bw / W, bh / H"
      ],
      "metadata": {
        "id": "WzXcexNI35xj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xml_to_yolo(xml_path: Path, txt_path: Path, voc_one_indexed=True): # converts a single .xml file to a .txt file in the YOLO style\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_path: Path to input XML annotation file\n",
        "    txt_path: Path to output YOLO .txt annotation file\n",
        "    voc_one_indexed: bool = True\n",
        "  \"\"\"\n",
        "  tree = ET.parse(xml_path)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  w = int(root.find('size/width').text)\n",
        "  h = int(root.find('size/height').text)\n",
        "\n",
        "  lines = []\n",
        "\n",
        "  for obj in root.findall('object'):\n",
        "    cls = (obj.find('name').text or '').strip().lower()\n",
        "    if cls in CLASS_MAP:\n",
        "      b = obj.find('bndbox')\n",
        "      xmin = float(b.find('xmin').text); ymin = float(b.find('ymin').text)\n",
        "      xmax = float(b.find('xmax').text); ymax = float(b.find('ymax').text)\n",
        "      x, y, ww, hh = voc_box_to_yolo((w, h), (xmin, ymin, xmax, ymax), voc_one_indexed=voc_one_indexed)\n",
        "\n",
        "      # discard degenerate post-clamp boxes\n",
        "      if ww > 0 and hh > 0:\n",
        "        lines.append(f\"{CLASS_MAP[cls]} {x:.6f} {y:.6f} {ww:.6f} {hh:.6f}\")\n",
        "\n",
        "  txt_path.write_text(\"\\n\".join(lines) + (\"\\n\" if lines else \"\"))"
      ],
      "metadata": {
        "id": "pDZLZdYKxrcz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_convert(xml_dir: Path, txt_dir: Path): # to convert all xml files to one directory\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_dir: Path to input directory containing .xml annotations\n",
        "    txt_dir: Path to output directory for .txt annotations\n",
        "  \"\"\"\n",
        "  txt_dir.mkdir(parents=True, exist_ok=True)\n",
        "  for xml in xml_dir.rglob(\"*.xml\"):\n",
        "    xml_to_yolo(xml, txt_dir / f\"{xml.stem}.txt\")"
      ],
      "metadata": {
        "id": "gdQIWSCFxnpj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try for YOLOv8n:"
      ],
      "metadata": {
        "id": "6oSTuGKaUx8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts for TRAIN\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/train/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/train/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for VAL\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/val/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/val/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for TEST\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/labels\") # output\n",
        ")"
      ],
      "metadata": {
        "id": "wrpteaWobg4A"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now for YOLOv11n as well:"
      ],
      "metadata": {
        "id": "AxhkOKG3Wsee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts for TRAIN\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/train/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/train/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for VAL\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/val/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/val/labels\") # output\n",
        ")\n",
        "\n",
        "# Converts for TEST\n",
        "batch_convert(\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/test/annotations\"), # input\n",
        "  Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/test/labels\") # output\n",
        ")"
      ],
      "metadata": {
        "id": "gEytdfWoWjro"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check everything:"
      ],
      "metadata": {
        "id": "pxt7ee5VX0l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to verify that for each .xml file in a directory there is a corresponding .txt file in the directory of converted YOLO labels:\n",
        "def check_yolo_conversion(xml_dir: Path, txt_dir: Path):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    xml_dir: Path to input directory containing .xml annotations\n",
        "    txt_dir: Path to input directory containing .txt annotations\n",
        "  \"\"\"\n",
        "  xml_files = sorted([f.stem for f in xml_dir.glob(\"*.xml\")])\n",
        "  txt_files = sorted([f.stem for f in txt_dir.glob(\"*.txt\")])\n",
        "\n",
        "  missing_txt = [f for f in xml_files if f not in txt_files]\n",
        "  extra_txt = [f for f in txt_files if f not in xml_files]\n",
        "\n",
        "  if not missing_txt and not extra_txt:\n",
        "    print(\"All .xml files have the corresponding .txt file.\")\n",
        "\n",
        "  else:\n",
        "    print(\"Some matches are not correct:\")\n",
        "\n",
        "    if missing_txt:\n",
        "      print(f\"The following .txt files are missing for: {missing_txt}\")\n",
        "    if extra_txt:\n",
        "      print(f\"Excess .txt file (no corresponding .xml): {extra_txt}\")"
      ],
      "metadata": {
        "id": "meZOX7tCX2fZ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check\n",
        "\n",
        "# YOLOv8n\n",
        "\n",
        "# TRAIN\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/train/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/train/labels\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/val/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/val/labels\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv8n/test/labels\")\n",
        ")"
      ],
      "metadata": {
        "id": "4h1uqcOzYWNu",
        "outputId": "7f39ab0c-abbc-4c9f-ec55-8f20a9dda8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLOv11n\n",
        "\n",
        "# TRAIN\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/train/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/train/labels\")\n",
        ")\n",
        "\n",
        "# VAL\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/val/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/val/labels\")\n",
        ")\n",
        "\n",
        "# TEST\n",
        "check_yolo_conversion(\n",
        "  xml_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/test/annotations\"),\n",
        "  txt_dir=Path(\"/content/drive/MyDrive/projectUPV/datasets/AERALIS_YOLOv11n/test/labels\")\n",
        ")"
      ],
      "metadata": {
        "id": "QJ1xcra_ZFuO",
        "outputId": "f97fa570-1736-482d-c41b-328f75a48e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n",
            "All .xml files have the corresponding .txt file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convert_bbox function and the xml_to_yolo, batch_convert scripts are based on standard methodologies for converting Pascal VOC -> YOLO annotations, as described for example in tutorials on Medium and in open GitHub repositories."
      ],
      "metadata": {
        "id": "AkHmCLgLFVDP"
      }
    }
  ]
}