{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvDbvyFl2Mk1OZCT78HFlX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanDaniele/drone-person-detection/blob/main/other_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up: mount drive + import libraries"
      ],
      "metadata": {
        "id": "WqGl9KBwGNWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Information:** We need to activate the GPU on Colab (_Runtime --> Change runtime type_). \\\n",
        "Every time you start a new session (or reopen the notebook after a few hours) check that the GPU is still active. If we are not using the GPU it can take up to tens of hours to train the models. \\\n",
        "_GPU T4 is the best choice._"
      ],
      "metadata": {
        "id": "9RnXcwI6GSYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this Every time you start a new session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to mount google drive (to see/access it)"
      ],
      "metadata": {
        "id": "Q6pfH1SPGUNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Import base for EfficientDet:\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision import transforms\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from effdet.efficientdet import HeadNet\n",
        "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
        "from effdet.bench import unwrap_bench\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import json\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "S9_L8lIzGYil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientDet D0"
      ],
      "metadata": {
        "id": "OMGXaiKbCi1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniziamo ora ad allenare il modello EfficientDet. Utilizzeremo la libreria effdet di Ross Wightman, che semplifica tutto il processo:"
      ],
      "metadata": {
        "id": "HnqdMoZ_Cl3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLbfUmXKCZ6S"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/rwightman/efficientdet-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Copy the entire dataset folder from Google Drive to Colab's local storage to increase speed during training:"
      ],
      "metadata": {
        "id": "JvFsAjlmCq_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0'\n",
        "dst = '/content/AERALIS_EfficientDet_D0_local'\n",
        "\n",
        "# Delete the destination if it already exists (shutil.rmtree), then recopy from scratch\n",
        "if os.path.exists(dst):\n",
        "  shutil.rmtree(dst)\n",
        "shutil.copytree(src, dst)\n",
        "print(\"Copy completed\")"
      ],
      "metadata": {
        "id": "FdJTyERaCqtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h / # It shows the total, used and free space on the root (/) of the Colab VM.\n",
        "\n",
        "# Avail column: space still available for your files."
      ],
      "metadata": {
        "id": "uFvLqxdxCuzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show space used by your local folder\n",
        "!du -sh /content/AERALIS_EfficientDet_D0_local"
      ],
      "metadata": {
        "id": "22Pvphb6CutZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show space occupied by various folders in /content/.\n",
        "!du -h --max-depth=1 /content/"
      ],
      "metadata": {
        "id": "-48ldlF4Cum4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main parameters\n",
        "BASE_DIR = dst\n",
        "VARIANT = 'tf_efficientdet_d0'  # Change to 'd1' or 'd2' for other versions\n",
        "IMAGE_SIZE = 512                # D0=512, D1=640, D2=768\n",
        "NUM_CLASSES = 1                 # only one class (ex: 'person')\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "PATIENCE = 20\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "6PQjSsFjCzd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the available GPU\n",
        "print(torch.cuda.is_available()) # True = you have GPU --> if False then use device='cpu'\n",
        "print(torch.cuda.device_count()) # Name of GPU\n",
        "\n",
        "# If True and at least 1, you can use device=0.\n",
        "# If you don't have GPU: use device='cpu' (much slower).\n",
        "# Locally (not Colab): check with nvidia-smi from terminal."
      ],
      "metadata": {
        "id": "p1mzmmA7C1Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We keep only those images that contain at least one bounding box:"
      ],
      "metadata": {
        "id": "y0UCGL-ZC3KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDetectionTransformed(CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transform=None):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        # 1) Trovo tutti gli image_id con almeno una bbox >1px\n",
        "        valid_img_ids = {\n",
        "            ann['image_id']\n",
        "            for ann in self.coco.dataset['annotations']\n",
        "            if ann['bbox'][2] > 1 and ann['bbox'][3] > 1\n",
        "        }\n",
        "\n",
        "        # 2) Conservo solo gli indici delle immagini valide\n",
        "        self.valid_indices = [\n",
        "            idx for idx, img_info in enumerate(self.coco.dataset['images'])\n",
        "            if img_info['id'] in valid_img_ids\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      real_idx = self.valid_indices[idx]\n",
        "      img, target = super().__getitem__(real_idx)\n",
        "\n",
        "      if self.transform:\n",
        "          bboxes = [obj['bbox'] for obj in target]\n",
        "          labels = [0] * len(bboxes)\n",
        "          augmented = self.transform(\n",
        "              image=np.array(img),\n",
        "              bboxes=bboxes,\n",
        "              category_id=labels\n",
        "          )\n",
        "          img = augmented['image']\n",
        "          for obj, new_bbox in zip(target, augmented['bboxes']):\n",
        "              obj['bbox'] = list(new_bbox)\n",
        "\n",
        "      # qui estrai l'image_id CORRETTO\n",
        "      img_id = self.coco.dataset['images'][real_idx]['id']\n",
        "      return img, target, img_id"
      ],
      "metadata": {
        "id": "JpvW0kg1C2-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a DataLoader to load images and annotations in COCO format:"
      ],
      "metadata": {
        "id": "_Oa23_R3C7xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(img_dir, ann_path, shuffle):\n",
        "    # Trasformazioni congiunte immagine + bbox in formato COCO\n",
        "    transform = A.Compose([\n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        # A.Normalize(),\n",
        "        A.Normalize(\n",
        "          mean=(0.485, 0.456, 0.406),  # ImageNet mean\n",
        "          std=(0.229, 0.224, 0.225),   # ImageNet std\n",
        "          max_pixel_value=255.0\n",
        "        ),\n",
        "        ToTensorV2(), # converte in torch.Tensor\n",
        "    ], bbox_params=A.BboxParams(\n",
        "        format='coco',\n",
        "        label_fields=['category_id'],\n",
        "        min_area=0.0,          # scarta bbox con area < 1px\n",
        "        min_visibility=0.0     # scarta bbox con visibilità < 0%\n",
        "      ))\n",
        "\n",
        "    # Passiamo l’albumentations transform al dataset\n",
        "    ds = CocoDetectionTransformed(img_dir, ann_path, transform=transform)\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=2\n",
        "    )"
      ],
      "metadata": {
        "id": "nxLLg7yOC8MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader_test(img_dir, ann_path):\n",
        "    transform = A.Compose([\n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        # A.Normalize(),\n",
        "        A.Normalize(\n",
        "          mean=(0.485, 0.456, 0.406),  # ImageNet mean\n",
        "          std=(0.229, 0.224, 0.225),   # ImageNet std\n",
        "          max_pixel_value=255.0\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ], bbox_params=A.BboxParams(\n",
        "        format='coco',\n",
        "        label_fields=['category_id'],\n",
        "        min_area=1.0,          # scarta bbox con area < 1px\n",
        "        min_visibility=0.0     # scarta bbox con visibilità < 0%\n",
        "      ))\n",
        "\n",
        "    ds = CocoDetectionTransformed(img_dir, ann_path, transform=transform)\n",
        "\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda x: tuple(zip(*x)),\n",
        "        num_workers=2\n",
        "    )"
      ],
      "metadata": {
        "id": "O1QT1v90C9wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the 3 sets (train, val, test):\n",
        "train_loader = get_loader(f'{BASE_DIR}/train/images', f'{BASE_DIR}/annotations_train.json', True)\n",
        "val_loader   = get_loader(f'{BASE_DIR}/val/images', f'{BASE_DIR}/annotations_val.json', False)\n",
        "test_loader = get_loader_test(f'{BASE_DIR}/test/images', f'{BASE_DIR}/annotations_test.json')"
      ],
      "metadata": {
        "id": "i24XDyqrC_Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam (short for Adaptive Moment Estimation) is one of the most widely used optimizers in deep learning.\n",
        "\n",
        "The optimizer is an algorithm that updates model weights during training to reduce loss.\n",
        "- _AdamW_ is a variant of Adam, which also includes a weight penalty (weight decay) in a more correct way than classical Adam.\n",
        "- The _W_ stands for “Weight Decay fix”: it improves regularization and reduces overfitting.\n",
        "\n",
        "In this way:\n",
        "- Converges quickly (like Adam)\n",
        "- Reduces overfitting better than Adam\n",
        "- Is the standard in many modern models (such as BERT, EfficientDet, etc.)."
      ],
      "metadata": {
        "id": "UN5iGkLXDCnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model initialization\n",
        "config = get_efficientdet_config(VARIANT) # load configuration for tf_efficientdet_d0\n",
        "config.num_classes = NUM_CLASSES # sets the number of classes\n",
        "config.image_size = (IMAGE_SIZE, IMAGE_SIZE) # sets the image size\n",
        "# config.image_size = None # This tells the model to accept input of any size and apply the correct resizing internally\n",
        "net = EfficientDet(config, pretrained_backbone=True) # Initialize the model (use pre-trained backbone)\n",
        "# net = EfficientDet(config)\n",
        "net.class_net = HeadNet(config, num_outputs=NUM_CLASSES) # Create the classifier (HeadNet)\n",
        "model = DetBenchTrain(net, config).to(DEVICE) # He wraps it in a training module\n",
        "\n",
        "# CONGELAMENTO del backbone\n",
        "# Blocca i gradenti del backbone: nelle prime epoche si addestrano solo class_net e box_net\n",
        "for param in net.backbone.parameters():\n",
        "    param.requires_grad = False  # freeze backbone layers :contentReference[oaicite:3]{index=3}\n",
        "\n",
        "# Ottimizzatore con solo parametri “trainabili”\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4\n",
        "    # lr=1e-5\n",
        ")  # learning rate iniziale per testate :contentReference[oaicite:4]{index=4}\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # Use AdamW optimizer"
      ],
      "metadata": {
        "id": "uHCqErJkDDdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HeadNet is the final block of the EfficientDet model that does box classification and regression.\n",
        "\n",
        "EfficientDet has two “heads”:\n",
        "- Classification head: says “what” is in the image (object class)\n",
        "- Regression head: says “where” it is (bounding box)\n",
        "\n",
        "When we use: _net.class_net = HeadNet(config, num_outputs=NUM_CLASSES)_ we are customizing the classification head of the model to use our number of classes, for example 1 for ‘person’.\n",
        "\n",
        "Without this change, the model would remain pre-configured for COCO (80 classes), so it would miss everything in the custom dataset."
      ],
      "metadata": {
        "id": "NGYqiZCJDGsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now initialize values to track the best validation and apply early stopping:"
      ],
      "metadata": {
        "id": "VYuJ8Q7-DMey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numero di epoche dopo cui “scongelare” il backbone\n",
        "freeze_epochs = 10\n",
        "\n",
        "# Early stopping setup:\n",
        "epoch = 0\n",
        "epochs_no_improve = 0  # Count how many epochs passed without validation improvement\n",
        "best_loss = float('inf')  # Best (minimal) validation loss so far\n",
        "best_model_path = f'effdet_{VARIANT}_best.pth'  # File to save best weights\n",
        "\n",
        "# Training loop\n",
        "while epoch < EPOCHS and epochs_no_improve < PATIENCE:\n",
        "    if epoch == freeze_epochs:\n",
        "        for param in net.backbone.parameters():\n",
        "            param.requires_grad = True   # ← unfreeze backbone layers\n",
        "        # ricrea l’optimizer per includere ora tutti i parametri\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "        print(f\"[Info] Backbone scongelato all'epoca {epoch}\")\n",
        "\n",
        "    model.train()  # Training mode\n",
        "    total_loss = 0.0  # Sum of training losses this epoch\n",
        "\n",
        "    for images, targets, _ in train_loader: # con _ scarto 'silenziosamente' il terzo elemento quando non serve\n",
        "        images = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for t in targets:\n",
        "            b = []\n",
        "            l = []\n",
        "            for obj in t:\n",
        "                x, y, w, h = obj['bbox']\n",
        "                if w > 1 and h > 1:  # Filter invalid bboxes\n",
        "                    x1, y1 = x, y\n",
        "                    x2, y2 = x + w, y + h\n",
        "                    b.append([x1, y1, x2, y2])\n",
        "                    l.append(0)  # Class index (0 for single class)\n",
        "\n",
        "            b = torch.tensor(b, dtype=torch.float32)\n",
        "            l = torch.tensor(l, dtype=torch.int64)\n",
        "            boxes.append(b.to(DEVICE))\n",
        "            labels.append(l.to(DEVICE))\n",
        "\n",
        "        # img_sizes = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(images), device=DEVICE)\n",
        "        # img_sizes = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "        # img_scales = torch.ones(len(images), device=DEVICE)\n",
        "        img_sizes  = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "        img_scales = torch.ones(len(images), device=DEVICE)\n",
        "\n",
        "        loss_dict = model(images, {\n",
        "            'bbox': boxes,\n",
        "            'cls':  labels,\n",
        "            'img_scale': img_scales,\n",
        "            'img_size': img_sizes\n",
        "        })\n",
        "\n",
        "        # Forward pass and loss calculation\n",
        "        #loss_dict = model(images, {\n",
        "          #   'bbox': boxes,\n",
        "          #  'cls': labels,\n",
        "            # 'img_scale': img_scales,\n",
        "            #'img_size': img_sizes\n",
        "        #})\n",
        "        loss = loss_dict['loss']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        print(f\"[Epoch {epoch}] Batch loss: {loss.item()}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets, _ in val_loader:\n",
        "            images = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "            boxes = []\n",
        "            labels = []\n",
        "\n",
        "            for t in targets:\n",
        "                b = []\n",
        "                l = []\n",
        "                for obj in t:\n",
        "                    x, y, w, h = obj['bbox']\n",
        "                    if w > 1 and h > 1:\n",
        "                        x1, y1 = x, y\n",
        "                        x2, y2 = x + w, y + h\n",
        "                        b.append([x1, y1, x2, y2])\n",
        "                        l.append(0)\n",
        "                b = torch.tensor(b, dtype=torch.float32)\n",
        "                l = torch.tensor(l, dtype=torch.int64)\n",
        "                boxes.append(b.to(DEVICE))\n",
        "                labels.append(l.to(DEVICE))\n",
        "\n",
        "            # img_sizes = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(images), device=DEVICE)\n",
        "            # img_sizes = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "            # img_scales = torch.ones(len(images), device=DEVICE)\n",
        "\n",
        "            img_sizes  = torch.tensor([[img.shape[1], img.shape[2]] for img in images], device=DEVICE)\n",
        "            img_scales = torch.ones(len(images), device=DEVICE)\n",
        "\n",
        "            loss_dict = model(images, {\n",
        "                'bbox': boxes,\n",
        "                'cls':  labels,\n",
        "                'img_scale': img_scales,\n",
        "                'img_size': img_sizes\n",
        "            })\n",
        "            #loss_dict = model(images, {\n",
        "             #   'bbox': boxes,\n",
        "              #  'cls': labels,\n",
        "               # 'img_scale': img_scales,\n",
        "                #'img_size': img_sizes\n",
        "            #})\n",
        "            loss = loss_dict['loss']\n",
        "\n",
        "            if not torch.isnan(loss):\n",
        "                val_loss += loss.item()\n",
        "            else:\n",
        "                print(\"NaN detected in loss, batch skipped.\")\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"[Epoch {epoch+1}/{EPOCHS}] Val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        epochs_no_improve = 0\n",
        "        print(\"Best saved model.\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Epochs without improvement: {epochs_no_improve}/{PATIENCE}\")\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "# Training end reason\n",
        "if epochs_no_improve >= PATIENCE:\n",
        "    print(\"Early stopping enabled.\")\n",
        "else:\n",
        "    print(\"End of training: reached maximum number of epochs.\")"
      ],
      "metadata": {
        "id": "6Yrsd8LpDGiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the weights of the best saved model (.pth) from your local file system to your Google Drive folder:"
      ],
      "metadata": {
        "id": "uHpwn4ibDQSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "# Campionatori sui primi N esempi\n",
        "small_train_sampler = SubsetRandomSampler(range(0, BATCH_SIZE * 5))\n",
        "small_val_sampler   = SubsetRandomSampler(range(0, BATCH_SIZE * 2))\n",
        "\n",
        "# DataLoader ridotti\n",
        "small_train_loader = DataLoader(\n",
        "  train_loader.dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  sampler=small_train_sampler,\n",
        "  collate_fn=train_loader.collate_fn,\n",
        "  num_workers=2\n",
        ")\n",
        "small_val_loader = DataLoader(\n",
        "  val_loader.dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  sampler=small_val_sampler,\n",
        "  collate_fn=val_loader.collate_fn,\n",
        "  num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "6mKGy2nHDQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 11\n",
        "PATIENCE = 4\n",
        "freeze_epochs = 0"
      ],
      "metadata": {
        "id": "1btyXKz_DTua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — Overfit test singola immagine —\n",
        "single_images, single_targets, _ = next(iter(small_train_loader))\n",
        "img = single_images[0].unsqueeze(0).to(DEVICE)   # shape (1,3,512,512)\n",
        "tgt = single_targets[0]\n",
        "\n",
        "# Costruisci bbox xyxy in pixel assoluti\n",
        "b, l = [], []\n",
        "for obj in tgt:\n",
        "    x,y,w,h = obj['bbox']\n",
        "    if w>1 and h>1:\n",
        "        b.append([x, y, x+w, y+h])\n",
        "        l.append(0)\n",
        "boxes  = [torch.tensor(b, dtype=torch.float32, device=DEVICE)]\n",
        "labels = [torch.tensor(l, dtype=torch.int64,   device=DEVICE)]\n",
        "\n",
        "# Scongela tutto il backbone e ricrea un optimizer rapido\n",
        "for p in net.backbone.parameters():\n",
        "    p.requires_grad = True\n",
        "# tmp_opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "tmp_opt = torch.optim.AdamW(model.parameters(), lr=1e-2)  # 0.01!\n",
        "\n",
        "\n",
        "print(\"=== Overfit test single image ===\")\n",
        "for i in range(20):\n",
        "    model.train()\n",
        "    loss = model(\n",
        "        img,\n",
        "        {\n",
        "          \"bbox\":      boxes,\n",
        "          \"cls\":       labels,\n",
        "          \"img_scale\": [1.0],\n",
        "          \"img_size\":  [[IMAGE_SIZE, IMAGE_SIZE]],\n",
        "        }\n",
        "    )[\"loss\"]\n",
        "    tmp_opt.zero_grad()\n",
        "    loss.backward()\n",
        "    tmp_opt.step()\n",
        "    print(f\"Iter {i:02d}, loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "h5rYywJcDVMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ottimo, questo è il comportamento giusto per un overfit-test: in 20 iterazioni passi da ~1500 a ~0.15 di loss. Vuol dire che la pipeline (resize, transform, format delle bboxes, img_size/img_scale) è finalmente allineata alle aspettative di DetBenchTrain."
      ],
      "metadata": {
        "id": "Z5PwOUoXDYXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (assicurati di avere optimizer a lr=1e-4 qui)\n",
        "epoch = 0\n",
        "epochs_no_improve = 0\n",
        "best_loss = float('inf')\n",
        "best_model_path = 'smoke_test_best.pth'\n",
        "\n",
        "while epoch < EPOCHS and epochs_no_improve < PATIENCE:\n",
        "    # — TRAINING\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, targets, _ in small_train_loader:\n",
        "        imgs_tensor = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "        # Costruisci liste di bbox xyxy e labels\n",
        "        boxes_batch, labels_batch = [], []\n",
        "        for t in targets:\n",
        "            b, l = [], []\n",
        "            for obj in t:\n",
        "                x,y,w,h = obj['bbox']\n",
        "                if w>1 and h>1:\n",
        "                    b.append([x, y, x+w, y+h])\n",
        "                    l.append(0)\n",
        "            boxes_batch.append(torch.tensor(b, dtype=torch.float32, device=DEVICE))\n",
        "            labels_batch.append(torch.tensor(l, dtype=torch.int64,   device=DEVICE))\n",
        "\n",
        "        # Meta‑dati\n",
        "        img_sizes  = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(imgs_tensor), device=DEVICE)\n",
        "        img_scales = torch.ones(len(imgs_tensor), device=DEVICE)\n",
        "\n",
        "        # Forward + loss\n",
        "        loss = model(\n",
        "            imgs_tensor,\n",
        "            {\n",
        "                'bbox':      boxes_batch,\n",
        "                'cls':       labels_batch,\n",
        "                'img_scale': img_scales,\n",
        "                'img_size':  img_sizes\n",
        "            }\n",
        "        )[\"loss\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Smoke Test] Epoch {epoch} train loss: {total_loss:.4f}\")\n",
        "\n",
        "    # — UNFREEZE rapido (se ti serve)\n",
        "    if epoch == freeze_epochs:\n",
        "        for p in net.backbone.parameters(): p.requires_grad = True\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "    # — VALIDATION\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, targets, _ in small_val_loader:\n",
        "            imgs_tensor = torch.stack([img.to(DEVICE) for img in images])\n",
        "            boxes_batch, labels_batch = [], []\n",
        "            for t in targets:\n",
        "                b, l = [], []\n",
        "                for obj in t:\n",
        "                    x,y,w,h = obj['bbox']\n",
        "                    if w>1 and h>1:\n",
        "                        b.append([x, y, x+w, y+h])\n",
        "                        l.append(0)\n",
        "                boxes_batch.append(torch.tensor(b, dtype=torch.float32, device=DEVICE))\n",
        "                labels_batch.append(torch.tensor(l, dtype=torch.int64,   device=DEVICE))\n",
        "\n",
        "            loss = model(\n",
        "                imgs_tensor,\n",
        "                {\n",
        "                    'bbox':      boxes_batch,\n",
        "                    'cls':       labels_batch,\n",
        "                    'img_scale': torch.ones(len(imgs_tensor), device=DEVICE),\n",
        "                    'img_size':  torch.tensor([[IMAGE_SIZE,IMAGE_SIZE]]*len(imgs_tensor), device=DEVICE)\n",
        "                }\n",
        "            )[\"loss\"]\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(small_val_loader)\n",
        "    print(f\"[Smoke Test] Epoch {epoch} val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "print(\"Smoke‑test completed!\")"
      ],
      "metadata": {
        "id": "tcVfac_bDWuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from effdet import DetBenchPredict\n",
        "\n",
        "# Ricrea la configurazione esatta usata in training\n",
        "config = get_efficientdet_config(VARIANT)\n",
        "config.num_classes = NUM_CLASSES\n",
        "config.image_size  = (IMAGE_SIZE, IMAGE_SIZE)  # o None se usi resize interno\n",
        "\n",
        "# Istanzia il modello per la predizione\n",
        "net_pred = EfficientDet(config, pretrained_backbone=False)\n",
        "net_pred.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "model_pred = DetBenchPredict(net_pred).to(DEVICE)\n",
        "\n",
        "# Carica i pesi salvati dal tuo smoke‑test (o training)\n",
        "model_pred.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
        "model_pred.eval()"
      ],
      "metadata": {
        "id": "VjZTC7A2DcpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "THRESHOLD = 0.3  # prova anche 0.0 per debug\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _, img_ids in val_loader:\n",
        "        for img, img_id in zip(images, img_ids):\n",
        "            img_t = img.unsqueeze(0).to(DEVICE)\n",
        "            output = model_pred(img_t)[0]  # già xyxy a 512×512\n",
        "\n",
        "            boxes  = output[:, :4].cpu().numpy()\n",
        "            print(\"▶ Raw boxes (prima dello scaling):\")\n",
        "            print(boxes[:5])\n",
        "            print(\"   (dovrebbero essere tutti numeri tra 0 e 512)\")\n",
        "            scores = output[:, 4].cpu().numpy()\n",
        "            labels = output[:, 5].cpu().numpy().astype(int)\n",
        "\n",
        "            # ricostruisci nel formato COCO a dimensione originale\n",
        "            orig = val_loader.dataset.coco.imgs[img_id]\n",
        "            sx, sy = orig['width']/IMAGE_SIZE, orig['height']/IMAGE_SIZE\n",
        "\n",
        "            for (x1,y1,x2,y2), score, label in zip(boxes, scores, labels):\n",
        "                  if score < THRESHOLD:\n",
        "                    continue\n",
        "                  x1o, y1o = x1*sx, y1*sy\n",
        "                  w, h = (x2-x1)*sx, (y2-y1)*sy\n",
        "\n",
        "                  predictions.append({\n",
        "                      \"image_id\":    int(img_id),\n",
        "                      # \"category_id\": int(label) + 1,    # ← qui il +1!\n",
        "                      \"category_id\": int(label),\n",
        "                      \"bbox\":        [float(x1o), float(y1o), float(w),   float(h)],# [x1o, y1o, w, h],\n",
        "                      \"score\":       float(score)\n",
        "                  })\n",
        "\"\"\"\n",
        "            for (x1,y1,x2,y2), score, lab in zip(boxes, scores, labels):\n",
        "                if score < THRESHOLD: continue\n",
        "                x1o,y1o = x1*sx, y1*sy\n",
        "                w, h   = (x2-x1)*sx, (y2-y1)*sy\n",
        "                predictions.append({\n",
        "                    \"image_id\":    int(img_id),\n",
        "                    \"category_id\": int(lab)+1,\n",
        "                    \"bbox\":        [x1o, y1o, w, h],\n",
        "                    \"score\":       float(score)\n",
        "                })\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4vfsuGjpDdMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "import json\n",
        "\n",
        "# 1) Salvo le predizioni\n",
        "with open(\"results_coco.json\", \"w\") as f:\n",
        "    json.dump(predictions, f)\n",
        "\n",
        "# 2) Carico le annotazioni GT e assicuro che 'info' esista\n",
        "coco_gt = COCO(f\"{BASE_DIR}/annotations_val.json\")\n",
        "if \"info\" not in coco_gt.dataset:\n",
        "    coco_gt.dataset[\"info\"] = {\n",
        "        \"description\": \"val annotations\",\n",
        "        \"version\": \"1.0\",\n",
        "        \"year\": 2025,\n",
        "        \"contributor\": \"Daniele\",\n",
        "        \"date_created\": \"2025/07/24\"\n",
        "    }\n",
        "\n",
        "# 3) Carico i risultati e lancio la valutazione\n",
        "coco_dt = coco_gt.loadRes(\"results_coco.json\")\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
        "\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()"
      ],
      "metadata": {
        "id": "_hx3PA9ZDgHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stampa i primi 5 elementi grezzi\n",
        "for i, p in enumerate(predictions[:5]):\n",
        "    print(f\"Pred {i}: image_id={p['image_id']}, bbox={p['bbox']}, score={p['score']:.3f}, cat={p['category_id']}\")\n",
        "\n",
        "# Se vuoi visualizzare a schermo una di quelle immagini con il box:\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "# Prendi il primo prediction\n",
        "p = predictions[0]\n",
        "img_id = p['image_id']\n",
        "bbox   = p['bbox']  # [x, y, w, h]\n",
        "\n",
        "# Trova il path dell'immagine a partire da img_id\n",
        "# (assumendo che test_loader.dataset.coco.imgs[img_id]['file_name'] dia il nome file)\n",
        "file_name = val_loader.dataset.coco.imgs[img_id]['file_name']\n",
        "img_path  = os.path.join(BASE_DIR, 'val', 'images', file_name)\n",
        "\n",
        "# Carica e mostra\n",
        "img = Image.open(img_path).convert('RGB')\n",
        "fig,ax = plt.subplots(1)\n",
        "ax.imshow(img)\n",
        "rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
        "                         linewidth=2, edgecolor='r', facecolor='none')\n",
        "ax.add_patch(rect)\n",
        "plt.title(f\"ID {img_id}, score {p['score']:.2f}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "omKek_1yDhzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 1) Prendi un real_idx (indice all’interno di coco.dataset['images'])\n",
        "real_idx = val_loader.dataset.valid_indices[0]\n",
        "\n",
        "# 2) Estraggo l’info corretta\n",
        "img_info = val_loader.dataset.coco.dataset['images'][real_idx]\n",
        "img_id   = img_info['id']\n",
        "file_name = img_info['file_name']\n",
        "img_path  = os.path.join(BASE_DIR, 'val', 'images', file_name)\n",
        "\n",
        "# 3) Carico l’immagine\n",
        "img = Image.open(img_path).convert('RGB')\n",
        "fig, ax = plt.subplots(1, figsize=(10,6))\n",
        "ax.imshow(img)\n",
        "\n",
        "# 4) Disegno le GT (in verde)\n",
        "coco_gt = COCO(f\"{BASE_DIR}/annotations_val.json\")\n",
        "gt_ids  = coco_gt.getAnnIds(imgIds=[img_id])\n",
        "for ann in coco_gt.loadAnns(gt_ids):\n",
        "    x,y,w,h = ann['bbox']\n",
        "    rect = patches.Rectangle((x,y), w, h,\n",
        "                             linewidth=2, edgecolor='g', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "# 5) Disegno le predizioni (in rosso)\n",
        "for p in predictions:\n",
        "    if p['image_id'] != img_id:\n",
        "        continue\n",
        "    x,y,w,h = p['bbox']\n",
        "    rect = patches.Rectangle((x,y), w, h,\n",
        "                             linewidth=2, edgecolor='r', facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "plt.title(f\"GT (verde) vs PRED (rosso) su image_id={img_id}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mauAEOQ4Djlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_xywh(boxA, boxB):\n",
        "    ax, ay, wA, hA = boxA\n",
        "    bx, by, wB, hB = boxB\n",
        "    inter_x1 = max(ax, bx)\n",
        "    inter_y1 = max(ay, by)\n",
        "    inter_x2 = min(ax + wA, bx + wB)\n",
        "    inter_y2 = min(ay + hA, by + hB)\n",
        "    inter_w  = max(0, inter_x2 - inter_x1)\n",
        "    inter_h  = max(0, inter_y2 - inter_y1)\n",
        "    inter_area = inter_w * inter_h\n",
        "    union = wA*hA + wB*hB - inter_area\n",
        "    return (inter_area / union) if union > 0 else 0.0"
      ],
      "metadata": {
        "id": "OMyewtcODlUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prendi le GT e le predizioni per img_id\n",
        "gt_boxes   = [ann['bbox'] for ann in coco_gt.loadAnns(gt_ids)]\n",
        "pred_boxes = [p['bbox']  for p in predictions if p['image_id']==img_id]\n",
        "\n",
        "for i, pb in enumerate(pred_boxes[:10]):\n",
        "    best_iou = max(iou_xywh(pb, gb) for gb in gt_boxes)\n",
        "    print(f\"Pred #{i}  score={predictions[i]['score']:.2f}  →  max IoU vs GT = {best_iou:.3f}\")"
      ],
      "metadata": {
        "id": "pKSx_lR6Dm2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(predictions))"
      ],
      "metadata": {
        "id": "w5CI82tnDolw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Prendi 1 immagine\n",
        "single_images, single_targets, _ = next(iter(small_train_loader))\n",
        "img  = single_images[0].unsqueeze(0).to(DEVICE)    # (1, 3, 512, 512)\n",
        "tgt  = single_targets[0]                           # lista di dict\n",
        "\n",
        "# 2) Costruisci bboxes in COCO‐format [x, y, w, h] normalizzato\n",
        "b, l = [], []\n",
        "for obj in tgt:\n",
        "    x,y,w,h = obj['bbox']\n",
        "    if w>1 and h>1:\n",
        "        b.append([x/IMAGE_SIZE,      # x_min\n",
        "                  y/IMAGE_SIZE,      # y_min\n",
        "                  w/IMAGE_SIZE,      # width\n",
        "                  h/IMAGE_SIZE])     # height\n",
        "        l.append(0)\n",
        "boxes  = [torch.tensor(b, dtype=torch.float32, device=DEVICE)]\n",
        "labels = [torch.tensor(l, dtype=torch.int64,   device=DEVICE)]\n",
        "\n",
        "# 3) Scongela tutto e ottimizza\n",
        "for p in net.backbone.parameters(): p.requires_grad = True\n",
        "tmp_opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"=== Overfit test single image COCO‐format normalized xywh ===\")\n",
        "for i in range(20):\n",
        "    model.train()\n",
        "    loss = model(\n",
        "        img,\n",
        "        {\n",
        "            \"bbox\": boxes,\n",
        "            \"cls\":  labels,\n",
        "        }\n",
        "    )[\"loss\"]\n",
        "    tmp_opt.zero_grad()\n",
        "    loss.backward()\n",
        "    tmp_opt.step()\n",
        "    print(f\"Iter {i:02d}, loss = {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "OhEMLfNzDo-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "epochs_no_improve = 0\n",
        "best_loss = float('inf')\n",
        "best_model_path = 'smoke_test_best.pth'\n",
        "\n",
        "while epoch < EPOCHS and epochs_no_improve < PATIENCE:\n",
        "    # — TRAINING\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, targets, _ in small_train_loader:\n",
        "        # 1) Impila le immagini\n",
        "        imgs_tensor = torch.stack([img.to(DEVICE) for img in images])\n",
        "        print(\"▷ imgs_tensor\", imgs_tensor.shape, imgs_tensor.min(), imgs_tensor.max())\n",
        "\n",
        "        # 2) Costruisci le liste di box e label\n",
        "        boxes_batch = []\n",
        "        labels_batch = []\n",
        "        for t in targets:\n",
        "            b, l = [], []\n",
        "            \"\"\"\n",
        "              for obj in t:\n",
        "                x, y, w, h = obj['bbox']\n",
        "                if w > 1 and h > 1:\n",
        "                  x1 = x      / IMAGE_SIZE\n",
        "                  y1 = y      / IMAGE_SIZE\n",
        "                  x2 = (x + w)/ IMAGE_SIZE\n",
        "                  y2 = (y + h)/ IMAGE_SIZE\n",
        "              \"\"\"\n",
        "            for obj in t:\n",
        "              x, y, w, h = obj['bbox']\n",
        "              if w > 1 and h > 1:\n",
        "                x1 = x\n",
        "                y1 = y\n",
        "                x2 = x + w\n",
        "                y2 = y + h\n",
        "                b.append([x1, y1, x2, y2])\n",
        "                l.append(0)\n",
        "            boxes_batch.append(torch.tensor(b, dtype=torch.float32, device=DEVICE))\n",
        "            labels_batch.append(torch.tensor(l, dtype=torch.int64,   device=DEVICE))\n",
        "\n",
        "        print(\"▷ sample boxes px:\", boxes_batch[0][:5])\n",
        "        print(\"▷ sample labels:\", labels_batch[0][:5])\n",
        "\n",
        "        # 3) Costruisci img_sizes e img_scales\n",
        "        img_sizes  = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(imgs_tensor), device=DEVICE)\n",
        "        img_scales = torch.ones(len(imgs_tensor), device=DEVICE)\n",
        "\n",
        "        # 4) Forward + loss\n",
        "        loss_dict = model(imgs_tensor, {\n",
        "            'bbox':      boxes_batch,\n",
        "            'cls':       labels_batch,\n",
        "            'img_scale': img_scales,\n",
        "            'img_size':  img_sizes\n",
        "        })\n",
        "        loss = loss_dict['loss']\n",
        "\n",
        "        # 5) Backward + step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Smoke Test] Epoch {epoch} train loss: {total_loss:.4f}\")\n",
        "\n",
        "    # — UNFREEZE rapido\n",
        "    if epoch == freeze_epochs:\n",
        "        for p in net.backbone.parameters():\n",
        "            p.requires_grad = True\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "        print(\"[Smoke Test] Backbone unfrozen\")\n",
        "\n",
        "    # — VALIDATION\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, targets, _ in small_val_loader:\n",
        "            imgs_tensor = torch.stack([img.to(DEVICE) for img in images])\n",
        "            print(\"▷ imgs_tensor.shape, min/max:\", imgs_tensor.shape, imgs_tensor.min().item(), imgs_tensor.max().item())\n",
        "\n",
        "            boxes_batch = []\n",
        "            labels_batch = []\n",
        "            for t in targets:\n",
        "                b, l = [], []\n",
        "                \"\"\"\n",
        "                for obj in t:\n",
        "                  x, y, w, h = obj['bbox']\n",
        "                  if w > 1 and h > 1:\n",
        "                    x1 = x      / IMAGE_SIZE\n",
        "                    y1 = y      / IMAGE_SIZE\n",
        "                    x2 = (x + w)/ IMAGE_SIZE\n",
        "                    y2 = (y + h)/ IMAGE_SIZE\n",
        "                \"\"\"\n",
        "                for obj in t:\n",
        "                  x, y, w, h = obj['bbox']\n",
        "                  if w > 1 and h > 1:\n",
        "                    x1 = x\n",
        "                    y1 = y\n",
        "                    x2 = x + w\n",
        "                    y2 = y + h\n",
        "                    b.append([x1, y1, x2, y2])\n",
        "                    l.append(0)\n",
        "                boxes_batch.append(torch.tensor(b, dtype=torch.float32, device=DEVICE))\n",
        "                labels_batch.append(torch.tensor(l, dtype=torch.int64,   device=DEVICE))\n",
        "\n",
        "\n",
        "            print(\"▷ sample boxes px:\", boxes_batch[0][:5])\n",
        "            print(\"▷ sample labels:\", labels_batch[0][:5])\n",
        "\n",
        "            img_sizes  = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(imgs_tensor), device=DEVICE)\n",
        "            img_scales = torch.ones(len(imgs_tensor), device=DEVICE)\n",
        "\n",
        "            loss_dict = model(imgs_tensor, {\n",
        "                'bbox':      boxes_batch,\n",
        "                'cls':       labels_batch,\n",
        "                'img_scale': img_scales,\n",
        "                'img_size':  img_sizes\n",
        "            })\n",
        "            val_loss += loss_dict['loss'].item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(small_val_loader)\n",
        "    print(f\"[Smoke Test] Epoch {epoch} val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # — Early stopping\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        epochs_no_improve = 0\n",
        "        print(\"[Smoke Test] Best model saved\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"[Smoke Test] No improvement: {epochs_no_improve}/{PATIENCE}\")\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "print(\"Smoke‑test completed!\")"
      ],
      "metadata": {
        "id": "zDLkM-KiDqfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "epochs_no_improve = 0\n",
        "best_loss = float('inf')\n",
        "best_model_path = 'smoke_test_best.pth'\n",
        "\n",
        "while epoch < EPOCHS and epochs_no_improve < PATIENCE:\n",
        "    # — TRAINING\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, targets, _) in enumerate(small_train_loader):\n",
        "        # 1) Impila le immagini\n",
        "        imgs_tensor = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "        # --- DEBUG STEP: stampa target e bbox sul primo batch della prima epoca\n",
        "        if epoch == 0 and batch_idx == 0:\n",
        "            print(\"Sample targets:\", targets)\n",
        "            print(\"Sample bboxes:\", [obj['bbox'] for t in targets for obj in t][:5])\n",
        "\n",
        "        # 2) Costruisci le liste di box e label\n",
        "        boxes_batch = []\n",
        "        labels_batch = []\n",
        "        for t in targets:\n",
        "            b, l = [], []\n",
        "            for obj in t:\n",
        "                x, y, w, h = obj['bbox']\n",
        "                if w > 1 and h > 1:\n",
        "                    b.append([x, y, x+w, y+h])\n",
        "                    l.append(0)\n",
        "            boxes_batch.append(torch.tensor(b, dtype=torch.float32, device=DEVICE))\n",
        "            labels_batch.append(torch.tensor(l, dtype=torch.int64,   device=DEVICE))\n",
        "\n",
        "        # 3) Costruisci img_sizes e img_scales\n",
        "        img_sizes  = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(imgs_tensor), device=DEVICE)\n",
        "        img_scales = torch.ones(len(imgs_tensor), device=DEVICE)\n",
        "\n",
        "        # --- DEBUG STEP: controlla min/max dei pixel normalizzati\n",
        "        if epoch == 0 and batch_idx == 0:\n",
        "            print(\"▶ Image min/max:\", imgs_tensor.min().item(), imgs_tensor.max().item())\n",
        "\n",
        "        # 4) Forward + loss\n",
        "        loss_dict = model(imgs_tensor, {\n",
        "            'bbox':      boxes_batch,\n",
        "            'cls':       labels_batch,\n",
        "            'img_scale': img_scales,\n",
        "            'img_size':  img_sizes\n",
        "        })\n",
        "        loss = loss_dict['loss']\n",
        "\n",
        "        # --- DEBUG STEP: stampa la prima loss\n",
        "        if epoch == 0 and batch_idx == 0:\n",
        "            print(f\"▶ First batch loss: {loss.item():.4f}\")\n",
        "\n",
        "        # 5) Backward + step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Smoke Test] Epoch {epoch} train loss: {total_loss:.4f}\")\n",
        "\n",
        "    # — UNFREEZE rapido\n",
        "    if epoch == freeze_epochs:\n",
        "        for p in net.backbone.parameters():\n",
        "            p.requires_grad = True\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "        print(\"[Smoke Test] Backbone unfrozen\")\n",
        "\n",
        "    # — VALIDATION\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, targets, _ in small_val_loader:\n",
        "            imgs_tensor = torch.stack([img.to(DEVICE) for img in images])\n",
        "\n",
        "            boxes_batch = []\n",
        "            labels_batch = []\n",
        "            for t in targets:\n",
        "                b, l = [], []\n",
        "                for obj in t:\n",
        "                    x, y, w, h = obj['bbox']\n",
        "                    if w > 1 and h > 1:\n",
        "                        b.append([x, y, x+w, y+h])\n",
        "                        l.append(0)\n",
        "                boxes_batch.append(torch.tensor(b, dtype=torch.float32, device=DEVICE))\n",
        "                labels_batch.append(torch.tensor(l, dtype=torch.int64,   device=DEVICE))\n",
        "\n",
        "            img_sizes  = torch.tensor([[IMAGE_SIZE, IMAGE_SIZE]] * len(imgs_tensor), device=DEVICE)\n",
        "            img_scales = torch.ones(len(imgs_tensor), device=DEVICE)\n",
        "\n",
        "            loss_dict = model(imgs_tensor, {\n",
        "                'bbox':      boxes_batch,\n",
        "                'cls':       labels_batch,\n",
        "                'img_scale': img_scales,\n",
        "                'img_size':  img_sizes\n",
        "            })\n",
        "            val_loss += loss_dict['loss'].item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(small_val_loader)\n",
        "    print(f\"[Smoke Test] Epoch {epoch} val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # — Early stopping\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        # torch.save(model.state_dict(), best_model_path)\n",
        "        torch.save(net.state_dict(), 'best_net.pth')\n",
        "        epochs_no_improve = 0\n",
        "        print(\"[Smoke Test] Best model saved\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"[Smoke Test] No improvement: {epochs_no_improve}/{PATIENCE}\")\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "print(\"Smoke‑test completed!\")\n"
      ],
      "metadata": {
        "id": "AF_jBOOJDsT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "imgs, targets, img_ids = next(iter(small_val_loader))\n",
        "imgs_tensor = torch.stack([img.to(DEVICE) for img in imgs])\n",
        "with torch.no_grad():\n",
        "    outputs = model(imgs_tensor)\n",
        "\n",
        "# Disegno tutte le immagini del batch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "for i, output in enumerate(outputs):\n",
        "    img = imgs[i].permute(1,2,0).cpu().numpy()\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    boxes  = output[:,:4].cpu().numpy()\n",
        "    scores = output[:,4].cpu().numpy()\n",
        "    labels = output[:,5].cpu().numpy().astype(int)\n",
        "\n",
        "    orig = small_val_loader.dataset.coco.imgs[img_ids[i]]\n",
        "    sx, sy = orig['width']/IMAGE_SIZE, orig['height']/IMAGE_SIZE\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(6,6))\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Smoke‑test Image {img_ids[i]}\")\n",
        "\n",
        "    for (x1,y1,x2,y2), s in zip(boxes, scores):\n",
        "        # riporto e clamp on‑the‑fly\n",
        "        x1, y1 = max(0, x1*sx), max(0, y1*sy)\n",
        "        x2, y2 = min(orig['width'],  x2*sx), min(orig['height'], y2*sy)\n",
        "        w, h = max(1, x2-x1), max(1, y2-y1)\n",
        "        rect = patches.Rectangle((x1,y1), w,h,\n",
        "                                 linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x1, y1-3, f\"{s:.2f}\",\n",
        "                color='black', backgroundcolor='yellow',\n",
        "                fontsize=8, weight='bold')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "O8FZVZqlDvZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/weights"
      ],
      "metadata": {
        "id": "J5KGtCcrDxnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifichiamo che il file esista:"
      ],
      "metadata": {
        "id": "Px4tx4zhDzvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(best_model_path):\n",
        "  raise FileNotFoundError(f\"Il file dei migliori pesi non è stato trovato: {best_model_path}\")\n",
        "\n",
        "weights_dir = '/content/drive/MyDrive/projectUPV/datasets/AERALIS_EfficientDet_D0/weights'\n",
        "os.makedirs(weights_dir, exist_ok=True)\n",
        "shutil.copy(best_model_path, weights_dir)\n",
        "print(\"EfficientDet weights copied to Google Drive.\")"
      ],
      "metadata": {
        "id": "rPoNX8BpDyDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model in evaluation mode (inference):"
      ],
      "metadata": {
        "id": "TgwKUfebD2wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_efficientdet_config(VARIANT)\n",
        "config.num_classes = NUM_CLASSES\n",
        "config.image_size = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "net = EfficientDet(config, pretrained_backbone=False)\n",
        "net.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "\n",
        "model = DetBenchPredict(net).to(DEVICE)\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "kUjT6TytD3N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on complete test set\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, targets, img_ids in test_loader:\n",
        "        for img, target, img_id in zip(imgs, targets, img_ids):\n",
        "            img_tensor = img.unsqueeze(0).to(DEVICE)\n",
        "            outputs = model(img_tensor)\n",
        "            output = outputs[0]\n",
        "            print(\"RAW output (first 5 rows):\\n\", output[:5])\n",
        "            if output.ndim != 2 or output.shape[1] != 6:\n",
        "                continue\n",
        "\n",
        "            boxes  = output[:, :4].cpu().numpy()\n",
        "            scores = output[:, 4].cpu().numpy()\n",
        "            labels = output[:, 5].cpu().numpy().astype(int)\n",
        "\n",
        "            # dimensioni originali\n",
        "            orig_w = test_loader.dataset.coco.imgs[img_id]['width']\n",
        "            orig_h = test_loader.dataset.coco.imgs[img_id]['height']\n",
        "            scale_x = orig_w / IMAGE_SIZE\n",
        "            scale_y = orig_h / IMAGE_SIZE\n",
        "\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = box\n",
        "\n",
        "                # 1) ripristino dimensioni e clamp\n",
        "                x1 = max(0,     x1 * scale_x)\n",
        "                y1 = max(0,     y1 * scale_y)\n",
        "                x2 = min(orig_w, x2 * scale_x)\n",
        "                y2 = min(orig_h, y2 * scale_y)\n",
        "\n",
        "                # 2) calcolo larghezza/altezza minime\n",
        "                w = max(1, x2 - x1)\n",
        "                h = max(1, y2 - y1)\n",
        "\n",
        "                # 3) appendo predizione “pulita”\n",
        "                predictions.append({\n",
        "                    \"image_id\":    img_id,\n",
        "                    \"category_id\": label + 1,\n",
        "                    \"bbox\":        [x1, y1, w, h],\n",
        "                    \"score\":       float(score)\n",
        "                })\n",
        "\n",
        "print(f\"\\nInference completed. Total predictions collected: {len(predictions)}\")"
      ],
      "metadata": {
        "id": "fiYlRrLRD6CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Ricrea la configurazione come prima\n",
        "config = get_efficientdet_config(VARIANT)\n",
        "config.num_classes = NUM_CLASSES\n",
        "config.image_size  = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "# config.image_size = None\n",
        "\n",
        "# 2) Inizializza EfficientDet senza backbone pretrained (non serve per predict)\n",
        "net_pred = EfficientDet(config, pretrained_backbone=False)\n",
        "net_pred.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "net_pred.load_state_dict(torch.load('best_net.pth', map_location=DEVICE))\n",
        "\n",
        "THRESHOLD = 0.3\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, _, img_ids in small_val_loader:     # batch_size=1\n",
        "        img_tensor = imgs[0].to(DEVICE).unsqueeze(0)\n",
        "        output     = model_pred(img_tensor)[0]     # [num_detections, 6]\n",
        "\n",
        "        # ora output ha già subito NMS e contiene box in pixel rispettosi\n",
        "        boxes  = output[:, :4].cpu().numpy()\n",
        "        scores = output[:, 4].cpu().numpy()\n",
        "        labels = output[:, 5].cpu().numpy().astype(int)\n",
        "\n",
        "        img_id = img_ids[0]\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score < THRESHOLD:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = box\n",
        "            w, h = max(1, x2 - x1), max(1, y2 - y1)\n",
        "            predictions.append({\n",
        "                \"image_id\":    img_id,\n",
        "                \"category_id\": label+1,\n",
        "                \"bbox\":        [float(x1), float(y1), float(w), float(h)],\n",
        "                \"score\":       float(score)\n",
        "            })\n",
        "\n",
        "print(f\"Inference completed: {len(predictions)} boxes\")\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "id": "C2eAHVzPD7tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Ricrea la configurazione come prima\n",
        "config = get_efficientdet_config(VARIANT)\n",
        "config.num_classes = NUM_CLASSES\n",
        "# config.image_size  = (IMAGE_SIZE, IMAGE_SIZE)\n",
        "config.image_size = None\n",
        "\n",
        "# 2) Inizializza EfficientDet senza backbone pretrained (non serve per predict)\n",
        "net_pred = EfficientDet(config, pretrained_backbone=False)\n",
        "net_pred.class_net = HeadNet(config, num_outputs=NUM_CLASSES)\n",
        "\n",
        "# 3) Avvolgilo in DetBenchPredict e carica i pesi\n",
        "model_pred = DetBenchPredict(net_pred).to(DEVICE)\n",
        "# model_pred = DetBenchPredict(net_pred, config).to(DEVICE)\n",
        "model_pred.load_state_dict(torch.load('smoke_test_best.pth'))\n",
        "model_pred.eval()\n",
        "\n",
        "# 4) Inference sul small_val_loader\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for imgs, targets, img_ids in small_val_loader:\n",
        "        for img, img_id in zip(imgs, img_ids):\n",
        "            # img è già un Tensor; aggiungo batch dim\n",
        "            img_tensor = img.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "            # uso il modello di prediction, non train!\n",
        "            outputs = model_pred(img_tensor)\n",
        "            output  = outputs[0]\n",
        "            # --- DEBUG 1: vedi le prime 5 raw preds\n",
        "            if img_id == img_ids[0]:\n",
        "                print(\"RAW preds [0:5]:\\n\", output[:5])\n",
        "\n",
        "            if output.ndim != 2 or output.shape[1] != 6:\n",
        "                continue\n",
        "\n",
        "            boxes  = output[:, :4].cpu().numpy()\n",
        "            scores = output[:, 4].cpu().numpy()\n",
        "            labels = output[:, 5].cpu().numpy().astype(int)\n",
        "\n",
        "            orig = small_val_loader.dataset.coco.imgs[img_id]\n",
        "            scale_x = orig['width']  / IMAGE_SIZE\n",
        "            scale_y = orig['height'] / IMAGE_SIZE\n",
        "\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "              x1, y1, x2, y2 = box\n",
        "              w, h = max(1, x2 - x1), max(1, y2 - y1)\n",
        "              if score < THRESHOLD:\n",
        "                  continue\n",
        "\n",
        "              predictions.append({\n",
        "                  \"image_id\":    img_id,\n",
        "                  \"category_id\": label+1,\n",
        "                  \"bbox\":        [float(x1), float(y1), float(w), float(h)],\n",
        "                  \"score\":       float(score)\n",
        "              })\n",
        "              \"\"\"\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "#                x1, y1, x2, y2 = box\n",
        "#                x1 = max(0,     x1 * scale_x)\n",
        "#                y1 = max(0,     y1 * scale_y)\n",
        "#                x2 = min(orig['width'],  x2 * scale_x)\n",
        "#                y2 = min(orig['height'], y2 * scale_y)\n",
        "#                w  = max(1, x2 - x1)\n",
        "#                h  = max(1, y2 - y1)\n",
        "                x1, y1, x2, y2 = box\n",
        "                # --- DEBUG 2: scala SENZA clamp\n",
        "                x1s, y1s = x1 * scale_x, y1 * scale_y\n",
        "                x2s, y2s = x2 * scale_x, y2 * scale_y\n",
        "                print(f\"SCALED no‑clamp: {(x1s,y1s,x2s,y2s)}  score={score:.2f}\")\n",
        "\n",
        "                # poi reinstaura il clamp/filtri come prima:\n",
        "                x1 = max(0,     x1s)\n",
        "                y1 = max(0,     y1s)\n",
        "                x2 = min(orig['width'],  x2s)\n",
        "                y2 = min(orig['height'], y2s)\n",
        "                w  = max(1, x2 - x1)\n",
        "                h  = max(1, y2 - y1)\n",
        "\n",
        "                predictions.append({\n",
        "                  \"image_id\":    img_id,\n",
        "                  \"category_id\": label + 1,\n",
        "                  \"bbox\":        [x1, y1, w, h],\n",
        "                  \"score\":       float(score)\n",
        "                })\n",
        "\"\"\"\n",
        "print(f\"Smoke‑test inference completed. Predictions: {len(predictions)}\")\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "id": "_Zn5Xh3_D9uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save all predictions in a .json file in the format required by COCOeval:"
      ],
      "metadata": {
        "id": "IgYzH9-GEAyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all fields to native Python types\n",
        "for pred in predictions:\n",
        "    pred[\"category_id\"] = int(pred[\"category_id\"])\n",
        "    pred[\"image_id\"] = int(pred[\"image_id\"])\n",
        "    pred[\"score\"] = float(pred[\"score\"])\n",
        "    pred[\"bbox\"] = [float(x) for x in pred[\"bbox\"]]\n",
        "\n",
        "# Save to COCO-style JSON\n",
        "with open(\"results_coco.json\", \"w\") as f:\n",
        "    json.dump(predictions, f)"
      ],
      "metadata": {
        "id": "qooaBTgBEBLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dopo aver popolato `predictions` (con bbox e score NumPy)\n",
        "\n",
        "# 1) Conversione ai tipi Python nativi e correzione category_id\n",
        "cleaned = []\n",
        "for p in predictions:\n",
        "    img_id = int(p[\"image_id\"])\n",
        "    score  = float(p[\"score\"])\n",
        "    x, y, w, h = p[\"bbox\"]\n",
        "\n",
        "    cleaned.append({\n",
        "        \"image_id\":    img_id,\n",
        "        \"category_id\": 1,            # unica classe nel tuo dataset\n",
        "        \"bbox\":        [float(x), float(y), float(w), float(h)],\n",
        "        \"score\":       score,\n",
        "    })\n",
        "predictions = cleaned\n",
        "\n",
        "# 2) Salvataggio su JSON\n",
        "with open(\"results_coco.json\", \"w\") as f:\n",
        "    json.dump(predictions, f)"
      ],
      "metadata": {
        "id": "ujRYciesECs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_gt = COCO(f\"{BASE_DIR}/annotations_val.json\")\n",
        "if \"info\" not in coco_gt.dataset:\n",
        "    coco_gt.dataset[\"info\"] = {}\n",
        "\n",
        "coco_dt = coco_gt.loadRes(\"results_coco.json\")\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()"
      ],
      "metadata": {
        "id": "1kOdMAEDEESn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coco_gt = COCO(f\"{BASE_DIR}/annotations_test.json\")\n",
        "if \"info\" not in coco_gt.dataset:\n",
        "    coco_gt.dataset[\"info\"] = {}\n",
        "# loadRes prende lo stesso file che hai scritto durante l’inference col test_loader\n",
        "coco_dt = coco_gt.loadRes(\"results_coco2.json\")\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()"
      ],
      "metadata": {
        "id": "W8FDt5r9EF1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final evaluation with pycocotools:"
      ],
      "metadata": {
        "id": "saUruFmpEIbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ground truth annotations\n",
        "coco_gt = COCO(f\"{BASE_DIR}/annotations_test.json\")\n",
        "\n",
        "# Assicuriamoci che esista il campo 'info'\n",
        "if 'info' not in coco_gt.dataset:\n",
        "    coco_gt.dataset['info'] = {}\n",
        "\n",
        "# Load predictions from file …\n",
        "with open(\"results_coco.json\", \"r\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "# … poi prosegui con loadRes e COCOeval\n",
        "coco_dt = coco_gt.loadRes(results)\n",
        "coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
        "coco_eval.evaluate()\n",
        "coco_eval.accumulate()\n",
        "coco_eval.summarize()"
      ],
      "metadata": {
        "id": "6FexrXOvEI7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that values of AP and AR practically at zero indicate that the model is not making correct or meaningful predictions. In essence, it is not recognizing objects well. \\\n",
        "Let's try a small test on a few images and annotations to see if the model recognizes anything."
      ],
      "metadata": {
        "id": "kNjJgfTkEOE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, it may be worth considering that EfficientDet-D0 (or your current EfficientDet configuration) might not be the ideal detector for your specific dataset."
      ],
      "metadata": {
        "id": "s6NIKFZmEOq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to changing the model, we could also:\n",
        "- Review preprocessing and augmentations to make the training data more representative of the test set.\n",
        "- Experiment with learning rate schedulers (e.g., ReduceLROnPlateau or CosineAnnealingLR) to improve convergence.\n",
        "- Adjust inference filtering thresholds (score and minimum box size) so as not to discard potentially valid predictions."
      ],
      "metadata": {
        "id": "QvJMT-oYESQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientDet D1"
      ],
      "metadata": {
        "id": "6PiWdqGMET2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MobileNetV2 + SSD Lite"
      ],
      "metadata": {
        "id": "j-1rX8sVEWte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MobileNetV3 + SSD Lite"
      ],
      "metadata": {
        "id": "fsW0wOYKEaJd"
      }
    }
  ]
}